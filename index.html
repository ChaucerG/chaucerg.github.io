<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>集智书童</title><meta name="keywords" content="AI算法爱好者"><meta name="author" content="ChaucerG"><meta name="copyright" content="ChaucerG"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="AI算法爱好者33">
<meta property="og:type" content="website">
<meta property="og:title" content="集智书童">
<meta property="og:url" content="https://chaucerg.github.io/index.html">
<meta property="og:site_name" content="集智书童">
<meta property="og:description" content="AI算法爱好者33">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://chaucerg.github.io/img/avatar.jpg">
<meta property="article:author" content="ChaucerG">
<meta property="article:tag" content="AI算法爱好者">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://chaucerg.github.io/img/avatar.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://chaucerg.github.io/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":50,"languages":{"author":"作者: ChaucerG","link":"链接: ","source":"来源: 集智书童","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: true,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '集智书童',
  isPost: false,
  isHome: true,
  isHighlightShrink: false,
  isToc: false,
  postUpdate: '2021-09-18 17:34:39'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if (GLOBAL_CONFIG_SITE.isHome && /iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 5.4.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">16</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">28</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-map-signs"></i><span> 目录</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/messageboard/"><i class="fa-fw fas fa-coffee"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于我</span></a></div></div></div></div><div class="page" id="body-wrap"><header class="full_page" id="page-header" style="background-image: url('/img/home_img.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">集智书童</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-map-signs"></i><span> 目录</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/messageboard/"><i class="fa-fw fas fa-coffee"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于我</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="site-info"><h1 id="site-title">集智书童</h1><div id="site-subtitle"><span id="subtitle"></span></div><div id="site_social_icons"><a class="social-icon" href="https://github.com/ChaucerG" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:chaucer_g@126.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="/img/weixin.jpg" target="_blank" title="Wechat"><i class="fab fa-weixin"></i></a></div></div><div id="scroll-down"><i class="fas fa-angle-down scroll-down-effects"></i></div></header><main class="layout" id="content-inner"><div class="recent-posts" id="recent-posts"><div class="recent-post-item"><div class="post_cover left_radius"><a href="/2021/09/18/16/" title="教你How to train自己的Transformer模型">     <img class="post_bg" src="/img/achieve_img.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="教你How to train自己的Transformer模型"></a></div><div class="recent-post-info"><a class="article-title" href="/2021/09/18/16/" title="教你How to train自己的Transformer模型">教你How to train自己的Transformer模型</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-09-18T09:20:49.000Z" title="发表于 2021-09-18 17:20:49">2021-09-18</time><span class="article-meta__separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-09-18T09:33:04.344Z" title="更新于 2021-09-18 17:33:04">2021-09-18</time></span><span class="article-meta"><span class="article-meta__separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Transformer/">Transformer</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/%E8%AF%A6%E7%BB%86%E8%A7%A3%E8%AF%BBGooGle%E6%96%B0%E4%BD%9C/">详细解读GooGle新作</a></span></div><div class="content">
简介
Vision Transformers(Vision transformer, ViT)在图像分类、目标检测和语义分割等视觉应用中得到了具有竞争力得性能。
与卷积神经网络相比，当在较小的训练数据集上训练时，通常发现Vision Transformer较弱的归纳偏差导致对模型正则化或数据增强(简称AugReg)的依赖增加。为了更好地理解训练数据量、AugReg、模型大小和计算预算之间的相互作用，作者进行了系统的实验研究。
研究的一个结果是，作者发现增加的计算和AugReg相结合，可以产生与在更多训练数据上训练的模型具有相同性能的模型:在ImageNet-21k数据集上训练各种大小的ViT模型，这些模型与在更大的JFT-300M数据集上训练的模型比较甚至可以得到更好得结果。
论文作者主要说了什么？


第一次系统的、大规模的研究在训练Vision Transformer之前，正则化、数据增强、模型大小和训练数据大小之间的相互作用，包括它们各自对达到一定性能水平所需的计算预算的影响。


通过迁移学习的视角来评估预训练模型。因此，作者描述了一个相当复杂的训练设置训练前的Vision  ...</div></div></div><div class="recent-post-item"><div class="post_cover right_radius"><a href="/2021/09/18/15/" title="AdaConv自适应卷积让你的GAN比AdaIN更看重细节">     <img class="post_bg" src="/img/achieve_img.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="AdaConv自适应卷积让你的GAN比AdaIN更看重细节"></a></div><div class="recent-post-info"><a class="article-title" href="/2021/09/18/15/" title="AdaConv自适应卷积让你的GAN比AdaIN更看重细节">AdaConv自适应卷积让你的GAN比AdaIN更看重细节</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-09-18T09:15:38.000Z" title="发表于 2021-09-18 17:15:38">2021-09-18</time><span class="article-meta__separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-09-18T09:27:20.374Z" title="更新于 2021-09-18 17:27:20">2021-09-18</time></span><span class="article-meta"><span class="article-meta__separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/CVPR2021-GAN%E8%A7%A3%E8%AF%BB/">CVPR2021 GAN解读</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/AdaConv%E8%87%AA%E9%80%82%E5%BA%94%E5%8D%B7%E7%A7%AF/">AdaConv自适应卷积</a><span class="article-meta__link">•</span><a class="article-meta__tags" href="/tags/GAN/">GAN</a></span></div><div class="content">

本文提出了AdaIN的改进版本，称为自适应卷积 (AdaConv)，它可以同时适应统计和结构风格，表现SOTA！性能优于AdaIN等网络，已收录于CVPR 2021！作者单位：迪士尼研究院, ETH Zurich

简介
图像的风格迁移是CNN在艺术领域的一种应用，这里的风格迁移是指将其中一幅图像的“风格”迁移到另一幅图像上，同时保留后者的内容。
近期的SOTA风格迁移模型大多数都是基于最新的自适应实例归一化(AdaIN)，这是一种将风格特征的统计特性迁移到内容图像的技术，可以实时迁移大量风格。
然而，AdaIN是一个全局的操作；因此，在迁移过程中，风格图像中的局部几何结构常常被忽略。于是作者提出了自适应卷积(AdaConv)，这是AdaIN的通用扩展，允许同时传输统计和结构风格。除了风格迁移，本文的方法还可以很容易地扩展到基于风格的图像生成，以及其他已经采用AdaIN的任务。
相关工作
2.1 Neural Style Transfer based on CNNs
基于CNN的神经网络风格转移最初是由Gatys等人提出的。虽然该方法允许在图像之间转换任意样式，但它的优化过程是比较 ...</div></div></div><div class="recent-post-item"><div class="post_cover left_radius"><a href="/2021/09/18/14/" title="详细解读：HP-x激活函数">     <img class="post_bg" src="/img/achieve_img.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="详细解读：HP-x激活函数"></a></div><div class="recent-post-info"><a class="article-title" href="/2021/09/18/14/" title="详细解读：HP-x激活函数">详细解读：HP-x激活函数</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-09-18T09:07:25.000Z" title="发表于 2021-09-18 17:07:25">2021-09-18</time><span class="article-meta__separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-09-18T09:15:05.175Z" title="更新于 2021-09-18 17:15:05">2021-09-18</time></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/%E5%85%A8%E6%96%B0%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/">全新激活函数</a></span></div><div class="content">
简介
本文提出了orthogonal-Padé激活函数，它是可以训练的激活函数，在标准深度学习数据集和模型中具有更快的学习能力，同时可以提高模型的准确率。根据实验，在六种orthogonal-Padé激活中找到了2种最佳的候选函数，作者称之为 safe Hermite-Pade(HP)激活函数，即HP-1和HP-2。
与ReLU相比,HP-1和HP-2帮助PreActResNet-34带来不同程度的提升(top-1精度提升分别为5.06%和4.63%),在CIFAR100数据集上MobileNet V2模型提升分别为3.02%和2.75%分别，在CIFAR10数据集上PreActResNet-34的top-1精度分别增加了2.02%和1.78%,LeNet的top-1精度分别提升为2.24%和2.06%,Efficientnet B0的top-1精度分别提升为2.15%和2.03%。
前人工作简介
深度卷积神经网络由多个隐藏层和神经元构成。然后通过每个神经元的激活函数引入非线性。
ReLU由于其简单性，是深度学习中最受欢迎的激活函数。虽然ReLU有一个缺点叫做 dying ReLU， ...</div></div></div><div class="recent-post-item"><div class="post_cover right_radius"><a href="/2021/09/18/13/" title="详细解读Attention-Based方法解决遮挡人脸识别问题">     <img class="post_bg" src="/img/achieve_img.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="详细解读Attention-Based方法解决遮挡人脸识别问题"></a></div><div class="recent-post-info"><a class="article-title" href="/2021/09/18/13/" title="详细解读Attention-Based方法解决遮挡人脸识别问题">详细解读Attention-Based方法解决遮挡人脸识别问题</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-09-18T09:00:12.000Z" title="发表于 2021-09-18 17:00:12">2021-09-18</time><span class="article-meta__separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-09-18T09:07:05.752Z" title="更新于 2021-09-18 17:07:05">2021-09-18</time></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/Attention-Based%E6%96%B9%E6%B3%95/">Attention-Based方法</a><span class="article-meta__link">•</span><a class="article-meta__tags" href="/tags/%E9%81%AE%E6%8C%A1%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%E9%97%AE%E9%A2%98/">遮挡人脸识别问题</a></span></div><div class="content">
简介
在非约束性环境(如大量人群)中捕获的人脸照片，仍然对当前的人脸识别方法构成挑战，因为人脸经常被前景中的物体或人遮挡。然而，很少有研究涉及到识别部分面孔的任务。

本文提出了一种新的遮挡人脸识别方法，能够识别不同遮挡区域的人脸。通过将一个ResNet中间特征映射的attentional pooling与一个单独的聚合模块相结合来实现这一点。为了保证attention map的多样性，并处理被遮挡的部分，作者进一步对遮挡Face的常见损失函数进行了调整。实验表明，在多个benchmark下本文方法的性能优于所有baseline。
本文工作贡献可以概括为以下几点:


以ResNet为例，利用attentional pooling和聚合网络提出了一种新的扩展，并使用2种适用于部分FR的常见损失函数进行训练；


在多个局部FR的详尽分析中表明，本文的改进大大提高了识别性能。


方法
2.1 Network Architecture

下图描述了partial FR方法，分为3个模块:Extract、Attend和Aggregate。
Extract模块从输入图像中提取特征图$F\i ...</div></div></div><div class="recent-post-item"><div class="post_cover left_radius"><a href="/2021/09/18/12/" title="详细解读：如何让EfficientNet更加高效、速度更快">     <img class="post_bg" src="/img/achieve_img.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="详细解读：如何让EfficientNet更加高效、速度更快"></a></div><div class="recent-post-info"><a class="article-title" href="/2021/09/18/12/" title="详细解读：如何让EfficientNet更加高效、速度更快">详细解读：如何让EfficientNet更加高效、速度更快</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-09-18T08:51:10.000Z" title="发表于 2021-09-18 16:51:10">2021-09-18</time><span class="article-meta__separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-09-18T08:58:47.218Z" title="更新于 2021-09-18 16:58:47">2021-09-18</time></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/EfficientNet/">EfficientNet</a><span class="article-meta__link">•</span><a class="article-meta__tags" href="/tags/%E6%95%88%E7%8E%87%E6%96%B0%E7%A7%80/">效率新秀</a></span></div><div class="content">
简介
近年来，许多研究致力于提高图像分类训练和推理的效率。这种研究通常集中于提高理论效率，通常以每个FLOP的ImageNet验证精度来衡量。然而，事实证明，这些理论上的改进在实践中很难实现，特别是在高性能训练加速器上。
在这项工作中，作者关注的是在一个新的加速器类Graphcore IPU上提高最先进的EfficientNet模型的实际效率。本文主要通过以下方式扩展这类模型:

将Depthwise CNN推广为Group CNN;
添加proxy-normalized激活，以使batch normalization性能与batch-independent statistics相匹配;
通过降低训练分辨率和低成本的微调来减少计算量。


作者发现这3种方法都提高了训练和推理的实际效率。
研究背景
2.1 Efficient CNNs分析
在CNN的发展过程中，实际训练效率的提高是创新的重要力量。比如说，AlexNet的成功很大一部分因素便得益于GPU加速，ResNet的成功不仅可以归因于其良好的性能，而且其在GPU上的高吞吐量也相对比较高。
最近，在理论效率方面也取得了重大改进。最 ...</div></div></div><div class="recent-post-item"><div class="post_cover right_radius"><a href="/2021/09/18/11/" title="详细解读：如何再CNN模型中插入Transformer后速度不变精度剧增？">     <img class="post_bg" src="/img/achieve_img.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="详细解读：如何再CNN模型中插入Transformer后速度不变精度剧增？"></a></div><div class="recent-post-info"><a class="article-title" href="/2021/09/18/11/" title="详细解读：如何再CNN模型中插入Transformer后速度不变精度剧增？">详细解读：如何再CNN模型中插入Transformer后速度不变精度剧增？</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-09-18T08:41:17.000Z" title="发表于 2021-09-18 16:41:17">2021-09-18</time><span class="article-meta__separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-09-18T08:49:22.799Z" title="更新于 2021-09-18 16:49:22">2021-09-18</time></span><span class="article-meta"><span class="article-meta__separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Transformer/">Transformer</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/Transformer/">Transformer</a><span class="article-meta__link">•</span><a class="article-meta__tags" href="/tags/%E5%8D%B7%E7%A7%AFCNN/">卷积CNN</a></span></div><div class="content">
简介
本文工作解决了Multi-Head Self-Attention(MHSA)中由于计算/空间复杂度高而导致的vision transformer效率低的缺陷。为此，作者提出了分层的MHSA(H-MHSA)，其表示以分层的方式计算。
具体来说，H-MHSA首先通过把图像patch作为tokens来学习小网格内的特征关系。然后将小网格合并到大网格中，通过将上一步中的每个小网格作为token来学习大网格中的特征关系。这个过程多次迭代以逐渐减少token的数量。
H-MHSA模块很容易插入到任何CNN架构中，并且可以通过反向传播进行训练。作者称这种新的Backbone为TransCNN，它本质上继承了transformer和CNN的优点。实验证明，TransCNN在图像识别中具有最先进的准确性。
Vision Transformer回顾
大家应该都很清楚Transformer严重依赖MHSA来建模长时间依赖关系。假设$X\in R^{N×C}$为输入，其中N和C分别为Token的数量和每个Token的特征维数。这里定义了Query $Q=XW^q$、key $K=XW^k$和 valu ...</div></div></div><div class="recent-post-item"><div class="post_cover left_radius"><a href="/2021/09/18/10/" title="太顶流！Scaling ViT将ImageNet Top-1 Acc刷到90.45%！">     <img class="post_bg" src="/img/achieve_img.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="太顶流！Scaling ViT将ImageNet Top-1 Acc刷到90.45%！"></a></div><div class="recent-post-info"><a class="article-title" href="/2021/09/18/10/" title="太顶流！Scaling ViT将ImageNet Top-1 Acc刷到90.45%！">太顶流！Scaling ViT将ImageNet Top-1 Acc刷到90.45%！</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-09-18T08:25:33.000Z" title="发表于 2021-09-18 16:25:33">2021-09-18</time><span class="article-meta__separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-09-18T08:40:53.503Z" title="更新于 2021-09-18 16:40:53">2021-09-18</time></span><span class="article-meta"><span class="article-meta__separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Transformer/">Transformer</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/Scaling-ViT/">Scaling ViT</a><span class="article-meta__link">•</span><a class="article-meta__tags" href="/tags/ImageNet-Top-1-Acc/">ImageNet Top-1 Acc</a><span class="article-meta__link">•</span><a class="article-meta__tags" href="/tags/Transformer/">Transformer</a></span></div><div class="content">

本文改进了ViT的架构和训练，减少了内存消耗并提高了模型的准确性！最终成功训练了一个具有20亿参数的ViT模型：ViT-G，在ImageNet上达到了90.45%的Top-1准确率.作者单位：谷歌大脑（苏黎世），有原ViT一作和二作

简介
视觉Transformer(ViT)等基于注意力的神经网络最近在许多计算机视觉基准测试中取得了最先进的结果。比例是获得出色结果的主要因素，因此，了解模型的scaling属性是有效设计的关键。虽然已经研究了扩展Transformer语言模型的规律，但尚不清楚Vision Transformers如何扩展。
为了解决这个问题，作者向上和向下扩展ViT模型和数据，并描述错误率、数据和计算之间的关系。在此过程中，作者改进了ViT的架构和训练，减少了内存消耗并提高了结果模型的准确性。结果，作者成功地训练了一个具有20亿个参数的ViT模型，该模型在ImageNet上达到了90.45%的Top-1准确率。该模型在小样本学习上也表现良好，例如，在ImageNet上每类只有10个examples的情况下可以达到84.86%的Top-1准确率。

主要结论
作者首 ...</div></div></div><div class="recent-post-item"><div class="post_cover right_radius"><a href="/2021/09/18/9%20%E8%AF%A6%E7%BB%86%E8%A7%A3%E8%AF%BBNMS-Loss%E6%98%AF%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%AD%E7%9A%84%E9%81%AE%E6%8C%A1%E9%97%AE%E9%A2%98/" title="详细解读NMS-Loss是如何解决目标检测中的遮挡问题">     <img class="post_bg" src="/img/achieve_img.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="详细解读NMS-Loss是如何解决目标检测中的遮挡问题"></a></div><div class="recent-post-info"><a class="article-title" href="/2021/09/18/9%20%E8%AF%A6%E7%BB%86%E8%A7%A3%E8%AF%BBNMS-Loss%E6%98%AF%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%AD%E7%9A%84%E9%81%AE%E6%8C%A1%E9%97%AE%E9%A2%98/" title="详细解读NMS-Loss是如何解决目标检测中的遮挡问题">详细解读NMS-Loss是如何解决目标检测中的遮挡问题</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-09-18T08:10:45.000Z" title="发表于 2021-09-18 16:10:45">2021-09-18</time><span class="article-meta__separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-09-18T08:24:58.144Z" title="更新于 2021-09-18 16:24:58">2021-09-18</time></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/NMS-Loss/">NMS-Loss</a><span class="article-meta__link">•</span><a class="article-meta__tags" href="/tags/%E8%AE%A9%E6%A3%80%E6%B5%8B%E5%91%8A%E5%88%AB%E9%81%AE%E6%8C%A1/">让检测告别遮挡</a></span></div><div class="content">
简介
非极大值抑制(Non-Maximum Suppression, NMS)在目标检测中至关重要，它通过合并假阳性(FP)和假阴性(FN)影响目标检测结果，尤其是在人群遮挡场景中。在本文中提出了NMS造成的训练目标和评估指标之间的弱连接问题，并提出了一种新的损失函数NMS-loss，使NMS过程可以端到端地被训练而不需要任何附加的网络参数。
NMS-loss惩罚2种情况，即FP没有被抑制，而FN被NMS错误地删除。具体来说，NMS-Loss提出了pull loss将具有相同目标的预测拉得很近，以及push loss将具有不同目标的预测推得很远。

实验结果表明，在NMS-Loss的帮助下NMS-Ped检测器在Caltech数据集上的Miss Rate为5.92%，在CityPersons数据集上的Miss Rate为10.08%，均优于现有的同类检测器。
本文主要贡献


首先提出了行人检测中训练目标与评估指标之间的弱连接问题，并提出了一种新的NMS-loss，使NMS过程在不引入任何参数和运行时间开销的情况下可以端到端进行训练。


作者提出了精心设计的pull loss和pus ...</div></div></div><div class="recent-post-item"><div class="post_cover left_radius"><a href="/2021/09/05/8%20%E5%8D%B7%E7%A7%AF%E4%B8%8ESelf-Attention%E5%AE%8C%E7%BE%8E%E8%9E%8D%E5%90%88X-volution%E6%8F%92%E5%85%A5CV%E6%A8%A1%E5%9E%8B%E5%B0%86%E5%B8%A6%E6%9D%A5%E5%85%A8%E4%BB%BB%E5%8A%A1%E7%9A%84%E6%B6%A8%E7%82%B9/" title="卷积与Self-Attention完美融合X-volution插入CV模型将带来全任务的涨点">     <img class="post_bg" src="/img/achieve_img.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="卷积与Self-Attention完美融合X-volution插入CV模型将带来全任务的涨点"></a></div><div class="recent-post-info"><a class="article-title" href="/2021/09/05/8%20%E5%8D%B7%E7%A7%AF%E4%B8%8ESelf-Attention%E5%AE%8C%E7%BE%8E%E8%9E%8D%E5%90%88X-volution%E6%8F%92%E5%85%A5CV%E6%A8%A1%E5%9E%8B%E5%B0%86%E5%B8%A6%E6%9D%A5%E5%85%A8%E4%BB%BB%E5%8A%A1%E7%9A%84%E6%B6%A8%E7%82%B9/" title="卷积与Self-Attention完美融合X-volution插入CV模型将带来全任务的涨点">卷积与Self-Attention完美融合X-volution插入CV模型将带来全任务的涨点</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-09-05T15:35:11.000Z" title="发表于 2021-09-05 23:35:11">2021-09-05</time><span class="article-meta__separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-09-18T08:24:52.975Z" title="更新于 2021-09-18 16:24:52">2021-09-18</time></span><span class="article-meta"><span class="article-meta__separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E5%8D%B7%E7%A7%AFCNN/">卷积CNN</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/%E5%8D%B7%E7%A7%AF/">卷积</a><span class="article-meta__link">•</span><a class="article-meta__tags" href="/tags/Self-Attention/">Self-Attention</a><span class="article-meta__link">•</span><a class="article-meta__tags" href="/tags/CV%E6%A8%A1%E5%9E%8B/">CV模型</a></span></div><div class="content">


本文建立了一个由卷积和self-attention组成的多分支基本模块，能够统一局部和非局部特征交互，然后可以结构重新参数化为一个纯卷积风格的算子：X-volution，即插即用！可助力分类、检测和分割任务的涨点！作者单位：上海交通大学(倪冰冰团队), 华为海思

简介
卷积和self-attention是深度神经网络中的2个基本构建块，前者以线性方式提取图像的局部特征，而后者通过非局部关系编码高阶上下文关系。尽管本质上是相互补充的，即一阶/高阶、最先进的架构，但是，CNN或Transformer均缺乏一种原则性的方法来在单个计算模块中同时应用这2种操作，因为它们的异构计算视觉任务的全局点积的模式和过度负担。
在这项工作中，作者从理论上推导出一种全局self-attention近似方案，该方案通过对变换特征的卷积运算来近似self-attention。基于近似方案建立了一个由卷积和self-attention操作组成的多分支基本模块，能够统一局部和非局部特征交互。重要的是，一旦经过训练，这个多分支模块可以通过结构重新参数化有条件地转换为单个标准卷积操作，呈现一个名为X-volut ...</div></div></div><div class="recent-post-item"><div class="post_cover right_radius"><a href="/2021/09/05/7%20%E8%AF%A6%E7%BB%86%E8%A7%A3%E8%AF%BBTransformer%E6%80%8E%E6%A0%B7%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%B9%B6%E8%B6%85%E8%B6%8AResNet/" title="详细解读Transformer怎样从零训练并超越ResNet">     <img class="post_bg" src="/img/achieve_img.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="详细解读Transformer怎样从零训练并超越ResNet"></a></div><div class="recent-post-info"><a class="article-title" href="/2021/09/05/7%20%E8%AF%A6%E7%BB%86%E8%A7%A3%E8%AF%BBTransformer%E6%80%8E%E6%A0%B7%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%B9%B6%E8%B6%85%E8%B6%8AResNet/" title="详细解读Transformer怎样从零训练并超越ResNet">详细解读Transformer怎样从零训练并超越ResNet</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-09-05T14:59:08.000Z" title="发表于 2021-09-05 22:59:08">2021-09-05</time><span class="article-meta__separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-09-18T08:24:46.383Z" title="更新于 2021-09-18 16:24:46">2021-09-18</time></span><span class="article-meta"><span class="article-meta__separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Transformer/">Transformer</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/ResNet/">ResNet</a><span class="article-meta__link">•</span><a class="article-meta__tags" href="/tags/Transformer/">Transformer</a><span class="article-meta__link">•</span><a class="article-meta__tags" href="/tags/Tricks/">Tricks</a><span class="article-meta__link">•</span><a class="article-meta__tags" href="/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/">图像分类</a></span></div><div class="content">


本文证明了在没有大规模预训练或强数据增广的情况下，在ImageNet上从头开始训练时，所得ViT的性能优于类似大小和吞吐量的ResNet！而且还拥有更敏锐的注意力图。作者单位：谷歌,UCLA

简介
Vision Transformers(ViTs)和MLPs标志着在用通用神经架构替换手动特征或归纳偏置方面的进一步努力。现有工作通过大量数据为模型赋能，例如大规模预训练和/或重复的强数据增广，并且还报告了与优化相关的问题（例如，对初始化和学习率的敏感性）。
因此，本文从损失几何的角度研究了ViTs和MLP-Mixer，旨在提高模型在训练和推理时的泛化效率。可视化和Hessian揭示了收敛模型极其敏感的局部最小值。
同时通过使用最近提出的锐度感知优化器提高平滑度，进而大大提高了ViT和MLP-Mixer在跨越监督、对抗、对比和迁移学习（例如，+5.3% 和 +11.0%）的各种任务上的准确性和鲁棒性使用简单的Inception进行预处理，ViT-B/16和Mixer-B/16在ImageNet上的准确率分别为Top-1）。
作者研究表明，改进的平滑度归因于前几层中较稀疏的活动神经元。 ...</div></div></div><nav id="pagination"><div class="pagination"><span class="page-number current">1</span><a class="page-number" href="/page/2/#content-inner">2</a><a class="extend next" rel="next" href="/page/2/#content-inner"><i class="fas fa-chevron-right fa-fw"></i></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">ChaucerG</div><div class="author-info__description">干就完了！</div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">16</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">28</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/ChaucerG"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/ChaucerG" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:chaucer_g@126.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="/img/weixin.jpg" target="_blank" title="Wechat"><i class="fab fa-weixin"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">更多内容可以关注【集智书童】公众号和【集智书童】知识星球，获取原创文章以及项目源码 ~</div></div><div class="sticky_layout"><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2021/09/18/16/" title="教你How to train自己的Transformer模型"><img src="/img/achieve_img.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="教你How to train自己的Transformer模型"/></a><div class="content"><a class="title" href="/2021/09/18/16/" title="教你How to train自己的Transformer模型">教你How to train自己的Transformer模型</a><time datetime="2021-09-18T09:20:49.000Z" title="发表于 2021-09-18 17:20:49">2021-09-18</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/09/18/15/" title="AdaConv自适应卷积让你的GAN比AdaIN更看重细节"><img src="/img/achieve_img.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="AdaConv自适应卷积让你的GAN比AdaIN更看重细节"/></a><div class="content"><a class="title" href="/2021/09/18/15/" title="AdaConv自适应卷积让你的GAN比AdaIN更看重细节">AdaConv自适应卷积让你的GAN比AdaIN更看重细节</a><time datetime="2021-09-18T09:15:38.000Z" title="发表于 2021-09-18 17:15:38">2021-09-18</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/09/18/14/" title="详细解读：HP-x激活函数"><img src="/img/achieve_img.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="详细解读：HP-x激活函数"/></a><div class="content"><a class="title" href="/2021/09/18/14/" title="详细解读：HP-x激活函数">详细解读：HP-x激活函数</a><time datetime="2021-09-18T09:07:25.000Z" title="发表于 2021-09-18 17:07:25">2021-09-18</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/09/18/13/" title="详细解读Attention-Based方法解决遮挡人脸识别问题"><img src="/img/achieve_img.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="详细解读Attention-Based方法解决遮挡人脸识别问题"/></a><div class="content"><a class="title" href="/2021/09/18/13/" title="详细解读Attention-Based方法解决遮挡人脸识别问题">详细解读Attention-Based方法解决遮挡人脸识别问题</a><time datetime="2021-09-18T09:00:12.000Z" title="发表于 2021-09-18 17:00:12">2021-09-18</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/09/18/12/" title="详细解读：如何让EfficientNet更加高效、速度更快"><img src="/img/achieve_img.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="详细解读：如何让EfficientNet更加高效、速度更快"/></a><div class="content"><a class="title" href="/2021/09/18/12/" title="详细解读：如何让EfficientNet更加高效、速度更快">详细解读：如何让EfficientNet更加高效、速度更快</a><time datetime="2021-09-18T08:51:10.000Z" title="发表于 2021-09-18 16:51:10">2021-09-18</time></div></div></div></div><div class="card-widget card-categories"><div class="item-headline">
            <i class="fas fa-folder-open"></i>
            <span>分类</span>
            
            </div>
            <ul class="card-category-list" id="aside-cat-list">
            <li class="card-category-list-item "><a class="card-category-list-link" href="/categories/CVPR2021-GAN%E8%A7%A3%E8%AF%BB/"><span class="card-category-list-name">CVPR2021 GAN解读</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Transformer/"><span class="card-category-list-name">Transformer</span><span class="card-category-list-count">6</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/YOLO/"><span class="card-category-list-name">YOLO</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/%E5%8D%B7%E7%A7%AFCNN/"><span class="card-category-list-name">卷积CNN</span><span class="card-category-list-count">2</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/%E9%A1%B9%E7%9B%AE%E5%AE%9E%E8%B7%B5/"><span class="card-category-list-name">项目实践</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/%E9%A1%B9%E7%9B%AE%E9%83%A8%E7%BD%B2/"><span class="card-category-list-name">项目部署</span><span class="card-category-list-count">1</span></a></li>
            </ul></div><div class="card-widget card-tags"><div class="item-headline"><i class="fas fa-tags"></i><span>标签</span></div><div class="card-tag-cloud"><a href="/tags/%E6%AE%8B%E5%B7%AE%E8%BF%9E%E6%8E%A5/" style="font-size: 1.15em; color: rgb(29, 78, 199)">残差连接</a><a href="/tags/CNN/" style="font-size: 1.15em; color: rgb(71, 48, 87)">CNN</a><a href="/tags/Tansformer/" style="font-size: 1.3em; color: rgb(91, 160, 108)">Tansformer</a><a href="/tags/ResNet/" style="font-size: 1.3em; color: rgb(153, 196, 153)">ResNet</a><a href="/tags/Scaling-ViT/" style="font-size: 1.15em; color: rgb(35, 178, 63)">Scaling ViT</a><a href="/tags/ImageNet-Top-1-Acc/" style="font-size: 1.15em; color: rgb(122, 94, 172)">ImageNet Top-1 Acc</a><a href="/tags/Transformer/" style="font-size: 1.45em; color: rgb(136, 90, 200)">Transformer</a><a href="/tags/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB/" style="font-size: 1.3em; color: rgb(133, 111, 199)">人脸识别</a><a href="/tags/%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B/" style="font-size: 1.3em; color: rgb(52, 32, 101)">人脸检测</a><a href="/tags/YOLO/" style="font-size: 1.15em; color: rgb(19, 190, 180)">YOLO</a><a href="/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/" style="font-size: 1.15em; color: rgb(10, 34, 89)">目标检测</a><a href="/tags/Attention/" style="font-size: 1.15em; color: rgb(152, 116, 173)">Attention</a><a href="/tags/Tricks/" style="font-size: 1.15em; color: rgb(75, 150, 31)">Tricks</a><a href="/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/" style="font-size: 1.15em; color: rgb(1, 96, 191)">图像分类</a><a href="/tags/%E5%8D%B7%E7%A7%AF/" style="font-size: 1.15em; color: rgb(35, 117, 173)">卷积</a><a href="/tags/Self-Attention/" style="font-size: 1.15em; color: rgb(16, 58, 171)">Self-Attention</a><a href="/tags/CV%E6%A8%A1%E5%9E%8B/" style="font-size: 1.15em; color: rgb(15, 45, 44)">CV模型</a><a href="/tags/NMS-Loss/" style="font-size: 1.15em; color: rgb(113, 71, 54)">NMS-Loss</a><a href="/tags/%E8%AE%A9%E6%A3%80%E6%B5%8B%E5%91%8A%E5%88%AB%E9%81%AE%E6%8C%A1/" style="font-size: 1.15em; color: rgb(137, 136, 135)">让检测告别遮挡</a><a href="/tags/%E5%8D%B7%E7%A7%AFCNN/" style="font-size: 1.15em; color: rgb(123, 22, 13)">卷积CNN</a><a href="/tags/EfficientNet/" style="font-size: 1.15em; color: rgb(60, 161, 34)">EfficientNet</a><a href="/tags/%E6%95%88%E7%8E%87%E6%96%B0%E7%A7%80/" style="font-size: 1.15em; color: rgb(87, 114, 76)">效率新秀</a><a href="/tags/Attention-Based%E6%96%B9%E6%B3%95/" style="font-size: 1.15em; color: rgb(40, 48, 68)">Attention-Based方法</a><a href="/tags/%E9%81%AE%E6%8C%A1%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%E9%97%AE%E9%A2%98/" style="font-size: 1.15em; color: rgb(1, 190, 105)">遮挡人脸识别问题</a><a href="/tags/%E5%85%A8%E6%96%B0%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/" style="font-size: 1.15em; color: rgb(99, 135, 182)">全新激活函数</a><a href="/tags/AdaConv%E8%87%AA%E9%80%82%E5%BA%94%E5%8D%B7%E7%A7%AF/" style="font-size: 1.15em; color: rgb(31, 62, 55)">AdaConv自适应卷积</a><a href="/tags/GAN/" style="font-size: 1.15em; color: rgb(144, 106, 44)">GAN</a><a href="/tags/%E8%AF%A6%E7%BB%86%E8%A7%A3%E8%AF%BBGooGle%E6%96%B0%E4%BD%9C/" style="font-size: 1.15em; color: rgb(93, 97, 77)">详细解读GooGle新作</a></div></div><div class="card-widget card-archives"><div class="item-headline"><i class="fas fa-archive"></i><span>归档</span></div><ul class="card-archive-list"><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2021/09/"><span class="card-archive-list-date">九月 2021</span><span class="card-archive-list-count">16</span></a></li></ul></div><div class="card-widget card-webinfo"><div class="item-headline"><i class="fas fa-chart-line"></i><span>网站资讯</span></div><div class="webinfo"><div class="webinfo-item"><div class="item-name">文章数目 :</div><div class="item-count">16</div></div><div class="webinfo-item"><div class="item-name">已运行时间 :</div><div class="item-count" id="runtimeshow" data-publishDate="2021-01-07T16:00:00.000Z"></div></div><div class="webinfo-item"><div class="item-name">本站访客数 :</div><div class="item-count" id="busuanzi_value_site_uv"></div></div><div class="webinfo-item"><div class="item-name">本站总访问量 :</div><div class="item-count" id="busuanzi_value_site_pv"></div></div><div class="webinfo-item"><div class="item-name">最后更新时间 :</div><div class="item-count" id="last-push-date" data-lastPushDate="2021-09-18T09:34:39.479Z"></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('/img/home_img.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2021 By ChaucerG</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Welcome to my CHANNEL!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (true){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><script src="/js/search/local-search.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())</script><div class="js-pjax"><script>function subtitleType () {
  if (true) { 
    var typed = new Typed("#subtitle", {
      strings: "抱有一颗终身学习的心&#44; 持续前进 ~,Study till the end of my life ~".split(","),
      startDelay: 300,
      typeSpeed: 150,
      loop: true,
      backSpeed: 50
    })
  } else {
    document.getElementById("subtitle").innerHTML = '抱有一颗终身学习的心&#44; 持续前进 ~'
  }
}

if (true) {
  if (typeof Typed === 'function') {
    subtitleType()
  } else {
    getScript('https://cdn.jsdelivr.net/npm/typed.js/lib/typed.min.js').then(subtitleType)
  }
} else {
  subtitleType()
}</script></div><script defer="defer" id="fluttering_ribbon" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-fluttering-ribbon.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>