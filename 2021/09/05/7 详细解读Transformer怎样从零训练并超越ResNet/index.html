<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>详细解读Transformer怎样从零训练并超越ResNet | 集智书童</title><meta name="keywords" content="Transformer,ResNet,Tricks,图像分类"><meta name="author" content="ChaucerG"><meta name="copyright" content="ChaucerG"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="本文证明了在没有大规模预训练或强数据增广的情况下，在ImageNet上从头开始训练时，所得ViT的性能优于类似大小和吞吐量的ResNet！而且还拥有更敏锐的注意力图。作者单位：谷歌,UCLA  简介Vision Transformers(ViTs)和MLPs标志着在用通用神经架构替换手动特征或归纳偏置方面的进一步努力。现有工作通过大量数据为模型赋能，例如大规模预训练和&#x2F;或重复的强数据增广，并">
<meta property="og:type" content="article">
<meta property="og:title" content="详细解读Transformer怎样从零训练并超越ResNet">
<meta property="og:url" content="https://chaucerg.github.io/2021/09/05/7%20%E8%AF%A6%E7%BB%86%E8%A7%A3%E8%AF%BBTransformer%E6%80%8E%E6%A0%B7%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%B9%B6%E8%B6%85%E8%B6%8AResNet/index.html">
<meta property="og:site_name" content="集智书童">
<meta property="og:description" content="本文证明了在没有大规模预训练或强数据增广的情况下，在ImageNet上从头开始训练时，所得ViT的性能优于类似大小和吞吐量的ResNet！而且还拥有更敏锐的注意力图。作者单位：谷歌,UCLA  简介Vision Transformers(ViTs)和MLPs标志着在用通用神经架构替换手动特征或归纳偏置方面的进一步努力。现有工作通过大量数据为模型赋能，例如大规模预训练和&#x2F;或重复的强数据增广，并">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://chaucerg.github.io/img/achieve_img.jpg">
<meta property="article:published_time" content="2021-09-05T14:59:08.000Z">
<meta property="article:modified_time" content="2021-09-18T08:24:46.383Z">
<meta property="article:author" content="ChaucerG">
<meta property="article:tag" content="ResNet">
<meta property="article:tag" content="Transformer">
<meta property="article:tag" content="Tricks">
<meta property="article:tag" content="图像分类">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://chaucerg.github.io/img/achieve_img.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://chaucerg.github.io/2021/09/05/7%20%E8%AF%A6%E7%BB%86%E8%A7%A3%E8%AF%BBTransformer%E6%80%8E%E6%A0%B7%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%B9%B6%E8%B6%85%E8%B6%8AResNet/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":50,"languages":{"author":"作者: ChaucerG","link":"链接: ","source":"来源: 集智书童","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: true,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '详细解读Transformer怎样从零训练并超越ResNet',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2021-09-18 16:24:46'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if (GLOBAL_CONFIG_SITE.isHome && /iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 5.4.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">17</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">29</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-map-signs"></i><span> 目录</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/messageboard/"><i class="fa-fw fas fa-coffee"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于我</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/achieve_img.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">集智书童</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-map-signs"></i><span> 目录</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/messageboard/"><i class="fa-fw fas fa-coffee"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于我</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">详细解读Transformer怎样从零训练并超越ResNet</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-09-05T14:59:08.000Z" title="发表于 2021-09-05 22:59:08">2021-09-05</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-09-18T08:24:46.383Z" title="更新于 2021-09-18 16:24:46">2021-09-18</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Transformer/">Transformer</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="详细解读Transformer怎样从零训练并超越ResNet"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p><br></p>
<p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210606/1.png" alt=""></p>
<blockquote>
<p>本文证明了在没有大规模预训练或强数据增广的情况下，在ImageNet上从头开始训练时，所得ViT的性能优于类似大小和吞吐量的ResNet！而且还拥有更敏锐的注意力图。<br><strong>作者单位</strong>：谷歌,UCLA</p>
</blockquote>
<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>Vision Transformers(ViTs)和MLPs标志着在用通用神经架构替换手动特征或归纳偏置方面的进一步努力。现有工作通过大量数据为模型赋能，例如大规模预训练和/或重复的强数据增广，并且还报告了与优化相关的问题（例如，对初始化和学习率的敏感性）。</p>
<p>因此，本文从损失几何的角度研究了ViTs和MLP-Mixer，旨在提高模型在训练和推理时的泛化效率。可视化和Hessian揭示了收敛模型极其敏感的局部最小值。</p>
<p>同时通过使用最近提出的<strong>锐度感知优化器</strong>提高平滑度，进而大大提高了ViT和MLP-Mixer在跨越监督、对抗、对比和迁移学习（例如，+5.3\% 和 +11.0\%）的各种任务上的准确性和鲁棒性使用简单的Inception进行预处理，ViT-B/16和Mixer-B/16在ImageNet上的准确率分别为Top-1）。</p>
<p>作者研究表明，改进的平滑度归因于前几层中较稀疏的活动神经元。在没有大规模预训练或强数据增强的情况下，在ImageNet上从头开始训练时，所得 ViT的性能优于类似大小和吞吐量的ResNet。同时还拥有更敏锐的注意力图。</p>
<h2 id="Background和Related-Work"><a href="#Background和Related-Work" class="headerlink" title="Background和Related Work"></a>Background和Related Work</h2><p>最近的研究发现，ViT中的self-attention对性能并不是至关重要的，因此出现了一些专门基于mlp的架构。这里作者以MLP-Mixer为例。MLP-Mixer与ViT共享相同的输入层;也就是说，它将一个图像分割成一系列不重叠的Patches/Toekns。然后，它在torkn mlp和channel mlp之间交替使用，其中前者允许来自不同空间位置的特征融合。</p>
<h2 id="ViTs和MLP-Mixers收敛到锐局部极小值"><a href="#ViTs和MLP-Mixers收敛到锐局部极小值" class="headerlink" title="ViTs和MLP-Mixers收敛到锐局部极小值"></a>ViTs和MLP-Mixers收敛到锐局部极小值</h2><p>目前的ViTs、mlp-mixer和相关的无卷积架构的训练方法很大程度上依赖于大量的预训练或强数据增强。它对数据和计算有很高的要求，并导致许多超参数需要调整。</p>
<p>现有的研究表明，当在ImageNet上从头开始训练时，如果不结合那些先进的数据增强，尽管使用了各种正则化技术(例如，权重衰减，Dropout等)ViTs的精度依然低于类似大小和吞吐量的卷积网络。同时在鲁棒性测试方面，vit和resnet之间也存在较大的差距。</p>
<p>此外，Chen等人发现，在训练vit时，梯度会出现峰值，导致精确度突然下降，Touvron等人也发现初始化和超参数对训练很敏感。这些问题其实都可以归咎于优化问题。</p>
<p>在本文中，作者研究了ViTs和mlp-mixer的损失情况，从优化的角度理解它们，旨在减少它们对大规模预训练或强数据增强的依赖。</p>
<h3 id="3-1-ViTs和MLP-Mixers收敛到极sharp局部极小值"><a href="#3-1-ViTs和MLP-Mixers收敛到极sharp局部极小值" class="headerlink" title="3.1 ViTs和MLP-Mixers收敛到极sharp局部极小值"></a>3.1 ViTs和MLP-Mixers收敛到极sharp局部极小值</h3><p>众所周知，当模型收敛到曲率小的平坦区域时模型会具有更好的泛化性能。在[36]之后，当resnet、vit和MLP-Mixers在ImageNet上使用基本的初始风格预处理从头开始训练时，作者绘制损失图：</p>
<p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210606/2.png" alt=""></p>
<p>如图1(a)到1(c)所示，ViTs和mlp-mixer比ResNets收敛到更清晰的区域。</p>
<p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210606/3.png" alt=""></p>
<p>在表1中，通过计算主要的Hessian特征值$\lambda<em>{max}$进一步验证了结果。ViT和MLP-Mixer的$\lambda</em>{max}$值比ResNet大一个数量级，并且MLP-Mixer的曲率在3种中是最大的(具体分析见4.4节)。</p>
<h3 id="3-2-Small-training-errors"><a href="#3-2-Small-training-errors" class="headerlink" title="3.2 Small training errors"></a>3.2 Small training errors</h3><p>这种向sharp区域的收敛与图2(左)所示的训练动态一致。尽管Mixer-B/16参数少于ViT-B/16(59M vs 87M)，同时它有一个小的训练误差，但测试性能还是比较差的，这意味着使用cross-token MLP学习的相互作用比ViTs’ self-attention机制更容易过度拟合。这种差异可能解释了mlp-mixer更容易陷入尖锐的局部最小值。</p>
<p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210606/4.png" alt=""></p>
<h3 id="3-3-ViTs和MLP-Mixers的可训练性较差"><a href="#3-3-ViTs和MLP-Mixers的可训练性较差" class="headerlink" title="3.3 ViTs和MLP-Mixers的可训练性较差"></a>3.3 ViTs和MLP-Mixers的可训练性较差</h3><p>此外，作者还发现ViTs和MLP-Mixers的可训练性较差，可训练性定义为通过梯度下降优化的网络的有效性。Xiao等人的研究表明，神经网络的可训练性可以用相关的神经切线核(NTK)的条件数来表征:</p>
<script type="math/tex; mode=display">Θ(x,x')= J(x)J(x')^T</script><p>其中$J$是雅可比矩阵。</p>
<p>用$\lambda<em>1≥··≥\lambda_m$表示NTK $Θ</em>{train}$的特征值，最小的特征值$\lambda_m$以条件数κ$=\lambda_1=\lambda_m$的速率指数收敛。如果κ是发散的，那么网络将变得不可训练。如表1所示，ResNets的κ是相当稳定的，这与之前的研究结果一致，即ResNets无论深度如何都具有优越的可训练性。然而，当涉及到ViT和时，条件数是不同的MLP-Mixer，证实了对ViTs的训练需要额外的辅助。</p>
<h2 id="CNN-Free视觉架构优化器原理"><a href="#CNN-Free视觉架构优化器原理" class="headerlink" title="CNN-Free视觉架构优化器原理"></a>CNN-Free视觉架构优化器原理</h2><p>常用的一阶优化器(如SGD,Adam)只寻求最小化训练损失。它们通常会忽略与泛化相关的高阶信息，如曲率。然而，深度神经网络的损失具有高度非凸性，在评估时容易达到接近0的训练误差，但泛化误差较高，更谈不上在测试集具有不同分布时的鲁棒性。</p>
<p>由于对视觉数据缺乏归纳偏差ViTs和MLPs放大了一阶优化器的这种缺陷，导致过度急剧的损失scene和较差的泛化性能，如前一节所示。假设平滑收敛时的损失scene可以显著提高那些无卷积架构的泛化能力，那么最近提出的锐度感知最小化(SAM)可以很好的避免锐度最小值。</p>
<h3 id="4-1-SAM-Overview"><a href="#4-1-SAM-Overview" class="headerlink" title="4.1 SAM:Overview"></a>4.1 SAM:Overview</h3><p>从直觉上看，SAM寻找的是可以使整个邻近训练损失最低的参数w，训练损失$L_{train}$通过构造极小极大目标:</p>
<p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210606/5.png" width = "350" align=center /></p>
<p>其中$\rho$是neighbourhood ball的大小。在不失一般性的情况下，这里使用$l_2$范数作为其强经验结果，这里为了简单起见省略了正则化项。</p>
<p>由于内部最大化下式的确切解很难获得：</p>
<p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210606/6.png" width = "350" align=center /></p>
<p>因此，这里采用了一个有效的一阶近似:</p>
<p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210606/7.png" width = "400" align=center /></p>
<p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210606/8.png" width = "350" align=center /></p>
<p>在$l_2$范数下，$\hat\epsilon(w)$是当前权值$w$的缩放梯度。计算$\hat\epsilon$后，SAM基于锐度感知梯度更新w。</p>
<h3 id="4-2-SAM优化器实质上改进了ViTs和MLP-Mixers"><a href="#4-2-SAM优化器实质上改进了ViTs和MLP-Mixers" class="headerlink" title="4.2 SAM优化器实质上改进了ViTs和MLP-Mixers"></a>4.2 SAM优化器实质上改进了ViTs和MLP-Mixers</h3><p>作者在没有大规模的预训练或强大的数据增强的情况下训练了vit和MLP-Mixers。直接将SAM应用于vit的原始ImageNet训练pipeline，而不改变任何超参数。<br>pipeline使用了基本的Inception-style的预处理。最初的mlp-mixer的训练设置包括强数据增强的组合;也用同样的Inception-style的预处理来替换它，以便进行公平的比较。</p>
<p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210606/9.png" alt=""></p>
<p>注意，在应用SAM之前，我们对学习速率、权重衰减、Dropout和随机深度进行网格搜索。</p>
<h4 id="1-局部极小值周围的平滑区域"><a href="#1-局部极小值周围的平滑区域" class="headerlink" title="1 局部极小值周围的平滑区域"></a>1 局部极小值周围的平滑区域</h4><p>由于SAM, ViTs和mlp-mixer都汇聚在更平滑的区域，如图1(d)和1(e)所示。</p>
<p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210606/10.png" alt=""></p>
<p>曲率测量，即Hessian矩阵的最大特征值$\lambda_{max}$，也减小到一个小值(见表1)。</p>
<p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210606/11.png" alt=""></p>
<h4 id="2-Higher-accuracy"><a href="#2-Higher-accuracy" class="headerlink" title="2 Higher accuracy"></a>2 Higher accuracy</h4><p>随之而来的是对泛化性能的极大改进。在ImageNet验证集上，SAM将ViT-B/16的top-1精度从74.6%提高到79.9%，将Mixer-B/16的top-1精度从66.4%提高到77.4%。</p>
<p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210606/12.png" alt=""></p>
<p>相比之下，类似规模的ResNet-152的性能提高了0.8%。根据经验，<strong>改进的程度与架构中内置的归纳偏差水平呈负相关</strong>。与基于注意力的ViTs相比，具有inherent translation equivalence和locality的ResNets从landscape smoothing中获益较少。MLP-Mixers从平滑的loss geometry中获得最多。</p>
<p>此外，SAM对更大容量(例如:+4.1%的Mixer-S/16 vs. +11.0%的Mixer-B/16)和更长的patch序列(例如:+2.1%的vits/32 vs. +5.3%的vits /8)的模型带来了更大的改进。</p>
<h4 id="3-Better-robustness"><a href="#3-Better-robustness" class="headerlink" title="3 Better robustness"></a>3 Better robustness</h4><p>作者还使用ImageNet-R和ImageNetC评估了模型的鲁棒性，并发现了smoothed loss landscapes的更大影响。在ImageNet-C上，它通过噪音、恶劣天气、模糊等来破坏图像，实验了5种严重程度上19种破坏的平均精度。如表1和表2所示， ViT-B/16和Mixer-B/16的精度分别增加了9.9%和15.0%。</p>
<h3 id="4-3-无需预训练或强大的数据增强ViTs优于ResNets"><a href="#4-3-无需预训练或强大的数据增强ViTs优于ResNets" class="headerlink" title="4.3 无需预训练或强大的数据增强ViTs优于ResNets"></a>4.3 无需预训练或强大的数据增强ViTs优于ResNets</h3><p>模型体系结构的性能通常与训练策略合并，其中数据增强起着关键作用。然而，数据增广的设计需要大量的领域专业知识，而且可能无法在图像和视频之间进行转换。由于有了锐度感知优化器SAM，可以删除高级的数据增强，并专注于体系结构本身(使用基本的Inception-style的预处理)。</p>
<p>当使用SAM在ImageNet上从0开始训练时，ViT的准确性(在ImageNet、ImageNet-Real和ImageNet V2上)和健壮性(在ImageNet-R和ImageNet-R上)方面都优于类似和更大的ResNet(在推理时也具有相当的吞吐量)。</p>
<p>ViT-B/16在ImageNet、ImageNet-r和ImageNet-C上分别达到79.9%、26.4%和56.6%的top精度，而对应的ResNet-152则分别达到79.3%、25.7%和52.2%(见表2)。对于小型架构，vit和resnet之间的差距甚至更大。<br>在ImageNet上，ViT-S/16的表现比同样大小的ResNet-50好1.4%，在ImageNet-C上好6.5%。SAM还显著改善了MLP-Mixers的结果。</p>
<h3 id="4-4-SAM后的内在变化"><a href="#4-4-SAM后的内在变化" class="headerlink" title="4.4 SAM后的内在变化"></a>4.4 SAM后的内在变化</h3><p>作者对模型进行了更深入的研究，以理解它们如何从本质上改变以减少Hessian的特征值$\lambda_{max}$以及除了增强泛化之外的变化意味着什么。</p>
<h4 id="结论1：每个网络组件具有Smoother-loss-landscapes"><a href="#结论1：每个网络组件具有Smoother-loss-landscapes" class="headerlink" title="结论1：每个网络组件具有Smoother loss landscapes"></a>结论1：每个网络组件具有Smoother loss landscapes</h4><p>在表3中，将整个体系结构的Hessian分解成与每一组参数相关的小的斜对角Hessian块，试图分析在没有SAM训练的模型中，是什么特定的成分导致$\lambda_{max}$爆炸。</p>
<p>作者观察到较浅的层具有较大的Hessian特征值$\lambda_{max}$，并且第1个linear embedding layer产生sharpest的几何形状。</p>
<p>此外，ViTs中的多头自注意(MSA)和MLP-Mixers中的token mlp(Token mlp)跨空间位置混合信息，其$\lambda<em>{max}$相对较低。SAM一致地降低了所有网络块的$\lambda</em>{max}$。</p>
<p>可以通过递归mlp的Hessian矩阵得到上述发现。设$h_k$和$a_k$分别为第k层激活前的值和激活后的值。它们满足$h_k=W_ka_k−1,a_k=f_k(h_k)$，其中$W_k$为权值矩阵，$f_k$为激活函数(mlp-mixer中的GELU)。为了简单起见，在这里省略偏置项。Hessian矩阵$H_k$相对于$W_k$的对角块可递归计算为:</p>
<p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210606/19.png" width = "500" align=center /></p>
<p>其中⊗为Kronecker product，$H<em>k$为第$k$层的预激活Hessian，L为目标函数。因此，当递归公式反向传播到浅层时，Hessian范数累积，这也解释了为什么表3中第一个块的$\lambda</em>{max}$比最后一个块大得多。</p>
<p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210606/13.png" alt=""></p>
<h4 id="结论2：Greater-weight-norms"><a href="#结论2：Greater-weight-norms" class="headerlink" title="结论2：Greater weight norms"></a>结论2：Greater weight norms</h4><p>应用SAM后，作者发现激活后的值$a<em>{k−1}$的范数和权重$W</em>{k+1}$的范数变得更大(见表3)，说明常用的权重衰减可能不能有效地正则化ViTs和MLP-Mixers。</p>
<h4 id="结论3：MLP-Mixers中较稀疏的active-neurons"><a href="#结论3：MLP-Mixers中较稀疏的active-neurons" class="headerlink" title="结论3：MLP-Mixers中较稀疏的active neurons"></a>结论3：MLP-Mixers中较稀疏的active neurons</h4><p>根据递归公式(3)到(4)，作者确定了另一个影响Hessian的MLP-Mixers的内在度量:激活神经元的数量。</p>
<p>事实上，$B_k$是由大于零的被激活神经元决定的，因为当输入为负时，GELU的一阶导数变得非常小。因此，活跃的GELU神经元的数量直接与Hessian规范相连。</p>
<p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210606/14.png" alt=""></p>
<p>图2(右)显示了每个块中被激活的神经元的比例，使用ImageNet训练集的10%进行计算。可以看到，SAM极大地减少了前几层被激活神经元的比例，使它们处于更稀疏的状态。这一结果也说明了图像patch的潜在冗余性。</p>
<h4 id="结论4：ViTs的active-neurons高度稀疏"><a href="#结论4：ViTs的active-neurons高度稀疏" class="headerlink" title="结论4：ViTs的active neurons高度稀疏"></a>结论4：ViTs的active neurons高度稀疏</h4><p>虽然公式(3)和(4)只涉及mlp，但仍然可以观察到vit的第1层激活神经元的减少(但不如MLP-Mixers显著)。更有趣的是，作者发现ViT中被激活神经元的比例比ResNets或MLP-Mixers中要小得多——在大多数ViT层中，只有不到5%的神经元的值大于零。换句话说，ViT为网络修剪提供了巨大的潜力。</p>
<p>这种稀疏性也可以解释<strong>为什么一个Transformer可以处理多模态信号(视觉、文本和音频)?</strong></p>
<h4 id="结论5：ViTs中有更多的感知注意力Maps"><a href="#结论5：ViTs中有更多的感知注意力Maps" class="headerlink" title="结论5：ViTs中有更多的感知注意力Maps"></a>结论5：ViTs中有更多的感知注意力Maps</h4><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210606/15.png" alt=""></p>
<p>在图3中可视化了classification token的attention map。有趣的是，经过SAM优化的ViT模型能够编码合理的分割信息，比传统SGD优化训练的模型具有更好的可解释性。</p>
<h4 id="结论6：Higher-training-errors"><a href="#结论6：Higher-training-errors" class="headerlink" title="结论6：Higher training errors"></a>结论6：Higher training errors</h4><p>如图2(左)所示，使用SAM的ViT-B/16比使用vanilla SGD的训练误差更高。当在训练中使用强数据增强时，这种正则化效应也会发生，它迫使网络显式地学习RandAugment中的旋转平移等方差和mixup中的线性插值等先验。然而，增益对不同的训练设置很敏感(第5.2节)，并导致高噪声损失曲线(图2(中间))。</p>
<p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210606/16.png" alt=""></p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>具有smoother loss geometry的ViTs和MLP-Mixers可以更好地迁移到下游任务。</p>
<p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210606/17.png" alt=""></p>
<p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210606/18.png" alt=""></p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>[1].When Vision Transformers Outperform ResNets without Pretraining or Strong Data Augmentations.<br></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">ChaucerG</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://chaucerg.github.io/2021/09/05/7%20%E8%AF%A6%E7%BB%86%E8%A7%A3%E8%AF%BBTransformer%E6%80%8E%E6%A0%B7%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%B9%B6%E8%B6%85%E8%B6%8AResNet/">https://chaucerg.github.io/2021/09/05/7%20%E8%AF%A6%E7%BB%86%E8%A7%A3%E8%AF%BBTransformer%E6%80%8E%E6%A0%B7%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%B9%B6%E8%B6%85%E8%B6%8AResNet/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://chaucerg.github.io" target="_blank">集智书童</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/ResNet/">ResNet</a><a class="post-meta__tags" href="/tags/Transformer/">Transformer</a><a class="post-meta__tags" href="/tags/Tricks/">Tricks</a><a class="post-meta__tags" href="/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/">图像分类</a></div><div class="post_share"><div class="social-share" data-image="/img/achieve_img.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button button--animated"><i class="fas fa-qrcode"></i> 打赏</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/wechat.png" target="_blank"><img class="post-qr-code-img" src="/img/wechat.png" alt="微信"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="/img/alipay.png" target="_blank"><img class="post-qr-code-img" src="/img/alipay.png" alt="支付宝"/></a><div class="post-qr-code-desc">支付宝</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2021/09/05/6%20%E5%A4%9A%E5%9F%9F%E8%87%AA%E9%80%82%E5%BA%94MSDA-YOLO%E8%A7%A3%E8%AF%BB%EF%BC%8C%E6%81%B6%E5%8A%A3%E5%A4%A9%E6%B0%94%E4%B9%9F%E7%9C%8B%E5%BE%97%E8%A7%81/"><img class="prev-cover" src="/img/achieve_img.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">多域自适应MSDA-YOLO解读，恶劣天气也看得见</div></div></a></div><div class="next-post pull-right"><a href="/2021/09/05/8%20%E5%8D%B7%E7%A7%AF%E4%B8%8ESelf-Attention%E5%AE%8C%E7%BE%8E%E8%9E%8D%E5%90%88X-volution%E6%8F%92%E5%85%A5CV%E6%A8%A1%E5%9E%8B%E5%B0%86%E5%B8%A6%E6%9D%A5%E5%85%A8%E4%BB%BB%E5%8A%A1%E7%9A%84%E6%B6%A8%E7%82%B9/"><img class="next-cover" src="/img/achieve_img.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">卷积与Self-Attention完美融合X-volution插入CV模型将带来全任务的涨点</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> 相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2021/09/05/1 在ResNet与Transformer均适用的Skip Connection解读/" title="在ResNet与Transformer均适用的Skip Connection解读"><img class="cover" src="/img/achieve_img.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-09-05</div><div class="title">在ResNet与Transformer均适用的Skip Connection解读</div></div></a></div><div><a href="/2021/09/18/10/" title="太顶流！Scaling ViT将ImageNet Top-1 Acc刷到90.45%！"><img class="cover" src="/img/achieve_img.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-09-18</div><div class="title">太顶流！Scaling ViT将ImageNet Top-1 Acc刷到90.45%！</div></div></a></div><div><a href="/2021/09/18/11/" title="详细解读：如何再CNN模型中插入Transformer后速度不变精度剧增？"><img class="cover" src="/img/achieve_img.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-09-18</div><div class="title">详细解读：如何再CNN模型中插入Transformer后速度不变精度剧增？</div></div></a></div><div><a href="/2021/09/23/17/" title="85FPS！CNN+Transformer语义分割的又一境界，真的很快！"><img class="cover" src="/img/achieve_img.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-09-23</div><div class="title">85FPS！CNN+Transformer语义分割的又一境界，真的很快！</div></div></a></div><div><a href="/2021/09/05/2 详细解读Transformer那些有趣的特性/" title="详细解读Transformer那些有趣的特性"><img class="cover" src="/img/achieve_img.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-09-05</div><div class="title">详细解读Transformer那些有趣的特性</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="gitalk-container"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">ChaucerG</div><div class="author-info__description">干就完了！</div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">17</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">29</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/ChaucerG"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/ChaucerG" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:chaucer_g@126.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="/img/weixin.jpg" target="_blank" title="Wechat"><i class="fab fa-weixin"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">更多内容可以关注【集智书童】公众号和【集智书童】知识星球，获取原创文章以及项目源码 ~</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AE%80%E4%BB%8B"><span class="toc-number">1.</span> <span class="toc-text">简介</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Background%E5%92%8CRelated-Work"><span class="toc-number">2.</span> <span class="toc-text">Background和Related Work</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ViTs%E5%92%8CMLP-Mixers%E6%94%B6%E6%95%9B%E5%88%B0%E9%94%90%E5%B1%80%E9%83%A8%E6%9E%81%E5%B0%8F%E5%80%BC"><span class="toc-number">3.</span> <span class="toc-text">ViTs和MLP-Mixers收敛到锐局部极小值</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-ViTs%E5%92%8CMLP-Mixers%E6%94%B6%E6%95%9B%E5%88%B0%E6%9E%81sharp%E5%B1%80%E9%83%A8%E6%9E%81%E5%B0%8F%E5%80%BC"><span class="toc-number">3.1.</span> <span class="toc-text">3.1 ViTs和MLP-Mixers收敛到极sharp局部极小值</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-Small-training-errors"><span class="toc-number">3.2.</span> <span class="toc-text">3.2 Small training errors</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-ViTs%E5%92%8CMLP-Mixers%E7%9A%84%E5%8F%AF%E8%AE%AD%E7%BB%83%E6%80%A7%E8%BE%83%E5%B7%AE"><span class="toc-number">3.3.</span> <span class="toc-text">3.3 ViTs和MLP-Mixers的可训练性较差</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#CNN-Free%E8%A7%86%E8%A7%89%E6%9E%B6%E6%9E%84%E4%BC%98%E5%8C%96%E5%99%A8%E5%8E%9F%E7%90%86"><span class="toc-number">4.</span> <span class="toc-text">CNN-Free视觉架构优化器原理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-SAM-Overview"><span class="toc-number">4.1.</span> <span class="toc-text">4.1 SAM:Overview</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-SAM%E4%BC%98%E5%8C%96%E5%99%A8%E5%AE%9E%E8%B4%A8%E4%B8%8A%E6%94%B9%E8%BF%9B%E4%BA%86ViTs%E5%92%8CMLP-Mixers"><span class="toc-number">4.2.</span> <span class="toc-text">4.2 SAM优化器实质上改进了ViTs和MLP-Mixers</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E5%B1%80%E9%83%A8%E6%9E%81%E5%B0%8F%E5%80%BC%E5%91%A8%E5%9B%B4%E7%9A%84%E5%B9%B3%E6%BB%91%E5%8C%BA%E5%9F%9F"><span class="toc-number">4.2.1.</span> <span class="toc-text">1 局部极小值周围的平滑区域</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-Higher-accuracy"><span class="toc-number">4.2.2.</span> <span class="toc-text">2 Higher accuracy</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-Better-robustness"><span class="toc-number">4.2.3.</span> <span class="toc-text">3 Better robustness</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-%E6%97%A0%E9%9C%80%E9%A2%84%E8%AE%AD%E7%BB%83%E6%88%96%E5%BC%BA%E5%A4%A7%E7%9A%84%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BAViTs%E4%BC%98%E4%BA%8EResNets"><span class="toc-number">4.3.</span> <span class="toc-text">4.3 无需预训练或强大的数据增强ViTs优于ResNets</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-SAM%E5%90%8E%E7%9A%84%E5%86%85%E5%9C%A8%E5%8F%98%E5%8C%96"><span class="toc-number">4.4.</span> <span class="toc-text">4.4 SAM后的内在变化</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BB%93%E8%AE%BA1%EF%BC%9A%E6%AF%8F%E4%B8%AA%E7%BD%91%E7%BB%9C%E7%BB%84%E4%BB%B6%E5%85%B7%E6%9C%89Smoother-loss-landscapes"><span class="toc-number">4.4.1.</span> <span class="toc-text">结论1：每个网络组件具有Smoother loss landscapes</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BB%93%E8%AE%BA2%EF%BC%9AGreater-weight-norms"><span class="toc-number">4.4.2.</span> <span class="toc-text">结论2：Greater weight norms</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BB%93%E8%AE%BA3%EF%BC%9AMLP-Mixers%E4%B8%AD%E8%BE%83%E7%A8%80%E7%96%8F%E7%9A%84active-neurons"><span class="toc-number">4.4.3.</span> <span class="toc-text">结论3：MLP-Mixers中较稀疏的active neurons</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BB%93%E8%AE%BA4%EF%BC%9AViTs%E7%9A%84active-neurons%E9%AB%98%E5%BA%A6%E7%A8%80%E7%96%8F"><span class="toc-number">4.4.4.</span> <span class="toc-text">结论4：ViTs的active neurons高度稀疏</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BB%93%E8%AE%BA5%EF%BC%9AViTs%E4%B8%AD%E6%9C%89%E6%9B%B4%E5%A4%9A%E7%9A%84%E6%84%9F%E7%9F%A5%E6%B3%A8%E6%84%8F%E5%8A%9BMaps"><span class="toc-number">4.4.5.</span> <span class="toc-text">结论5：ViTs中有更多的感知注意力Maps</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BB%93%E8%AE%BA6%EF%BC%9AHigher-training-errors"><span class="toc-number">4.4.6.</span> <span class="toc-text">结论6：Higher training errors</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C"><span class="toc-number">5.</span> <span class="toc-text">实验</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83"><span class="toc-number">6.</span> <span class="toc-text">参考</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2021/09/23/17/" title="85FPS！CNN+Transformer语义分割的又一境界，真的很快！"><img src="/img/achieve_img.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="85FPS！CNN+Transformer语义分割的又一境界，真的很快！"/></a><div class="content"><a class="title" href="/2021/09/23/17/" title="85FPS！CNN+Transformer语义分割的又一境界，真的很快！">85FPS！CNN+Transformer语义分割的又一境界，真的很快！</a><time datetime="2021-09-23T14:27:19.000Z" title="发表于 2021-09-23 22:27:19">2021-09-23</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/09/18/16/" title="教你How to train自己的Transformer模型"><img src="/img/achieve_img.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="教你How to train自己的Transformer模型"/></a><div class="content"><a class="title" href="/2021/09/18/16/" title="教你How to train自己的Transformer模型">教你How to train自己的Transformer模型</a><time datetime="2021-09-18T09:20:49.000Z" title="发表于 2021-09-18 17:20:49">2021-09-18</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/09/18/15/" title="AdaConv自适应卷积让你的GAN比AdaIN更看重细节"><img src="/img/achieve_img.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="AdaConv自适应卷积让你的GAN比AdaIN更看重细节"/></a><div class="content"><a class="title" href="/2021/09/18/15/" title="AdaConv自适应卷积让你的GAN比AdaIN更看重细节">AdaConv自适应卷积让你的GAN比AdaIN更看重细节</a><time datetime="2021-09-18T09:15:38.000Z" title="发表于 2021-09-18 17:15:38">2021-09-18</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/09/18/14/" title="详细解读：HP-x激活函数"><img src="/img/achieve_img.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="详细解读：HP-x激活函数"/></a><div class="content"><a class="title" href="/2021/09/18/14/" title="详细解读：HP-x激活函数">详细解读：HP-x激活函数</a><time datetime="2021-09-18T09:07:25.000Z" title="发表于 2021-09-18 17:07:25">2021-09-18</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/09/18/13/" title="详细解读Attention-Based方法解决遮挡人脸识别问题"><img src="/img/achieve_img.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="详细解读Attention-Based方法解决遮挡人脸识别问题"/></a><div class="content"><a class="title" href="/2021/09/18/13/" title="详细解读Attention-Based方法解决遮挡人脸识别问题">详细解读Attention-Based方法解决遮挡人脸识别问题</a><time datetime="2021-09-18T09:00:12.000Z" title="发表于 2021-09-18 17:00:12">2021-09-18</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('/img/achieve_img.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2021 By ChaucerG</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Welcome to my CHANNEL!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (true){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><script src="/js/search/local-search.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())</script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.css"><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script><script>function addGitalkSource () {
  const ele = document.createElement('link')
  ele.rel = 'stylesheet'
  ele.href= 'https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css'
  document.getElementsByTagName('head')[0].appendChild(ele)
}

function loadGitalk () {
  function initGitalk () {
    var gitalk = new Gitalk(Object.assign({
      clientID: 'c2f87403b1eb48242fd0',
      clientSecret: '2710e87d30566c624d462848d26c6ecfaa287296',
      repo: 'blog-comments',
      owner: 'ChaucerG',
      admin: ['ChaucerG'],
      id: 'dde2dd1a8fc1550fe7c762287cf4b195',
      language: 'zh-CN',
      perPage: 10,
      distractionFreeMode: false,
      pagerDirection: 'last',
      createIssueManually: false,
      updateCountCallback: commentCount
    },null))

    gitalk.render('gitalk-container')
  }

  if (typeof Gitalk === 'function') initGitalk()
  else {
    addGitalkSource()
    getScript('https://cdn.jsdelivr.net/npm/gitalk@latest/dist/gitalk.min.js').then(initGitalk)
  }
}

function commentCount(n){
  let isCommentCount = document.querySelector('#post-meta .gitalk-comment-count')
  if (isCommentCount) {
    isCommentCount.innerHTML= n
  }
}

if ('Gitalk' === 'Gitalk' || !true) {
  if (true) btf.loadComment(document.getElementById('gitalk-container'), loadGitalk)
  else loadGitalk()
} else {
  function loadOtherComment () {
    loadGitalk()
  }
}</script></div><script defer="defer" id="fluttering_ribbon" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-fluttering-ribbon.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>