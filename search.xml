<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>卷积与Self-Attention完美融合X-volution插入CV模型将带来全任务的涨点</title>
      <link href="/2021/09/05/%E5%8D%B7%E7%A7%AF%E4%B8%8ESelf-Attention%E5%AE%8C%E7%BE%8E%E8%9E%8D%E5%90%88X-volution%E6%8F%92%E5%85%A5CV%E6%A8%A1%E5%9E%8B%E5%B0%86%E5%B8%A6%E6%9D%A5%E5%85%A8%E4%BB%BB%E5%8A%A1%E7%9A%84%E6%B6%A8%E7%82%B9/"/>
      <url>/2021/09/05/%E5%8D%B7%E7%A7%AF%E4%B8%8ESelf-Attention%E5%AE%8C%E7%BE%8E%E8%9E%8D%E5%90%88X-volution%E6%8F%92%E5%85%A5CV%E6%A8%A1%E5%9E%8B%E5%B0%86%E5%B8%A6%E6%9D%A5%E5%85%A8%E4%BB%BB%E5%8A%A1%E7%9A%84%E6%B6%A8%E7%82%B9/</url>
      
        <content type="html"><![CDATA[<p><br></p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210607/1.png" alt=""></p><blockquote><p>本文建立了一个由卷积和self-attention组成的多分支基本模块，能够统一局部和非局部特征交互，然后可以结构重新参数化为一个纯卷积风格的算子：X-volution，即插即用！可助力分类、检测和分割任务的涨点！<br><strong>作者单位</strong>：上海交通大学(倪冰冰团队), 华为海思</p></blockquote><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>卷积和self-attention是深度神经网络中的2个基本构建块，前者以线性方式提取图像的局部特征，而后者通过非局部关系编码高阶上下文关系。尽管本质上是相互补充的，即一阶/高阶、最先进的架构，但是，CNN或Transformer均缺乏一种原则性的方法来在单个计算模块中同时应用这2种操作，因为它们的异构计算视觉任务的全局点积的模式和过度负担。</p><p>在这项工作中，作者从理论上推导出一种全局self-attention近似方案，该方案通过对变换特征的卷积运算来近似self-attention。基于近似方案建立了一个由卷积和self-attention操作组成的多分支基本模块，能够统一局部和非局部特征交互。重要的是，一旦经过训练，这个多分支模块可以通过结构重新参数化有条件地转换为单个标准卷积操作，呈现一个名为X-volution的纯卷积风格的算子，准备作为atomic操作插入任何现代网络。大量实验表明，所提出的X-volution实现了极具竞争力的视觉理解改进（ImageNet分类的top-1准确率+1.2%，COCO 检测和分割的+1.7box AP和+1.5mask AP）。</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>本文提出了一种新颖的原子算子<strong>X-volution</strong>，将基本卷积算子和self-attention算子集成到一个统一的计算块中，期望从<strong>局部vs非局部</strong>/<strong>线性vs非线性</strong>两方面获得令人印象深刻的性能改进。</p><p><strong>首先</strong>，回顾卷积和self-attention的基本数学公式；</p><p><strong>然后</strong>，解读全局self-attention近似方案，它可以直接转换为一个兼容的卷积模式。</p><p><strong>最后</strong>，解释在推断阶段如何有条件地合并卷积分支和所提出的self-attention近似到单个卷积风格原子操作符。</p><h3 id="2-1-回顾卷积和self-attention"><a href="#2-1-回顾卷积和self-attention" class="headerlink" title="2.1 回顾卷积和self-attention"></a>2.1 回顾卷积和self-attention</h3><p>这2个算子想必大家已经非常熟悉了，这里就简单的说一下哈！！！</p><h4 id="卷积Module"><a href="#卷积Module" class="headerlink" title="卷积Module"></a>卷积Module</h4><p>卷积算子是用于构建卷积神经网络(CNN)的基本算子，它通过有限局部区域内的线性加权来估计输出。给定一个特征张量$X\in R^{C_i×H×W}$, $C_i$表示输入通道的数量，H是高度，W是宽度。卷积算子的估计结果$Y\in R^{C_o×H×W}$由以下公式定义:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210607/2.png" width = "500" align=center /></p><p>其中$C<em>o$为输出通道数。$w\in R^{C_o×C_i×K×K}$为卷积核，$W</em>{c_o,c_i,δ_i+[K/2],δ_j+[K/2]}$为特定位置核标量值。$K$为卷积kernel大小，$B\in R^{C_o}$为偏差向量，$∆k\in Z^2$为$K × K$卷积kernel中所有可能偏移的集合。</p><h4 id="Self-Attention-Module"><a href="#Self-Attention-Module" class="headerlink" title="Self-Attention Module"></a>Self-Attention Module</h4><p>与卷积不同，self-attention不能直接处理图像张量，首先将输入特征张量reshape为向量$X\in R^{C×L}$。$L$表示向量的长度，$L=H×W$。$W^Q、W^K、W^V$分别表示Query、Key、Value的嵌入变换，是空间共享的线性变换。Self-Attention的定义如下:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210607/3.png" width = "500" align=center /></p><p>其中$\overline W(X)$表示最终的Self-Attention等价系数矩阵，可以认为是一个动态和空间变化的卷积kernel。</p><h3 id="2-2-全局self-attention近似方案"><a href="#2-2-全局self-attention近似方案" class="headerlink" title="2.2 全局self-attention近似方案"></a>2.2 全局self-attention近似方案</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210607/4.png" alt=""></p><p>全局自注意是最原始的attention方案，它得益于全局范围的优势而具有优异的性能。然而，它的复杂度太大了$O(n^2)(n表示总像素数)$使得其在CV任务中的应用受到严重限制。关键问题是<strong>能否在公式2中推导出$\overline W(X)$的适当近似结果，即能否找到$\overline W(X)$的兼容计算模式，即能否找到卷积、single element-wise product等现成的算子替代?</strong></p><p>在本部分中，作者展示了在简单的element-wise shift和dot-product之后，可以用卷积的形式近似全局self-attention算子。给定特征张量$X$中的一个位置，将其特征向量表示为$x_0$，其attention logit $s_0$可以写成如下公式:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210607/5.png" width = "500" align=center /></p><p>其中$\alpha _t = w^pw^qw^vx_t$， Ω为全局区域，A为以x0为中心的局部区域。在图1的左边说明了局部区域和非局部区域。图中灰框表示输入特征X的全局区域，绿框表示以$x_0$为中心的局部区域。</p><p>另外，non-local区域是指局部区域以外的区域。因为图像具有很强的说服力（根据马尔可夫性质），$x<em>0$可以用像素在其局部区域近似线性表示:$x_0≈\sum</em>{x_k\in A˚}\beta _kx_k$，其中$\beta_k$为线性权值。代入式3中第2项，可得:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210607/6.png" width = "500" align=center /></p><p>在不失一般性的情况下，可以在区域A中加入系数为零的项。通过设计，non-local区域也在局部区域的边界像素的接受域内。因此可以将上式转化为:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210607/7.png" width = "400" align=center /></p><p>根据图像的马尔可夫性质，可以假设对于$x_k\in A$，远离$x_k$的$x_i$与$x_k$之间的相互作用是弱的。因此，可以进一步简化式上式:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210607/8.png" width = "400" align=center /></p><p>其中$U(x_k)$为$x_k$的局部区域。将上式代入Eq.3中的第2项，可以改写为:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210607/10.png" width = "500" align=center /></p><p>注意,$x<em>k,x_i$是$x_k$和$x_i$之间的内积，它衡量了$x_k$和$x_i$之间的相似性。$\sum</em>{x_i\in U(x_k)}\alpha_i\beta_k (x_k,x_i)$是$x_k$在其邻近区域的attention结果。因此，在$x_0$处的全局注意力logit可以通过加权求和其邻域内像素的attention结果来近似。</p><p>根据以上理解，可以设计一个近似算子，通过逐点上下文关系传播来估计全局attention。因此，作者提出了一个全局注意力近似方案，Pixel Shift<br>Self-Attention (PSSA)，基于像素偏移和卷积来近似全局attention。</p><p>具体来说，首先将特征映射沿给定的方向(即左、右、上等)移动L个像素，然后将原始特征与移动的特征进行元素积，得到变换后的特征。</p><p>实际上，shift-product操作建立了邻域内点之间的上下文关系，通过分层叠加可以将上下文关系传播到全局区域。最后，对这些变换后的特征进行加权求和(可以通过卷积算子实现)，得到一个近似的自注意力映射。平移、元素积和加权求和的复杂度为O(n)，因此提出的PSSA是一个时间复杂度为O(n)的算子。值得注意的是，PSSA实际上是将self-attention转换为对转换特征的标准卷积运算。该结构通过层次叠加进而通过上下文关系传播实现全局self-attention logit的估计。</p><h3 id="2-3-卷积和Self-Attention的统一-X-volution"><a href="#2-3-卷积和Self-Attention的统一-X-volution" class="headerlink" title="2.3 卷积和Self-Attention的统一: X-volution"></a>2.3 卷积和Self-Attention的统一: X-volution</h3><h4 id="卷积和Self-Attention是相辅相成的"><a href="#卷积和Self-Attention是相辅相成的" class="headerlink" title="卷积和Self-Attention是相辅相成的"></a>卷积和Self-Attention是相辅相成的</h4><p>卷积采用局域性和各向同性的归纳偏差，使其具有平移等方差的能力。然而，局部固有的特性使卷积无法建立形成图所必需的长期关系。</p><p>与卷积相反，<strong>Self-Attention摒弃了提到的归纳偏差，即所谓的低偏差，并从数据集中发现自然模式，而没有明确的模型假设。低偏差原则给予Self-Attention以探索复杂关系的自由(例如，长期依赖、各向异性语义、CNN中的强局部相关性等)，因此该方案通常需要对超大数据集进行预训练(如JFT-300M、ImageNet21K)</strong>。</p><p>此外，Self-Attention很难优化，需要更长的训练周期和复杂的Tricks。有文献提出将卷积引入Self-Attention以提高Self-Attention的鲁棒性和性能。简而言之，采用不同的模型假设，使卷积和Self-Attention在优化特征、注意范围(即局部/长期)和内容依赖(内容依赖/独立)等方面得到相互补充。</p><h4 id="统一的多分支拓扑"><a href="#统一的多分支拓扑" class="headerlink" title="统一的多分支拓扑"></a>统一的多分支拓扑</h4><p>有一些工作试图将卷积和self-attention结合起来，然而，粗糙的拓扑组合(例如，分层堆叠，级联)阻止他们获得单个原子操作(在同一个模块中应用卷积和注意)，使结构不规则。例如，AANet将经过卷积层和Self-Attention层处理的结果直接连接起来，得到组合结果。说明单一的卷积或单一的Self-Attention都会导致性能下降，当它们同时存在时，性能会有显著的提高。</p><p>在这个工作中，作者研究卷积和self-attention的数学原理后找到了近似形式。作者还观察到全局元素相互作用(点积)可以用局部元素相互作用的传播来近似表示。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210607/11.png" alt=""></p><p>因此，这2种算子可以用统一的计算模式来处理，即卷积。从另一个角度来看，卷积运算可以看作是Self-Attention的空间不变偏差。考虑到这一点，可以将算子组合成一个多分支拓扑，如图所示，这可以同时受益于卷积和Self-Attention。多分支模块由2个主要分支组成。左边的分支由级联的Shift Pixel Self-Attention和batch-normalization组成起到近似全局Self-Attention操作的作用，右分支被设计成由级联卷积和批归一化组成的卷积分支。</p><h4 id="有条件地将多分支方案转换为Atomic-X-volution"><a href="#有条件地将多分支方案转换为Atomic-X-volution" class="headerlink" title="有条件地将多分支方案转换为Atomic X-volution"></a>有条件地将多分支方案转换为Atomic X-volution</h4><p>多分支模块实现了卷积与Self-Attention的功能组合。然而，它只是一种粗粒度的算子组合，这将使网络高度复杂和不规则。从硬件实现的角度来看，多分支结构需要更多的缓存来服务于多路径的处理。相反，单个算子操作效率更高，内存开销更低，这是硬件友好的。</p><p>为了简单起见，在这里省略批标准化的公式。实际上，批归一化可以看作是一个$1×1$组卷积(其组等于channel数)，可以合并到卷积/Self-Attention层中。实际上，一般采用分层叠加的PSSA，堆叠结构中的加权运算可以省略，因为分层叠加隐含了加权邻接像素的运算。本文提出的多分支模块的训练阶段如下：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210607/12.png" width = "500" align=center /></p><p>其中$w^c$为卷积权值，$b^c$为其对应的偏置。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210607/13.png" width = "500" align=center /></p><p>其中$w^A(x_0,x_i)=w^qw^kw^v(x_0,x_i)$表示来自pixel shift self-attention 分支的content-dependent/dynamic coefficients。$W_c$表示从卷积分支继承的content-independent/static coefficients，训练完成后会修复。</p><p>观察上式可以发现，经过一个简单的变换，多分支结构可以转换成卷积形式。值得指出的是，这个过程在CNN中被广泛使用，被称为structural re-parameterization。在这里首先把它扩展到卷积和self-attention的合并。根据上式将由卷积和self-attention组成的多分支模块等价地转换为一个动态卷积算子X-voultion。</p><p>请注意，这里建议X-volution可以作为一个原子操作插入主流网络(例如，ResNet)。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="3-1-图像分类"><a href="#3-1-图像分类" class="headerlink" title="3.1 图像分类"></a>3.1 图像分类</h3><h4 id="架构设计"><a href="#架构设计" class="headerlink" title="架构设计"></a>架构设计</h4><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210607/14.png" width = "500" align=center /><br><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210607/15.png" width = "400" align=center /></p><p>结果表明，第3阶段的替换效果最好，ResNet-34的top-1准确率为+1.2%，ResNet-50的top-1准确率为+0.9%。作者怀疑第4阶段替换的性能较差ResNet-50可以归因于可学习参数的增加，这减慢了网络的收敛。</p><h3 id="3-2-目标检测"><a href="#3-2-目标检测" class="headerlink" title="3.2 目标检测"></a>3.2 目标检测</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210607/16.png" alt=""></p><p>特别是，本文所提X-volution(SA)实现了最好的性能，与ResNet-50相比增加了+1.7boxes AP。通过结合低阶局部特征和高阶长依赖，所提出的X-volution算子比单独的卷积或自注意力算子具有更高的精度。</p><p>结果表明，图完备原子算符有助于视觉理解，而现有的计算算符忽略了这一性质。此外，基于PSSA的X-volution也取得了与X-volution(SA)相当的性能，表明在X-volution模块中，近似效果良好，对硬件实现和计算更加友好。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210607/17.png" alt=""></p><h3 id="3-3-语义分割"><a href="#3-3-语义分割" class="headerlink" title="3.3 语义分割"></a>3.3 语义分割</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210607/19.png" alt=""></p><p>可以观察到，作者提出的X-volution比其他算子的性能要好很多。其中，X-volution(SA)实现了41.1 box AP和37.2 mask AP。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210607/20.png" width = "400" align=center /></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>[1].X-volution: On the Unification of Convolution and Self-attention.<br></p>]]></content>
      
      
      <categories>
          
          <category> 卷积CNN </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 卷积 </tag>
            
            <tag> Self-Attention </tag>
            
            <tag> CV模型 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>详细解读Transformer怎样从零训练并超越ResNet</title>
      <link href="/2021/09/05/%E8%AF%A6%E7%BB%86%E8%A7%A3%E8%AF%BBTransformer%E6%80%8E%E6%A0%B7%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%B9%B6%E8%B6%85%E8%B6%8AResNet/"/>
      <url>/2021/09/05/%E8%AF%A6%E7%BB%86%E8%A7%A3%E8%AF%BBTransformer%E6%80%8E%E6%A0%B7%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%B9%B6%E8%B6%85%E8%B6%8AResNet/</url>
      
        <content type="html"><![CDATA[<p><br></p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210606/1.png" alt=""></p><blockquote><p>本文证明了在没有大规模预训练或强数据增广的情况下，在ImageNet上从头开始训练时，所得ViT的性能优于类似大小和吞吐量的ResNet！而且还拥有更敏锐的注意力图。<br><strong>作者单位</strong>：谷歌,UCLA</p></blockquote><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>Vision Transformers(ViTs)和MLPs标志着在用通用神经架构替换手动特征或归纳偏置方面的进一步努力。现有工作通过大量数据为模型赋能，例如大规模预训练和/或重复的强数据增广，并且还报告了与优化相关的问题（例如，对初始化和学习率的敏感性）。</p><p>因此，本文从损失几何的角度研究了ViTs和MLP-Mixer，旨在提高模型在训练和推理时的泛化效率。可视化和Hessian揭示了收敛模型极其敏感的局部最小值。</p><p>同时通过使用最近提出的<strong>锐度感知优化器</strong>提高平滑度，进而大大提高了ViT和MLP-Mixer在跨越监督、对抗、对比和迁移学习（例如，+5.3\% 和 +11.0\%）的各种任务上的准确性和鲁棒性使用简单的Inception进行预处理，ViT-B/16和Mixer-B/16在ImageNet上的准确率分别为Top-1）。</p><p>作者研究表明，改进的平滑度归因于前几层中较稀疏的活动神经元。在没有大规模预训练或强数据增强的情况下，在ImageNet上从头开始训练时，所得 ViT的性能优于类似大小和吞吐量的ResNet。同时还拥有更敏锐的注意力图。</p><h2 id="Background和Related-Work"><a href="#Background和Related-Work" class="headerlink" title="Background和Related Work"></a>Background和Related Work</h2><p>最近的研究发现，ViT中的self-attention对性能并不是至关重要的，因此出现了一些专门基于mlp的架构。这里作者以MLP-Mixer为例。MLP-Mixer与ViT共享相同的输入层;也就是说，它将一个图像分割成一系列不重叠的Patches/Toekns。然后，它在torkn mlp和channel mlp之间交替使用，其中前者允许来自不同空间位置的特征融合。</p><h2 id="ViTs和MLP-Mixers收敛到锐局部极小值"><a href="#ViTs和MLP-Mixers收敛到锐局部极小值" class="headerlink" title="ViTs和MLP-Mixers收敛到锐局部极小值"></a>ViTs和MLP-Mixers收敛到锐局部极小值</h2><p>目前的ViTs、mlp-mixer和相关的无卷积架构的训练方法很大程度上依赖于大量的预训练或强数据增强。它对数据和计算有很高的要求，并导致许多超参数需要调整。</p><p>现有的研究表明，当在ImageNet上从头开始训练时，如果不结合那些先进的数据增强，尽管使用了各种正则化技术(例如，权重衰减，Dropout等)ViTs的精度依然低于类似大小和吞吐量的卷积网络。同时在鲁棒性测试方面，vit和resnet之间也存在较大的差距。</p><p>此外，Chen等人发现，在训练vit时，梯度会出现峰值，导致精确度突然下降，Touvron等人也发现初始化和超参数对训练很敏感。这些问题其实都可以归咎于优化问题。</p><p>在本文中，作者研究了ViTs和mlp-mixer的损失情况，从优化的角度理解它们，旨在减少它们对大规模预训练或强数据增强的依赖。</p><h3 id="3-1-ViTs和MLP-Mixers收敛到极sharp局部极小值"><a href="#3-1-ViTs和MLP-Mixers收敛到极sharp局部极小值" class="headerlink" title="3.1 ViTs和MLP-Mixers收敛到极sharp局部极小值"></a>3.1 ViTs和MLP-Mixers收敛到极sharp局部极小值</h3><p>众所周知，当模型收敛到曲率小的平坦区域时模型会具有更好的泛化性能。在[36]之后，当resnet、vit和MLP-Mixers在ImageNet上使用基本的初始风格预处理从头开始训练时，作者绘制损失图：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210606/2.png" alt=""></p><p>如图1(a)到1(c)所示，ViTs和mlp-mixer比ResNets收敛到更清晰的区域。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210606/3.png" alt=""></p><p>在表1中，通过计算主要的Hessian特征值$\lambda<em>{max}$进一步验证了结果。ViT和MLP-Mixer的$\lambda</em>{max}$值比ResNet大一个数量级，并且MLP-Mixer的曲率在3种中是最大的(具体分析见4.4节)。</p><h3 id="3-2-Small-training-errors"><a href="#3-2-Small-training-errors" class="headerlink" title="3.2 Small training errors"></a>3.2 Small training errors</h3><p>这种向sharp区域的收敛与图2(左)所示的训练动态一致。尽管Mixer-B/16参数少于ViT-B/16(59M vs 87M)，同时它有一个小的训练误差，但测试性能还是比较差的，这意味着使用cross-token MLP学习的相互作用比ViTs’ self-attention机制更容易过度拟合。这种差异可能解释了mlp-mixer更容易陷入尖锐的局部最小值。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210606/4.png" alt=""></p><h3 id="3-3-ViTs和MLP-Mixers的可训练性较差"><a href="#3-3-ViTs和MLP-Mixers的可训练性较差" class="headerlink" title="3.3 ViTs和MLP-Mixers的可训练性较差"></a>3.3 ViTs和MLP-Mixers的可训练性较差</h3><p>此外，作者还发现ViTs和MLP-Mixers的可训练性较差，可训练性定义为通过梯度下降优化的网络的有效性。Xiao等人的研究表明，神经网络的可训练性可以用相关的神经切线核(NTK)的条件数来表征:</p><script type="math/tex; mode=display">Θ(x,x')= J(x)J(x')^T</script><p>其中$J$是雅可比矩阵。</p><p>用$\lambda<em>1≥··≥\lambda_m$表示NTK $Θ</em>{train}$的特征值，最小的特征值$\lambda_m$以条件数κ$=\lambda_1=\lambda_m$的速率指数收敛。如果κ是发散的，那么网络将变得不可训练。如表1所示，ResNets的κ是相当稳定的，这与之前的研究结果一致，即ResNets无论深度如何都具有优越的可训练性。然而，当涉及到ViT和时，条件数是不同的MLP-Mixer，证实了对ViTs的训练需要额外的辅助。</p><h2 id="CNN-Free视觉架构优化器原理"><a href="#CNN-Free视觉架构优化器原理" class="headerlink" title="CNN-Free视觉架构优化器原理"></a>CNN-Free视觉架构优化器原理</h2><p>常用的一阶优化器(如SGD,Adam)只寻求最小化训练损失。它们通常会忽略与泛化相关的高阶信息，如曲率。然而，深度神经网络的损失具有高度非凸性，在评估时容易达到接近0的训练误差，但泛化误差较高，更谈不上在测试集具有不同分布时的鲁棒性。</p><p>由于对视觉数据缺乏归纳偏差ViTs和MLPs放大了一阶优化器的这种缺陷，导致过度急剧的损失scene和较差的泛化性能，如前一节所示。假设平滑收敛时的损失scene可以显著提高那些无卷积架构的泛化能力，那么最近提出的锐度感知最小化(SAM)可以很好的避免锐度最小值。</p><h3 id="4-1-SAM-Overview"><a href="#4-1-SAM-Overview" class="headerlink" title="4.1 SAM:Overview"></a>4.1 SAM:Overview</h3><p>从直觉上看，SAM寻找的是可以使整个邻近训练损失最低的参数w，训练损失$L_{train}$通过构造极小极大目标:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210606/5.png" width = "350" align=center /></p><p>其中$\rho$是neighbourhood ball的大小。在不失一般性的情况下，这里使用$l_2$范数作为其强经验结果，这里为了简单起见省略了正则化项。</p><p>由于内部最大化下式的确切解很难获得：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210606/6.png" width = "350" align=center /></p><p>因此，这里采用了一个有效的一阶近似:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210606/7.png" width = "400" align=center /></p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210606/8.png" width = "350" align=center /></p><p>在$l_2$范数下，$\hat\epsilon(w)$是当前权值$w$的缩放梯度。计算$\hat\epsilon$后，SAM基于锐度感知梯度更新w。</p><h3 id="4-2-SAM优化器实质上改进了ViTs和MLP-Mixers"><a href="#4-2-SAM优化器实质上改进了ViTs和MLP-Mixers" class="headerlink" title="4.2 SAM优化器实质上改进了ViTs和MLP-Mixers"></a>4.2 SAM优化器实质上改进了ViTs和MLP-Mixers</h3><p>作者在没有大规模的预训练或强大的数据增强的情况下训练了vit和MLP-Mixers。直接将SAM应用于vit的原始ImageNet训练pipeline，而不改变任何超参数。<br>pipeline使用了基本的Inception-style的预处理。最初的mlp-mixer的训练设置包括强数据增强的组合;也用同样的Inception-style的预处理来替换它，以便进行公平的比较。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210606/9.png" alt=""></p><p>注意，在应用SAM之前，我们对学习速率、权重衰减、Dropout和随机深度进行网格搜索。</p><h4 id="1-局部极小值周围的平滑区域"><a href="#1-局部极小值周围的平滑区域" class="headerlink" title="1 局部极小值周围的平滑区域"></a>1 局部极小值周围的平滑区域</h4><p>由于SAM, ViTs和mlp-mixer都汇聚在更平滑的区域，如图1(d)和1(e)所示。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210606/10.png" alt=""></p><p>曲率测量，即Hessian矩阵的最大特征值$\lambda_{max}$，也减小到一个小值(见表1)。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210606/11.png" alt=""></p><h4 id="2-Higher-accuracy"><a href="#2-Higher-accuracy" class="headerlink" title="2 Higher accuracy"></a>2 Higher accuracy</h4><p>随之而来的是对泛化性能的极大改进。在ImageNet验证集上，SAM将ViT-B/16的top-1精度从74.6%提高到79.9%，将Mixer-B/16的top-1精度从66.4%提高到77.4%。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210606/12.png" alt=""></p><p>相比之下，类似规模的ResNet-152的性能提高了0.8%。根据经验，<strong>改进的程度与架构中内置的归纳偏差水平呈负相关</strong>。与基于注意力的ViTs相比，具有inherent translation equivalence和locality的ResNets从landscape smoothing中获益较少。MLP-Mixers从平滑的loss geometry中获得最多。</p><p>此外，SAM对更大容量(例如:+4.1%的Mixer-S/16 vs. +11.0%的Mixer-B/16)和更长的patch序列(例如:+2.1%的vits/32 vs. +5.3%的vits /8)的模型带来了更大的改进。</p><h4 id="3-Better-robustness"><a href="#3-Better-robustness" class="headerlink" title="3 Better robustness"></a>3 Better robustness</h4><p>作者还使用ImageNet-R和ImageNetC评估了模型的鲁棒性，并发现了smoothed loss landscapes的更大影响。在ImageNet-C上，它通过噪音、恶劣天气、模糊等来破坏图像，实验了5种严重程度上19种破坏的平均精度。如表1和表2所示， ViT-B/16和Mixer-B/16的精度分别增加了9.9%和15.0%。</p><h3 id="4-3-无需预训练或强大的数据增强ViTs优于ResNets"><a href="#4-3-无需预训练或强大的数据增强ViTs优于ResNets" class="headerlink" title="4.3 无需预训练或强大的数据增强ViTs优于ResNets"></a>4.3 无需预训练或强大的数据增强ViTs优于ResNets</h3><p>模型体系结构的性能通常与训练策略合并，其中数据增强起着关键作用。然而，数据增广的设计需要大量的领域专业知识，而且可能无法在图像和视频之间进行转换。由于有了锐度感知优化器SAM，可以删除高级的数据增强，并专注于体系结构本身(使用基本的Inception-style的预处理)。</p><p>当使用SAM在ImageNet上从0开始训练时，ViT的准确性(在ImageNet、ImageNet-Real和ImageNet V2上)和健壮性(在ImageNet-R和ImageNet-R上)方面都优于类似和更大的ResNet(在推理时也具有相当的吞吐量)。</p><p>ViT-B/16在ImageNet、ImageNet-r和ImageNet-C上分别达到79.9%、26.4%和56.6%的top精度，而对应的ResNet-152则分别达到79.3%、25.7%和52.2%(见表2)。对于小型架构，vit和resnet之间的差距甚至更大。<br>在ImageNet上，ViT-S/16的表现比同样大小的ResNet-50好1.4%，在ImageNet-C上好6.5%。SAM还显著改善了MLP-Mixers的结果。</p><h3 id="4-4-SAM后的内在变化"><a href="#4-4-SAM后的内在变化" class="headerlink" title="4.4 SAM后的内在变化"></a>4.4 SAM后的内在变化</h3><p>作者对模型进行了更深入的研究，以理解它们如何从本质上改变以减少Hessian的特征值$\lambda_{max}$以及除了增强泛化之外的变化意味着什么。</p><h4 id="结论1：每个网络组件具有Smoother-loss-landscapes"><a href="#结论1：每个网络组件具有Smoother-loss-landscapes" class="headerlink" title="结论1：每个网络组件具有Smoother loss landscapes"></a>结论1：每个网络组件具有Smoother loss landscapes</h4><p>在表3中，将整个体系结构的Hessian分解成与每一组参数相关的小的斜对角Hessian块，试图分析在没有SAM训练的模型中，是什么特定的成分导致$\lambda_{max}$爆炸。</p><p>作者观察到较浅的层具有较大的Hessian特征值$\lambda_{max}$，并且第1个linear embedding layer产生sharpest的几何形状。</p><p>此外，ViTs中的多头自注意(MSA)和MLP-Mixers中的token mlp(Token mlp)跨空间位置混合信息，其$\lambda<em>{max}$相对较低。SAM一致地降低了所有网络块的$\lambda</em>{max}$。</p><p>可以通过递归mlp的Hessian矩阵得到上述发现。设$h_k$和$a_k$分别为第k层激活前的值和激活后的值。它们满足$h_k=W_ka_k−1,a_k=f_k(h_k)$，其中$W_k$为权值矩阵，$f_k$为激活函数(mlp-mixer中的GELU)。为了简单起见，在这里省略偏置项。Hessian矩阵$H_k$相对于$W_k$的对角块可递归计算为:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210606/19.png" width = "500" align=center /></p><p>其中⊗为Kronecker product，$H<em>k$为第$k$层的预激活Hessian，L为目标函数。因此，当递归公式反向传播到浅层时，Hessian范数累积，这也解释了为什么表3中第一个块的$\lambda</em>{max}$比最后一个块大得多。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210606/13.png" alt=""></p><h4 id="结论2：Greater-weight-norms"><a href="#结论2：Greater-weight-norms" class="headerlink" title="结论2：Greater weight norms"></a>结论2：Greater weight norms</h4><p>应用SAM后，作者发现激活后的值$a<em>{k−1}$的范数和权重$W</em>{k+1}$的范数变得更大(见表3)，说明常用的权重衰减可能不能有效地正则化ViTs和MLP-Mixers。</p><h4 id="结论3：MLP-Mixers中较稀疏的active-neurons"><a href="#结论3：MLP-Mixers中较稀疏的active-neurons" class="headerlink" title="结论3：MLP-Mixers中较稀疏的active neurons"></a>结论3：MLP-Mixers中较稀疏的active neurons</h4><p>根据递归公式(3)到(4)，作者确定了另一个影响Hessian的MLP-Mixers的内在度量:激活神经元的数量。</p><p>事实上，$B_k$是由大于零的被激活神经元决定的，因为当输入为负时，GELU的一阶导数变得非常小。因此，活跃的GELU神经元的数量直接与Hessian规范相连。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210606/14.png" alt=""></p><p>图2(右)显示了每个块中被激活的神经元的比例，使用ImageNet训练集的10%进行计算。可以看到，SAM极大地减少了前几层被激活神经元的比例，使它们处于更稀疏的状态。这一结果也说明了图像patch的潜在冗余性。</p><h4 id="结论4：ViTs的active-neurons高度稀疏"><a href="#结论4：ViTs的active-neurons高度稀疏" class="headerlink" title="结论4：ViTs的active neurons高度稀疏"></a>结论4：ViTs的active neurons高度稀疏</h4><p>虽然公式(3)和(4)只涉及mlp，但仍然可以观察到vit的第1层激活神经元的减少(但不如MLP-Mixers显著)。更有趣的是，作者发现ViT中被激活神经元的比例比ResNets或MLP-Mixers中要小得多——在大多数ViT层中，只有不到5%的神经元的值大于零。换句话说，ViT为网络修剪提供了巨大的潜力。</p><p>这种稀疏性也可以解释<strong>为什么一个Transformer可以处理多模态信号(视觉、文本和音频)?</strong></p><h4 id="结论5：ViTs中有更多的感知注意力Maps"><a href="#结论5：ViTs中有更多的感知注意力Maps" class="headerlink" title="结论5：ViTs中有更多的感知注意力Maps"></a>结论5：ViTs中有更多的感知注意力Maps</h4><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210606/15.png" alt=""></p><p>在图3中可视化了classification token的attention map。有趣的是，经过SAM优化的ViT模型能够编码合理的分割信息，比传统SGD优化训练的模型具有更好的可解释性。</p><h4 id="结论6：Higher-training-errors"><a href="#结论6：Higher-training-errors" class="headerlink" title="结论6：Higher training errors"></a>结论6：Higher training errors</h4><p>如图2(左)所示，使用SAM的ViT-B/16比使用vanilla SGD的训练误差更高。当在训练中使用强数据增强时，这种正则化效应也会发生，它迫使网络显式地学习RandAugment中的旋转平移等方差和mixup中的线性插值等先验。然而，增益对不同的训练设置很敏感(第5.2节)，并导致高噪声损失曲线(图2(中间))。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210606/16.png" alt=""></p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>具有smoother loss geometry的ViTs和MLP-Mixers可以更好地迁移到下游任务。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210606/17.png" alt=""></p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210606/18.png" alt=""></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>[1].When Vision Transformers Outperform ResNets without Pretraining or Strong Data Augmentations.<br></p>]]></content>
      
      
      <categories>
          
          <category> Transformer </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ResNet </tag>
            
            <tag> Transformer </tag>
            
            <tag> Tricks </tag>
            
            <tag> 图像分类 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>多域自适应MSDA-YOLO解读，恶劣天气也看得见</title>
      <link href="/2021/09/05/%E5%A4%9A%E5%9F%9F%E8%87%AA%E9%80%82%E5%BA%94MSDA-YOLO%E8%A7%A3%E8%AF%BB%EF%BC%8C%E6%81%B6%E5%8A%A3%E5%A4%A9%E6%B0%94%E4%B9%9F%E7%9C%8B%E5%BE%97%E8%A7%81/"/>
      <url>/2021/09/05/%E5%A4%9A%E5%9F%9F%E8%87%AA%E9%80%82%E5%BA%94MSDA-YOLO%E8%A7%A3%E8%AF%BB%EF%BC%8C%E6%81%B6%E5%8A%A3%E5%A4%A9%E6%B0%94%E4%B9%9F%E7%9C%8B%E5%BE%97%E8%A7%81/</url>
      
        <content type="html"><![CDATA[<p><br></p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210604/1.png" alt=""></p><blockquote><p>本文介绍了一种新的多尺度域自适应YOLO(MS-DAYOLO)框架，该框架在YOLOv4检测器的不同尺度上使用多个域自适应路径和相应的域分类器来生成域不变特征。</p></blockquote><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>Domain Adaptation在解决许多应用中遇到的Domain Shift问题方面发挥了重要作用。这个问题的出现是由于用于训练的源数据的分布与实际测试场景中使用的目标数据之间存在差异。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210604/2.png" alt=""></p><p> 本文介绍了一种新的多尺度域自适应YOLO(MS-DAYOLO)框架，该框架在YOLOv4检测器的不同尺度上使用多个域自适应路径和相应的域分类器来生成域不变特征。实验表明，当使用本文提出的MS-DAYOLO训练YOLOv4时，以及在自动驾驶应用中具有挑战性的天气条件的目标数据上进行测试时，目标检测性能得到了显著改善。</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><h3 id="2-1-YOLO-V4简述"><a href="#2-1-YOLO-V4简述" class="headerlink" title="2.1 YOLO V4简述"></a>2.1 YOLO V4简述</h3><p>相对于YOLO V3，YOLOv4包含了许多新的改进和新技术，以提高整体检测精度。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210604/3.png" alt=""></p><p>如图所示YOLOv4有3个主要部分:backbone、neck和head。</p><p>backbone负责提取不同尺度下的多层特征。</p><p>neck使用上采样层将backbone的3种不同尺度的特征聚集在一起，并将它们输入head。</p><p>最后，head预测目标周围的边界框以及与每个边界框相关联的类别概率。</p><p>本文作者的目标是将域适应应用于这3个特征（图中的F1、F2、F3），使它们对不同尺度的域变化具有鲁棒性，从而使它们在基于域适应的训练中向域不变性收敛。</p><h3 id="2-2-Domain-Adaptive-Network-for-YOLO"><a href="#2-2-Domain-Adaptive-Network-for-YOLO" class="headerlink" title="2.2 Domain Adaptive Network for YOLO"></a>2.2 Domain Adaptive Network for YOLO</h3><p>提出的域自适应网络(DAN)仅在训练时附加到YOLOv4中以学习域不变特征。对于推理，在推理阶段，将使用原始的YOLOv4体系结构中使用领域自适应训练的权重(没有DAN网络)。因此，本文所提出的框架不会增加推理过程中底层检测器的复杂性。</p><p>DAN使用backbone的3个不同的尺度特征作为输入。它有几个卷积层来预测域类。然后，利用二元交叉熵计算域分类损失(Ldc):</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210604/4.png" width = "400" align=center /></p><p>这里$t_i$为第$i$个训练图像的ground truth域标签，其中$t_i = 1$为源域，$t_i = 0$为目标域。$P^{(x,y)}$是第$i$个训练图像在位置$(x,Y)$的特征图。</p><p>DAN通过最小化这种上述损失来区分源域和目标域。另一方面，为了最大限度地学习域不变特征，对主干也进行了优化。因此，对于这2个域，backbone的特征应该是难以区分的。因此，这将提高目标域的目标检测性能。</p><p>为了解决联合最小化和最大化问题，作者采用了对抗学习策略。通过在backbone网络和DAN网络之间使用梯度反转层(GRL)来实现这个矛盾的目标。</p><p>GRL是一个双向算子，用于实现2个不同的优化目标。在前向传播方向上，GRL作为恒等算子。这导致了在DAN内执行局部反向传播时最小化分类错误的标准目标。另一方面，向主干网络反向传播时，GRL变成一个负标量$(\lambda)$。因此，在这种情况下，它会导致最大的二分类错误，这种最大化促进了由backbone生成领域不变特征。</p><p>为了计算检测损失(ldt)，只使用源图像。因此，通过最小化ldt, YOLOv4的所有3个部分(即backbone, neck和head)都得到了优化。另一方面，利用源标记图像和目标未标记图像计算域分类损失(Ldc)，Ldc通过最小化来优化DAN, Ldc通过最大化来优化backbone。因此，Ldet和Ldc都被用来优化backbone。换句话说，通过最小化以下总损失，backbone被优化了：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210604/5.png" width = "300" align=center /></p><p>其中$(\lambda)$是GRL的一个负标量，用来平衡检测损失和域分类损失。事实上，$(\lambda)$是用来优化DAN对backbone的影响。</p><h3 id="2-3-DAN-Architecture"><a href="#2-3-DAN-Architecture" class="headerlink" title="2.3 DAN Architecture"></a>2.3 DAN Architecture</h3><p>与在Domain Adaptive Faster R-CNN架构中只对特征提取器的最终尺度应用域自适应不同，本文分别开发了3个尺度的域自适应来解决梯度消失问题。也就是说，只对最终的尺度(F3)进行域自适应，由于之前的尺度(F1和F2)之间有很多层，存在梯度消失的问题，因此对之前的尺度(F1和F2)没有显著影响。</p><p>因此，作者采用了一个多尺度策略，将主干的三个特征F1、F2和F3通过三个相应的grl连接到DAN，如图2所示。对于每个尺度，GRL之后有2个卷积层，第1个卷积层将特征通道减少一半，第2个卷积层预测域类概率。最后，利用域分类器层计算域分类损失。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="3-1-Clear-gt-Foggy"><a href="#3-1-Clear-gt-Foggy" class="headerlink" title="3.1 Clear=&gt;Foggy"></a>3.1 Clear=&gt;Foggy</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210604/6.png" width = "500" align=center /></p><p>从这些结果可以看出，将域自适应应用于所有3个特征尺度提高了目标域的检测性能，取得了最好的结果。此外，作者提出的MS-DAYOLO在性能上大大优于原来的YOLOv4方法，几乎达到了理想(oracle)场景的性能。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210604/7.png" alt=""></p><h3 id="3-2-Sunny-gt-Rainy"><a href="#3-2-Sunny-gt-Rainy" class="headerlink" title="3.2 Sunny=&gt;Rainy"></a>3.2 Sunny=&gt;Rainy</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210604/8.png" width = "500" align=center /></p><p>结果如表2所示。在2个数据集中，本文的方法都比原始的YOLO得到了明显的性能提升。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>[1].Multiscale Domain Adaptive YOLO for Cross-Domain Object Detection<br></p>]]></content>
      
      
      <categories>
          
          <category> YOLO </category>
          
      </categories>
      
      
        <tags>
            
            <tag> YOLO </tag>
            
            <tag> 目标检测 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>没有Attention的Transformer依然是顶流！！！</title>
      <link href="/2021/09/05/%E6%B2%A1%E6%9C%89Attention%E7%9A%84Transformer%E4%BE%9D%E7%84%B6%E6%98%AF%E9%A1%B6%E6%B5%81%EF%BC%81%EF%BC%81%EF%BC%81/"/>
      <url>/2021/09/05/%E6%B2%A1%E6%9C%89Attention%E7%9A%84Transformer%E4%BE%9D%E7%84%B6%E6%98%AF%E9%A1%B6%E6%B5%81%EF%BC%81%EF%BC%81%EF%BC%81/</url>
      
        <content type="html"><![CDATA[<p><br></p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210602/1.png" alt=""></p><blockquote><p>本文主要介绍了Attention Free Transformer(AFT)，同时作者还引入了AFT-local和AFT-Conv，这两个模型在保持全局连通性的同时，利用了局域性和空间权重共享的思想。通过实验验证了AFT在所有benchmarks上具有竞争性能的同时具有出色的效率。</p></blockquote><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>本文主要介绍了Attention Free Transformer(AFT)，在AFT层中，首先将key和value与一组学习到的位置偏差结合起来，然后以元素方式将其结果与query相乘。这个新的操作在context size和特征维度上都具有线性的内存复杂度，使得它能够兼容大的输入和模型大小。</p><p>作者还引入了AFT-local和AFT-Conv，这两个模型变种在保持全局连通性的同时还利用了局域性和空间权重共享的思想。作者对2个自回归建模任务(CIFAR10和Enwik8)以及一个图像识别任务(ImageNet-1K分类)进行了广泛的实验。验证了AFT在所有benchmarks上不仅具有不错的性能，同时还具有出色的效率。</p><h2 id="本文方法"><a href="#本文方法" class="headerlink" title="本文方法"></a>本文方法</h2><h3 id="2-1-Attention-Free-Transformer"><a href="#2-1-Attention-Free-Transformer" class="headerlink" title="2.1 Attention Free Transformer"></a>2.1 Attention Free Transformer</h3><p>首先，定义了Attention Free Transformer(AFT)，它是MHA的plugin replacement，不需要改变Transformer的其他架构。给定输入X, AFT先将它们线性变换为$Q=XW^Q$,$K=XW^K$,$V=XW^V$，然后执行以下操作:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210602/2.png" width = "500" align=center /></p><p>其中，$\bigodot$是元素的乘积;$\sigma_q$是应用于query的非线性映射，默认为sigmoid;$w\in R^{T\times T}$是学习到成对的位置偏差。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210602/3.png" width = "500" align=center /></p><p>换句话说，对于每个目标位置$t$, AFT把加权平均的结果与具有元素级乘法的query相结合。而加权操作则是由key和一组学习成对的位置偏差组成。这提供了一个直接的优势，即不需要计算和存储消耗大的注意力矩阵，同时能够像MHA那样维护query和value之间的全局交互。</p><p>为了进一步了解AFT与MHA的关系可以将方程改写为：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210602/4.png" width = "400" align=center /></p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210602/5.png" width = "200" align=center /></p><p>这里使用上标$i$来索引矩阵的特征维数。在这种重新排列的形式中，能够再次用注意力来表达AFT。具体来说，对于每个位置有一个关注向量$a_t^i\in R^T$，每个维度由$Q、K、w$组成。换句话说，AFT可以解释为与特征尺寸一样多的Head中进行implicit attention，其中注意力矩阵采用因数分解的形式进行求解。</p><h3 id="2-2-AFT-variants-locality-weight-sharing-and-parameterization"><a href="#2-2-AFT-variants-locality-weight-sharing-and-parameterization" class="headerlink" title="2.2 AFT variants: locality, weight sharing and parameterization"></a>2.2 AFT variants: locality, weight sharing and parameterization</h3><h4 id="1-AFT-full"><a href="#1-AFT-full" class="headerlink" title="1 AFT-full"></a>1 AFT-full</h4><p>将下面方程中定义的AFT的基本版本表示为AFT-full：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210602/6.png" width = "500" align=center /></p><h4 id="2-AFT-local"><a href="#2-AFT-local" class="headerlink" title="2 AFT-local"></a>2 AFT-local</h4><p>作者发现了训练的标准Transformers倾向于表现出广泛的局部注意力模式。具体地说，把ImagenetNet预训练Vision Transformer(ViT)，由12层组成，每层6个Head。为了实现可视化忽略分类标记，将每一层的注意力张量reshape为6×196×196(因为ViT特征图的空间大小为14×14)。然后从ImageNet验证集中采样256张图像。对于每一层和每一个Head，计算平均的average relative 2d attentions、averaged across position和images。这就产生了一组尺寸为12×6×27×27的注意力map（如下图）。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210602/7.png" alt=""></p><p>通过上图可以看到，相对注意力Map显示出强烈的局部模式，特别是在lower layers。这激发了AFT的一种变体，称为<strong>AFT-local</strong>，即只在局部应用一组学习到的相对位置偏差:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210602/8.png" width = "400" align=center /></p><p>这里s≤T是一个局部window size。AFT-local提供了进一步的计算量的节省，包括参数的数量和时间/空间复杂度。</p><h4 id="3-AFT-simple"><a href="#3-AFT-simple" class="headerlink" title="3 AFT-simple"></a>3 AFT-simple</h4><p>AFT-local的一个极端形式是当s=0时，即没有学习到位置偏差。这就产生了一个极其简单的AFT版本，<strong>AFT-simple</strong>，有:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210602/9.png" width = "300" align=center /></p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210602/10.png" width = "300" align=center /></p><p>在这个版本中，context reduction进一步简化为元素操作和全局池化。其实AFT-simple类似于线性化注意，公式为：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210602/11.png" width = "400" align=center /></p><p>然而，AFT-simple完全摆脱了点积操作，这促使复杂度从$O(Td^2)$降低为$O(Td)$。</p><h4 id="4-AFT-conv"><a href="#4-AFT-conv" class="headerlink" title="4 AFT-conv"></a>4 AFT-conv</h4><p>作者还可以进一步扩展局部化locality的思想，<strong>加入空间权值共享</strong>，即<strong>卷积</strong>。这种变体与视觉任务特别相关，因为它通常希望将一个预训练模型扩展到可变大小的输入。具体来说，让$w_{t,t’}$的值只依赖于$t$和$t’$, 而$w.r.t.$为在给定的空间网格(1d或2d)中的相对位置。与CNN类似也可以学习多组位置偏差(重用head的概念作为参考)。为了考虑到#parameters随着 #heads的增加而增长，作者还采用了一个设计，将K的维度与#heads联系起来。这使得AFT-conv可修改为依赖于深度可分离卷积、全局池化和元素操作来实现。</p><p>类似的尺寸的AFT-conv学习到的相对位置偏差。 </p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210602/12.png" alt=""></p><p>举一个例子，这里将模型构型表示为AFT-conv-h-s，其中h为head的个数，s×s为2d local window size。$w\in R^{h\times s\times s}, Q,V\in R^{T\times h\times d/h}, K\in R^{T\times h}$，于是对于每一个head $i=1,2,…,h$来说，有：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210602/13.png" width = "500" align=center /></p><p>注意，上式可以很容易地解释为一个特殊的卷积层，具有：<br>1) <strong>全局连通性</strong></p><p>2) <strong>非负卷积权值</strong></p><p>3) <strong>复杂的除法/乘法门机制</strong></p><p>实验表明，这3个方面对AFT-conv的性能都有显著的影响。</p><h4 id="5-Parameterization"><a href="#5-Parameterization" class="headerlink" title="5 Parameterization"></a>5 Parameterization</h4><p>根据经验，作者发现适当地参数化位置偏差是很重要的。</p><p>对于AFT-full和AFT-local，采用w的因数分解形式:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210602/14.png" width = "400" align=center /></p><p>其中$d’$是一个小的嵌入维数(例如128)。这种简单的因式分解不仅大大减少了参数量，而且在训练和测试中都有效地提高了模型的性能。</p><p>对于AFT-conv，因式分解的技巧并不适用。相反，作者采用一个简单的重新参数化，对于每个head i，让：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210602/15.png" width = "300" align=center /></p><p>其中$\gamma\in R^h, \beta \in R^h$是可学习增益和偏置参数，均初始化为0。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210602/16.png" width = "500" align=center /></p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="3-1-Image-Autoregressive-Modeling"><a href="#3-1-Image-Autoregressive-Modeling" class="headerlink" title="3.1 Image Autoregressive Modeling"></a>3.1 Image Autoregressive Modeling</h3><h4 id="SOTA模型对比"><a href="#SOTA模型对比" class="headerlink" title="SOTA模型对比"></a>SOTA模型对比</h4><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210602/17.png" alt=""></p><h4 id="Factorization的影响"><a href="#Factorization的影响" class="headerlink" title="Factorization的影响"></a>Factorization的影响</h4><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210602/18.png" width = "500" align=center /></p><h3 id="3-2-Language-Modeling"><a href="#3-2-Language-Modeling" class="headerlink" title="3.2 Language Modeling"></a>3.2 Language Modeling</h3><h4 id="SOTA模型对比-1"><a href="#SOTA模型对比-1" class="headerlink" title="SOTA模型对比"></a>SOTA模型对比</h4><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210602/19.png" alt=""></p><h4 id="local-window-size的影响"><a href="#local-window-size的影响" class="headerlink" title="local window size的影响"></a>local window size的影响</h4><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210602/20.png" alt=""></p><h4 id="Longer-sequence-size"><a href="#Longer-sequence-size" class="headerlink" title="Longer sequence size"></a>Longer sequence size</h4><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210602/21.png" alt=""></p><h3 id="3-3-Image-Classification"><a href="#3-3-Image-Classification" class="headerlink" title="3.3 Image Classification"></a>3.3 Image Classification</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210602/22.png" alt=""></p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210602/23.png" alt=""></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>[1].An Attention Free Transformer<br></p>]]></content>
      
      
      <categories>
          
          <category> Transformer </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Tansformer </tag>
            
            <tag> Attention </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>从零开始边缘部署轻量化人脸检测模型————EAIDK310部署篇</title>
      <link href="/2021/09/05/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E8%BE%B9%E7%BC%98%E9%83%A8%E7%BD%B2%E8%BD%BB%E9%87%8F%E5%8C%96%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B%E6%A8%A1%E5%9E%8B%E2%80%94%E2%80%94%E2%80%94%E2%80%94EAIDK310%E9%83%A8%E7%BD%B2%E7%AF%87/"/>
      <url>/2021/09/05/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E8%BE%B9%E7%BC%98%E9%83%A8%E7%BD%B2%E8%BD%BB%E9%87%8F%E5%8C%96%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B%E6%A8%A1%E5%9E%8B%E2%80%94%E2%80%94%E2%80%94%E2%80%94EAIDK310%E9%83%A8%E7%BD%B2%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<p><br></p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210527/0.png" alt=""></p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210527/1.png" alt=""></p><blockquote><p>继续上一章的话题，前面我们主要聊到关于人脸检测模型UltraFace的训练任务，本文将和大家讨论在开发板上如何部署UltraFace模型，并进行实时视频人脸检测，或者图片流人脸检测。</p></blockquote><h2 id="Tengine简介"><a href="#Tengine简介" class="headerlink" title="Tengine简介"></a>Tengine简介</h2><p>Tengine 由 OPEN AI LAB 主导开发，该项目实现了深度学习神经网络模型在嵌入式设备上的快速、高效部署需求。为实现在众多 AIoT 应用中的跨平台部署，本项目基于原有 Tengine 项目使用 C 语言进行重构，针对嵌入式设备资源有限的特点进行了深度框架裁剪。同时采用了完全分离的前后端设计，有利于 CPU、GPU、NPU 等异构计算单元的快速移植和部署，同时降低评估和迁移成本。</p><h3 id="Tengine推理流程"><a href="#Tengine推理流程" class="headerlink" title="Tengine推理流程"></a>Tengine推理流程</h3><p>依照顺序调用Tengine核心API如下：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210530/1.png" alt=""></p><h2 id="模块实现"><a href="#模块实现" class="headerlink" title="模块实现"></a>模块实现</h2><h3 id="1-模型转换"><a href="#1-模型转换" class="headerlink" title="1 模型转换"></a>1 模型转换</h3><h4 id="第1步：转换到onnx模型"><a href="#第1步：转换到onnx模型" class="headerlink" title="第1步：转换到onnx模型"></a>第1步：转换到onnx模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">model_path = <span class="string">&quot;models/pretrained/version-RFB-320.pth&quot;</span></span><br><span class="line">net = create_Mb_Tiny_RFB_fd(<span class="built_in">len</span>(class_names), is_test=<span class="literal">True</span>)</span><br><span class="line">net.load(model_path)</span><br><span class="line">net.<span class="built_in">eval</span>()</span><br><span class="line">net.to(<span class="string">&quot;cuda&quot;</span>)</span><br><span class="line"></span><br><span class="line">model_name = model_path.split(<span class="string">&quot;/&quot;</span>)[-<span class="number">1</span>].split(<span class="string">&quot;.&quot;</span>)[<span class="number">0</span>]</span><br><span class="line">model_path = <span class="string">f&quot;models/onnx/<span class="subst">&#123;model_name&#125;</span>.onnx&quot;</span></span><br><span class="line"></span><br><span class="line">dummy_input = torch.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">240</span>, <span class="number">320</span>).to(<span class="string">&quot;cuda&quot;</span>)</span><br><span class="line">torch.onnx.export(net, dummy_input, model_path, verbose=<span class="literal">False</span>, input_names=[<span class="string">&#x27;input&#x27;</span>], output_names=[<span class="string">&#x27;scores&#x27;</span>, <span class="string">&#x27;boxes&#x27;</span>])</span><br></pre></td></tr></table></figure><h4 id="第2步：编译Tengine模型转换工具"><a href="#第2步：编译Tengine模型转换工具" class="headerlink" title="第2步：编译Tengine模型转换工具"></a>第2步：编译Tengine模型转换工具</h4><p>依赖库安装</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install libprotobuf-dev protobuf-compiler</span><br></pre></td></tr></table></figure><p>源码编译<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir build &amp;&amp; cd build</span><br><span class="line">cmake ..</span><br><span class="line">make -j`nproc` &amp;&amp; make install</span><br></pre></td></tr></table></figure><br>编译完成后，生成的可行性文件tm_convert_tool存放在 ./build/install/bin/ 目录下。</p><h4 id="第3步：转换onnx模型为tmfile模型"><a href="#第3步：转换onnx模型为tmfile模型" class="headerlink" title="第3步：转换onnx模型为tmfile模型"></a>第3步：转换onnx模型为tmfile模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./tm_convert_tool -m xxx.onnx -o xxx.tmfile</span><br></pre></td></tr></table></figure><ul><li>-m 为<em>.caffemodel, </em>.params, <em>.weight, </em>.pb, <em>.onnx, </em>.tflite等模型；</li><li>-o 为output fp32 tmfile</li></ul><h3 id="2-NMS计算"><a href="#2-NMS计算" class="headerlink" title="2 NMS计算"></a>2 NMS计算</h3><h3 id="伪代码："><a href="#伪代码：" class="headerlink" title="伪代码："></a>伪代码：</h3><ul><li><p>1 将各组box按照score降序排列;</p></li><li><p>2 从score最大值开始，置为当前box，保存idex，然后依次遍历后面的box，计算与当前box的IOU值，若大于阈值，则抑制，不会输出;</p></li><li><p>3 完成一轮遍历后，继续选择下一个非抑制的box作为当前box，重复步骤2;</p></li><li><p>4 返回没有被抑制的index即符合条件的box;</p></li></ul><h3 id="python版本"><a href="#python版本" class="headerlink" title="python版本"></a>python版本</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">NMS</span>(<span class="params">dects,threshhold</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    detcs:二维数组(n_samples,5)</span></span><br><span class="line"><span class="string">    5列：x1,y1,x2,y2,score</span></span><br><span class="line"><span class="string">    threshhold: IOU阈值</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    x1=dects[:,<span class="number">0</span>]</span><br><span class="line">    y1=dects[:,<span class="number">1</span>]</span><br><span class="line">    x2=dects[:,<span class="number">2</span>]</span><br><span class="line">    y2=dects[:,<span class="number">3</span>]</span><br><span class="line">    score=dects[:,<span class="number">4</span>]</span><br><span class="line">    ndects=dects.shape[<span class="number">0</span>]<span class="comment">#box的数量</span></span><br><span class="line">    area=(x2-x1+<span class="number">1</span>)*(y2-y1+<span class="number">1</span>)</span><br><span class="line">    order=score.argsort()[::-<span class="number">1</span>] <span class="comment">#score从大到小排列的indexs,一维数组</span></span><br><span class="line">    keep=[] <span class="comment">#保存符合条件的index</span></span><br><span class="line">    suppressed=np.array([<span class="number">0</span>]*ndects) <span class="comment">#初始化为0，若大于threshhold,变为1，表示被抑制</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> _i <span class="keyword">in</span> <span class="built_in">range</span>(ndects):</span><br><span class="line">        i=order[_i]  <span class="comment">#从得分最高的开始遍历</span></span><br><span class="line">        <span class="keyword">if</span> suppressed[i]==<span class="number">1</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        keep.append(i) </span><br><span class="line">        <span class="keyword">for</span> _j <span class="keyword">in</span> <span class="built_in">range</span>(i+<span class="number">1</span>,ndects):</span><br><span class="line">            j=order[_j]</span><br><span class="line">            <span class="keyword">if</span> suppressed[j]==<span class="number">1</span>: <span class="comment">#若已经被抑制，跳过</span></span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            xx1=np.<span class="built_in">max</span>(x1[i],x1[j])<span class="comment">#求两个box的交集面积interface</span></span><br><span class="line">            yy1=np.<span class="built_in">max</span>(y1[i],y1j])</span><br><span class="line">            xx2=np.<span class="built_in">min</span>(x2[i],x2[j])</span><br><span class="line">            yy2=np.<span class="built_in">min</span>(y2[i],y2[j])</span><br><span class="line">            w=np.<span class="built_in">max</span>(<span class="number">0</span>,xx2-xx1+<span class="number">1</span>)</span><br><span class="line">            h=np.<span class="built_in">max</span>(<span class="number">0</span>,yy2-yy1+<span class="number">1</span>)</span><br><span class="line">            interface=w*h</span><br><span class="line">            overlap=interface/(area[i]+area[j]-interface) <span class="comment">#计算IOU（交/并）</span></span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> overlap&gt;=threshhold:<span class="comment">#IOU若大于阈值，则抑制</span></span><br><span class="line">                suppressed[j]=<span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> keep</span><br></pre></td></tr></table></figure><h3 id="C-版本"><a href="#C-版本" class="headerlink" title="C++版本"></a>C++版本</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">UltraFace::nms</span><span class="params">(std::vector&lt;FaceInfo&gt; &amp;input, std::vector&lt;FaceInfo&gt; &amp;output, <span class="keyword">int</span> type)</span> </span>&#123;</span><br><span class="line">    <span class="comment">//根据score对候选框进行 sort 排序操作</span></span><br><span class="line">    std::<span class="built_in">sort</span>(input.<span class="built_in">begin</span>(), input.<span class="built_in">end</span>(), [](<span class="keyword">const</span> FaceInfo &amp;a, <span class="keyword">const</span> FaceInfo &amp;b) &#123; <span class="keyword">return</span> a.score &gt; b.score; &#125;);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> box_num = input.<span class="built_in">size</span>();</span><br><span class="line"></span><br><span class="line">    <span class="function">std::vector&lt;<span class="keyword">int</span>&gt; <span class="title">merged</span><span class="params">(box_num, <span class="number">0</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; box_num; i++) &#123;</span><br><span class="line">        <span class="keyword">if</span> (merged[i])</span><br><span class="line">            <span class="keyword">continue</span>;</span><br><span class="line">        std::vector&lt;FaceInfo&gt; buf;</span><br><span class="line"></span><br><span class="line">        buf.<span class="built_in">push_back</span>(input[i]);</span><br><span class="line">        merged[i] = <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">float</span> h0 = input[i].y2 - input[i].y1 + <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">float</span> w0 = input[i].x2 - input[i].x1 + <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">float</span> area0 = h0 * w0;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j = i + <span class="number">1</span>; j &lt; box_num; j++) &#123;</span><br><span class="line">            <span class="keyword">if</span> (merged[j])</span><br><span class="line">                <span class="keyword">continue</span>;</span><br><span class="line">            <span class="comment">//确立每个候选框的坐标以及宽高</span></span><br><span class="line">            <span class="keyword">float</span> inner_x0 = input[i].x1 &gt; input[j].x1 ? input[i].x1 : input[j].x1;</span><br><span class="line">            <span class="keyword">float</span> inner_y0 = input[i].y1 &gt; input[j].y1 ? input[i].y1 : input[j].y1;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">float</span> inner_x1 = input[i].x2 &lt; input[j].x2 ? input[i].x2 : input[j].x2;</span><br><span class="line">            <span class="keyword">float</span> inner_y1 = input[i].y2 &lt; input[j].y2 ? input[i].y2 : input[j].y2;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">float</span> inner_h = inner_y1 - inner_y0 + <span class="number">1</span>;</span><br><span class="line">            <span class="keyword">float</span> inner_w = inner_x1 - inner_x0 + <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (inner_h &lt;= <span class="number">0</span> || inner_w &lt;= <span class="number">0</span>)</span><br><span class="line">                <span class="keyword">continue</span>;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">float</span> inner_area = inner_h * inner_w;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">float</span> h1 = input[j].y2 - input[j].y1 + <span class="number">1</span>;</span><br><span class="line">            <span class="keyword">float</span> w1 = input[j].x2 - input[j].x1 + <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">float</span> area1 = h1 * w1;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">float</span> score;</span><br><span class="line">            <span class="comment">//计算IOU</span></span><br><span class="line">            score = inner_area / (area0 + area1 - inner_area);</span><br><span class="line">            <span class="comment">//根据阈值进行极大值抑制的筛选</span></span><br><span class="line">            <span class="keyword">if</span> (score &gt; iou_threshold) &#123;</span><br><span class="line">                merged[j] = <span class="number">1</span>;</span><br><span class="line">                buf.<span class="built_in">push_back</span>(input[j]);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-3-获取候选框"><a href="#2-3-获取候选框" class="headerlink" title="2.3 获取候选框"></a>2.3 获取候选框</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//获取候选框</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">UltraFace::generateBBox</span><span class="params">(std::vector&lt;FaceInfo&gt; &amp;bbox_collection, <span class="keyword">tensor_t</span> scores, <span class="keyword">tensor_t</span> boxes)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">float</span>* scores_blob = ( <span class="keyword">float</span>* )<span class="built_in">get_tensor_buffer</span>(scores);</span><br><span class="line">    <span class="keyword">float</span>* boxes_blob = ( <span class="keyword">float</span>* )<span class="built_in">get_tensor_buffer</span>(boxes);</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; num_anchors; i++) &#123;</span><br><span class="line">        <span class="keyword">if</span> (scores_blob[i * <span class="number">2</span> + <span class="number">1</span>] &gt; score_threshold) &#123;</span><br><span class="line">            FaceInfo rects;</span><br><span class="line">            <span class="comment">//确定坐标中心以及box的宽高</span></span><br><span class="line">            <span class="keyword">float</span> x_center = boxes_blob[i * <span class="number">4</span>] * center_variance * priors[i][<span class="number">2</span>] + priors[i][<span class="number">0</span>];</span><br><span class="line">            <span class="keyword">float</span> y_center = boxes_blob[i * <span class="number">4</span> + <span class="number">1</span>] * center_variance * priors[i][<span class="number">3</span>] + priors[i][<span class="number">1</span>];</span><br><span class="line">            <span class="keyword">float</span> w = <span class="built_in">exp</span>(boxes_blob[i * <span class="number">4</span> + <span class="number">2</span>] * size_variance) * priors[i][<span class="number">2</span>];</span><br><span class="line">            <span class="keyword">float</span> h = <span class="built_in">exp</span>(boxes_blob[i * <span class="number">4</span> + <span class="number">3</span>] * size_variance) * priors[i][<span class="number">3</span>];</span><br><span class="line">            <span class="comment">//截取坐标结果</span></span><br><span class="line">            rects.x1 = <span class="built_in">clip</span>(x_center - w / <span class="number">2.0</span>, <span class="number">1</span>) * image_w;</span><br><span class="line">            rects.y1 = <span class="built_in">clip</span>(y_center - h / <span class="number">2.0</span>, <span class="number">1</span>) * image_h;</span><br><span class="line">            rects.x2 = <span class="built_in">clip</span>(x_center + w / <span class="number">2.0</span>, <span class="number">1</span>) * image_w;</span><br><span class="line">            rects.y2 = <span class="built_in">clip</span>(y_center + h / <span class="number">2.0</span>, <span class="number">1</span>) * image_h;</span><br><span class="line">            rects.score = <span class="built_in">clip</span>(scores_blob[i * <span class="number">2</span> + <span class="number">1</span>], <span class="number">1</span>);</span><br><span class="line">            bbox_collection.<span class="built_in">push_back</span>(rects);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-4-模型检测函数"><a href="#2-4-模型检测函数" class="headerlink" title="2.4 模型检测函数"></a>2.4 模型检测函数</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//模型检测函数</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">UltraFace::detect</span><span class="params">(cv::Mat &amp;raw_image, std::vector&lt;FaceInfo&gt; &amp;face_list)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (raw_image.<span class="built_in">empty</span>()) &#123;</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;image is empty ,please check!&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    image_h = raw_image.rows;</span><br><span class="line">    image_w = raw_image.cols;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> img_size      = in_w * in_h * <span class="number">3</span>;</span><br><span class="line">    <span class="keyword">float</span>* input_data = ( <span class="keyword">float</span>* )<span class="built_in">malloc</span>(img_size * <span class="built_in"><span class="keyword">sizeof</span></span>(<span class="keyword">float</span>));</span><br><span class="line">    <span class="comment">// 获取来自opencv读取的图片或者视频数据，并返回一个适应模型输入的结果</span></span><br><span class="line">    <span class="built_in">get_input_data_cv</span>(raw_image, input_data, in_w, in_h, mean_vals, norm_vals, <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">set_tensor_buffer</span>(input_tensor, input_data, (in_w * in_h * <span class="number">3</span>) * <span class="number">4</span>) &lt; <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Set input tensor buffer failed\n&quot;</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//开始计时⏲</span></span><br><span class="line">    <span class="keyword">auto</span> start = chrono::steady_clock::<span class="built_in">now</span>();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 6、Run网络</span></span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">run_graph</span>(graph, <span class="number">1</span>) &lt; <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Run graph failed\n&quot;</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取输出结果</span></span><br><span class="line">    string scores = <span class="string">&quot;scores&quot;</span>;</span><br><span class="line">    string boxes = <span class="string">&quot;boxes&quot;</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//7.1、获取分类得分结果</span></span><br><span class="line">    <span class="keyword">tensor_t</span> tensor_scores = <span class="built_in">get_graph_tensor</span>(graph, scores.<span class="built_in">c_str</span>());</span><br><span class="line">    <span class="comment">//7.2、获取检测框坐标结果</span></span><br><span class="line">    <span class="keyword">tensor_t</span> tensor_boxes = <span class="built_in">get_graph_tensor</span>(graph, boxes.<span class="built_in">c_str</span>());</span><br><span class="line"></span><br><span class="line">    std::vector&lt;FaceInfo&gt; bbox_collection;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//结束计时，然后计算推理时间</span></span><br><span class="line">    <span class="keyword">auto</span> end = chrono::steady_clock::<span class="built_in">now</span>();</span><br><span class="line">    chrono::duration&lt;<span class="keyword">double</span>&gt; elapsed = end - start;</span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;inference time:&quot;</span> &lt;&lt; elapsed.<span class="built_in">count</span>() &lt;&lt; <span class="string">&quot; s&quot;</span> &lt;&lt; endl;</span><br><span class="line">    <span class="comment">//后处理操作，主要是获取BBox以及NMS操作</span></span><br><span class="line">    <span class="built_in">generateBBox</span>(bbox_collection, tensor_scores, tensor_boxes);</span><br><span class="line">    <span class="built_in">nms</span>(bbox_collection, face_list);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">free</span>(input_data);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-5-主函数"><a href="#2-5-主函数" class="headerlink" title="2.5 主函数"></a>2.5 主函数</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&quot;UltraFace.hpp&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;opencv2/opencv.hpp&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;opencv2/highgui.hpp&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;opencv2/imgproc.hpp&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    string tengine_path = <span class="string">&quot;/home/chaucer/Tengine_Tutorial/2_FaceDetector/models/version-RFB-320_simplified.tmfile&quot;</span>;</span><br><span class="line">    <span class="function">UltraFace <span class="title">ultraface</span><span class="params">(tengine_path, <span class="number">320</span>, <span class="number">240</span>, <span class="number">4</span>, <span class="number">0.65</span>)</span></span>; <span class="comment">// config model input</span></span><br><span class="line"></span><br><span class="line">    cv::Mat frame;</span><br><span class="line">    <span class="comment">//cv::VideoCapture capture(0);</span></span><br><span class="line">    <span class="function">cv::VideoCapture <span class="title">capture</span><span class="params">(<span class="string">&quot;/home/chaucer/face_detect/test_1.mp4&quot;</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//cv::Mat frame = cv::imread(image_file);</span></span><br><span class="line">    <span class="keyword">while</span>(<span class="number">1</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        capture &gt;&gt; frame;</span><br><span class="line">        <span class="keyword">auto</span> start = chrono::steady_clock::<span class="built_in">now</span>();</span><br><span class="line">        vector&lt;FaceInfo&gt; face_info;</span><br><span class="line">        ultraface.<span class="built_in">detect</span>(frame, face_info);</span><br><span class="line"></span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;face_info &quot;</span> &lt;&lt; face_info.<span class="built_in">size</span>() &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">auto</span> face : face_info) &#123;</span><br><span class="line">            <span class="function">cv::Point <span class="title">pt1</span><span class="params">(face.x1, face.y1)</span></span>;</span><br><span class="line">            <span class="function">cv::Point <span class="title">pt2</span><span class="params">(face.x2, face.y2)</span></span>;</span><br><span class="line">            cv::<span class="built_in">rectangle</span>(frame, pt1, pt2, cv::<span class="built_in">Scalar</span>(<span class="number">0</span>, <span class="number">255</span>, <span class="number">0</span>), <span class="number">2</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">auto</span> end = chrono::steady_clock::<span class="built_in">now</span>();</span><br><span class="line">        chrono::duration&lt;<span class="keyword">double</span>&gt; elapsed = end - start;</span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;all time: &quot;</span> &lt;&lt; elapsed.<span class="built_in">count</span>() &lt;&lt; <span class="string">&quot; s&quot;</span> &lt;&lt; endl;</span><br><span class="line">        cv::<span class="built_in">imshow</span>(<span class="string">&quot;UltraFace&quot;</span>, frame);</span><br><span class="line">        cv::<span class="built_in">waitKey</span>(<span class="number">1</span>);</span><br><span class="line">        string result_name = <span class="string">&quot;result&quot;</span> + <span class="built_in">to_string</span>(<span class="number">2</span>) + <span class="string">&quot;.jpg&quot;</span>;</span><br><span class="line">        cv::<span class="built_in">imwrite</span>(result_name, frame);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="输出结果"><a href="#输出结果" class="headerlink" title="输出结果"></a>输出结果</h2><h3 id="3-1-图片检测结果"><a href="#3-1-图片检测结果" class="headerlink" title="3.1 图片检测结果"></a>3.1 图片检测结果</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210530/2.png" alt=""></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>[1].<a href="https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB">https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB</a><br><br>[2].<a href="https://github.com/OAID/Tengine">https://github.com/OAID/Tengine</a><br><br>[3].<a href="https://github.com/jiangzhongbo/Tengine_Tutorial">https://github.com/jiangzhongbo/Tengine_Tutorial</a><br></p>]]></content>
      
      
      <categories>
          
          <category> 项目部署 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人脸识别 </tag>
            
            <tag> 人脸检测 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>从零开始边缘部署轻量化人脸检测模型————训练篇</title>
      <link href="/2021/09/05/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E8%BE%B9%E7%BC%98%E9%83%A8%E7%BD%B2%E8%BD%BB%E9%87%8F%E5%8C%96%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B%E6%A8%A1%E5%9E%8B%E2%80%94%E2%80%94%E8%AE%AD%E7%BB%83%E7%AF%87/"/>
      <url>/2021/09/05/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E8%BE%B9%E7%BC%98%E9%83%A8%E7%BD%B2%E8%BD%BB%E9%87%8F%E5%8C%96%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B%E6%A8%A1%E5%9E%8B%E2%80%94%E2%80%94%E8%AE%AD%E7%BB%83%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210527/0.png" alt=""></p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210527/1.png" alt=""></p><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>该模型是针对边缘计算设备设计的轻量人脸检测模型。</p><ul><li>在模型大小上，默认FP32精度下（.pth）文件大小为 1.04~1.1MB，推理框架int8量化后大小为 300KB 左右。</li><li>在模型计算量上，320x240的输入分辨率下 90~109 MFlops左右。</li><li>模型有两个版本，version-slim(主干精简速度略快)，version-RFB(加入了修改后的RFB模块，精度更高)。</li><li>提供320x240、640x480不同输入分辨率下使用widerface训练的预训练模型，更好的工作于不同的应用场景。</li></ul><h2 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h2><h3 id="2-1-输入尺寸的选择"><a href="#2-1-输入尺寸的选择" class="headerlink" title="2.1 输入尺寸的选择"></a>2.1 输入尺寸的选择</h3><p>由于涉及实际部署时的推理速度，因此模型输入尺寸的选择也是一个很重要的话题。</p><p>在作者的原github中，也提到了一点，如果在实际部署的场景中大多数情况为中近距离、人脸大同时人脸的数量也比较少的时候，则可以采用$320\times 240$的输入尺寸；</p><p>如果在实际部署的场景中大多数情况为中远距离、人脸小同时人脸的数量也比较多的时候，则可以采用$640\times 480$或者$480\times 360$的输入尺寸；</p><blockquote><p>这里由于使用的是EAIDK310进行部署测试，边缘性能不是很好，因此选择原作者推荐的最小尺寸$320\times 240$进行训练和部署测试。<br><br><strong>注意：过小的输入分辨率虽然会明显加快推理速度，但是会大幅降低小人脸的召回率。</strong></p></blockquote><h3 id="2-2-数据筛选"><a href="#2-2-数据筛选" class="headerlink" title="2.2 数据筛选"></a>2.2 数据筛选</h3><p>由于widerface官网数据集中有比较多的低于10像素的人脸照片，因此在这里选择剔除这些像素长宽低于10个pixel的照片；</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210527/2.png" alt=""></p><blockquote><p>这样做的原因是：<strong>不清楚的人脸，不太利于高效模型的收敛，所以需要进行过滤训练。</strong></p></blockquote><h2 id="SSD网络结构"><a href="#SSD网络结构" class="headerlink" title="SSD网络结构"></a>SSD网络结构</h2><p>SSD是一个端到端的模型，所有的检测过程和识别过程都是在同一个网络中进行的；同时SSD借鉴了Faster R-CNN的Anchor机制的想法，这样就像相当于在基于回归的的检测过程中结合了区域的思想，可以使得检测效果较定制化边界框的YOLO v1有比较好的提升。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210527/3.png" alt=""></p><p>SSD较传统的检测方法使用顶层特征图的方法选择了使用多尺度特征图，因为在比较浅的特征图中可以对于小目标有比较好的表达，随着特征图的深入，网络对于比较大特征也有了比较好表达能力，故SSD选择使用多尺度特征图可以很好的兼顾大目标和小目标。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210527/4.png" alt=""></p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210527/5.png" alt=""></p><p>SSD模型结构如下：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210527/6.png" alt=""></p><p>这里关于SSD不进行更多的阐述，想了解的小伙伴可以扫描下方的二维码查看（是小编在CSDN的记录，非常详细！！！）：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210527/7.png" alt=""></p><p>整个项目模型搭建如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 网络的主题结构为SSD模型</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SSD</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, num_classes: <span class="built_in">int</span>, base_net: nn.ModuleList, source_layer_indexes: <span class="type">List</span>[<span class="built_in">int</span>],</span></span></span><br><span class="line"><span class="params"><span class="function">                 extras: nn.ModuleList, classification_headers: nn.ModuleList,</span></span></span><br><span class="line"><span class="params"><span class="function">                 regression_headers: nn.ModuleList, is_test=<span class="literal">False</span>, config=<span class="literal">None</span>, device=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Compose a SSD model using the given components.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(SSD, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.num_classes = num_classes</span><br><span class="line">        self.base_net = base_net</span><br><span class="line">        self.source_layer_indexes = source_layer_indexes</span><br><span class="line">        self.extras = extras</span><br><span class="line">        self.classification_headers = classification_headers</span><br><span class="line">        self.regression_headers = regression_headers</span><br><span class="line">        self.is_test = is_test</span><br><span class="line">        self.config = config</span><br><span class="line"></span><br><span class="line">        <span class="comment"># register layers in source_layer_indexes by adding them to a module list</span></span><br><span class="line">        self.source_layer_add_ons = nn.ModuleList([t[<span class="number">1</span>] <span class="keyword">for</span> t <span class="keyword">in</span> source_layer_indexes</span><br><span class="line">                                                   <span class="keyword">if</span> <span class="built_in">isinstance</span>(t, <span class="built_in">tuple</span>) <span class="keyword">and</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(t, GraphPath)])</span><br><span class="line">        <span class="keyword">if</span> device:</span><br><span class="line">            self.device = device</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">        <span class="keyword">if</span> is_test:</span><br><span class="line">            self.config = config</span><br><span class="line">            self.priors = config.priors.to(self.device)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x: torch.Tensor</span>) -&gt; <span class="type">Tuple</span>[torch.Tensor, torch.Tensor]:</span></span><br><span class="line">        confidences = []</span><br><span class="line">        locations = []</span><br><span class="line">        start_layer_index = <span class="number">0</span></span><br><span class="line">        header_index = <span class="number">0</span></span><br><span class="line">        end_layer_index = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> end_layer_index <span class="keyword">in</span> self.source_layer_indexes:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(end_layer_index, GraphPath):</span><br><span class="line">                path = end_layer_index</span><br><span class="line">                end_layer_index = end_layer_index.s0</span><br><span class="line">                added_layer = <span class="literal">None</span></span><br><span class="line">            <span class="keyword">elif</span> <span class="built_in">isinstance</span>(end_layer_index, <span class="built_in">tuple</span>):</span><br><span class="line">                added_layer = end_layer_index[<span class="number">1</span>]</span><br><span class="line">                end_layer_index = end_layer_index[<span class="number">0</span>]</span><br><span class="line">                path = <span class="literal">None</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                added_layer = <span class="literal">None</span></span><br><span class="line">                path = <span class="literal">None</span></span><br><span class="line">            <span class="keyword">for</span> layer <span class="keyword">in</span> self.base_net[start_layer_index: end_layer_index]:</span><br><span class="line">                x = layer(x)</span><br><span class="line">            <span class="keyword">if</span> added_layer:</span><br><span class="line">                y = added_layer(x)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                y = x</span><br><span class="line">            <span class="keyword">if</span> path:</span><br><span class="line">                sub = <span class="built_in">getattr</span>(self.base_net[end_layer_index], path.name)</span><br><span class="line">                <span class="keyword">for</span> layer <span class="keyword">in</span> sub[:path.s1]:</span><br><span class="line">                    x = layer(x)</span><br><span class="line">                y = x</span><br><span class="line">                <span class="keyword">for</span> layer <span class="keyword">in</span> sub[path.s1:]:</span><br><span class="line">                    x = layer(x)</span><br><span class="line">                end_layer_index += <span class="number">1</span></span><br><span class="line">            start_layer_index = end_layer_index</span><br><span class="line">            confidence, location = self.compute_header(header_index, y)</span><br><span class="line">            header_index += <span class="number">1</span></span><br><span class="line">            confidences.append(confidence)</span><br><span class="line">            locations.append(location)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.base_net[end_layer_index:]:</span><br><span class="line">            x = layer(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.extras:</span><br><span class="line">            x = layer(x)</span><br><span class="line">            confidence, location = self.compute_header(header_index, x)</span><br><span class="line">            header_index += <span class="number">1</span></span><br><span class="line">            confidences.append(confidence)</span><br><span class="line">            locations.append(location)</span><br><span class="line"></span><br><span class="line">        confidences = torch.cat(confidences, <span class="number">1</span>)</span><br><span class="line">        locations = torch.cat(locations, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.is_test:</span><br><span class="line">            confidences = F.softmax(confidences, dim=<span class="number">2</span>)</span><br><span class="line">            boxes = box_utils.convert_locations_to_boxes(</span><br><span class="line">                locations, self.priors, self.config.center_variance, self.config.size_variance</span><br><span class="line">            )</span><br><span class="line">            boxes = box_utils.center_form_to_corner_form(boxes)</span><br><span class="line">            <span class="keyword">return</span> confidences, boxes</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> confidences, locations</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compute_header</span>(<span class="params">self, i, x</span>):</span></span><br><span class="line">        confidence = self.classification_headers[i](x)</span><br><span class="line">        confidence = confidence.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>).contiguous()</span><br><span class="line">        confidence = confidence.view(confidence.size(<span class="number">0</span>), -<span class="number">1</span>, self.num_classes)</span><br><span class="line"></span><br><span class="line">        location = self.regression_headers[i](x)</span><br><span class="line">        location = location.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>).contiguous()</span><br><span class="line">        location = location.view(location.size(<span class="number">0</span>), -<span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> confidence, location</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_from_base_net</span>(<span class="params">self, model</span>):</span></span><br><span class="line">        self.base_net.load_state_dict(torch.load(model, map_location=<span class="keyword">lambda</span> storage, loc: storage), strict=<span class="literal">True</span>)</span><br><span class="line">        self.source_layer_add_ons.apply(_xavier_init_)</span><br><span class="line">        self.extras.apply(_xavier_init_)</span><br><span class="line">        self.classification_headers.apply(_xavier_init_)</span><br><span class="line">        self.regression_headers.apply(_xavier_init_)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_from_pretrained_ssd</span>(<span class="params">self, model</span>):</span></span><br><span class="line">        state_dict = torch.load(model, map_location=<span class="keyword">lambda</span> storage, loc: storage)</span><br><span class="line">        state_dict = &#123;k: v <span class="keyword">for</span> k, v <span class="keyword">in</span> state_dict.items() <span class="keyword">if</span> <span class="keyword">not</span> (k.startswith(<span class="string">&quot;classification_headers&quot;</span>) <span class="keyword">or</span> k.startswith(<span class="string">&quot;regression_headers&quot;</span>))&#125;</span><br><span class="line">        model_dict = self.state_dict()</span><br><span class="line">        model_dict.update(state_dict)</span><br><span class="line">        self.load_state_dict(model_dict)</span><br><span class="line">        self.classification_headers.apply(_xavier_init_)</span><br><span class="line">        self.regression_headers.apply(_xavier_init_)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.base_net.apply(_xavier_init_)</span><br><span class="line">        self.source_layer_add_ons.apply(_xavier_init_)</span><br><span class="line">        self.extras.apply(_xavier_init_)</span><br><span class="line">        self.classification_headers.apply(_xavier_init_)</span><br><span class="line">        self.regression_headers.apply(_xavier_init_)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">load</span>(<span class="params">self, model</span>):</span></span><br><span class="line">        self.load_state_dict(torch.load(model, map_location=<span class="keyword">lambda</span> storage, loc: storage))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save</span>(<span class="params">self, model_path</span>):</span></span><br><span class="line">        torch.save(self.state_dict(), model_path)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>损失函数作者选择使用的依旧是SSD的Smooth L1 Loss以及Cross Entropy Loss，其中Smooth L1 Loss用于边界框的回归，而Cross Entropy Loss则用于分类。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210527/8.png" width = "500" align=center /></p><p>具体pytorch实现如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiboxLoss</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, priors, neg_pos_ratio,</span></span></span><br><span class="line"><span class="params"><span class="function">                 center_variance, size_variance, device</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Implement SSD Multibox Loss.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Basically, Multibox loss combines classification loss</span></span><br><span class="line"><span class="string">         and Smooth L1 regression loss.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(MultiboxLoss, self).__init__()</span><br><span class="line">        self.neg_pos_ratio = neg_pos_ratio</span><br><span class="line">        self.center_variance = center_variance</span><br><span class="line">        self.size_variance = size_variance</span><br><span class="line">        self.priors = priors</span><br><span class="line">        self.priors.to(device)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, confidence, predicted_locations, labels, gt_locations</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Compute classification loss and smooth l1 loss.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            confidence (batch_size, num_priors, num_classes): class predictions.</span></span><br><span class="line"><span class="string">            locations (batch_size, num_priors, 4): predicted locations.</span></span><br><span class="line"><span class="string">            labels (batch_size, num_priors): real labels of all the priors.</span></span><br><span class="line"><span class="string">            boxes (batch_size, num_priors, 4): real boxes corresponding all the priors.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        num_classes = confidence.size(<span class="number">2</span>)</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            <span class="comment"># derived from cross_entropy=sum(log(p))</span></span><br><span class="line">            loss = -F.log_softmax(confidence, dim=<span class="number">2</span>)[:, :, <span class="number">0</span>]</span><br><span class="line">            mask = box_utils.hard_negative_mining(loss, labels, self.neg_pos_ratio)</span><br><span class="line"></span><br><span class="line">        confidence = confidence[mask, :]</span><br><span class="line">        <span class="comment"># 分类损失函数</span></span><br><span class="line">        classification_loss = F.cross_entropy(confidence.reshape(-<span class="number">1</span>, num_classes), labels[mask], reduction=<span class="string">&#x27;sum&#x27;</span>)</span><br><span class="line">        pos_mask = labels &gt; <span class="number">0</span></span><br><span class="line">        predicted_locations = predicted_locations[pos_mask, :].reshape(-<span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line">        gt_locations = gt_locations[pos_mask, :].reshape(-<span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line">        <span class="comment"># 边界框回归损失函数</span></span><br><span class="line">        smooth_l1_loss = F.smooth_l1_loss(predicted_locations, gt_locations, reduction=<span class="string">&#x27;sum&#x27;</span>)  <span class="comment"># smooth_l1_loss</span></span><br><span class="line">        <span class="comment"># smooth_l1_loss = F.mse_loss(predicted_locations, gt_locations, reduction=&#x27;sum&#x27;)  #l2 loss</span></span><br><span class="line">        num_pos = gt_locations.size(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> smooth_l1_loss / num_pos, classification_loss / num_pos</span><br></pre></td></tr></table></figure></p><h2 id="结果预测"><a href="#结果预测" class="headerlink" title="结果预测"></a>结果预测</h2><p>输入为：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210527/9.png" alt=""></p><p>输出为：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210527/10.png" alt=""></p><p>输入为：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210527/11.png" alt=""></p><p>输出为：<br><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210527/12.png" alt=""></p><h2 id="模型转换"><a href="#模型转换" class="headerlink" title="模型转换"></a>模型转换</h2><p>由于部署使用的是Tengine边缘推理框架，由于pytorch输出的模型无法直接转换到tmfile模型下，因此还是选择使用onnx中间件的形式进行过度，具体实现代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">model_path = <span class="string">&quot;models/pretrained/version-RFB-320.pth&quot;</span></span><br><span class="line">net = create_Mb_Tiny_RFB_fd(<span class="built_in">len</span>(class_names), is_test=<span class="literal">True</span>)</span><br><span class="line">net.load(model_path)</span><br><span class="line">net.<span class="built_in">eval</span>()</span><br><span class="line">net.to(<span class="string">&quot;cuda&quot;</span>)</span><br><span class="line"></span><br><span class="line">model_name = model_path.split(<span class="string">&quot;/&quot;</span>)[-<span class="number">1</span>].split(<span class="string">&quot;.&quot;</span>)[<span class="number">0</span>]</span><br><span class="line">model_path = <span class="string">f&quot;models/onnx/<span class="subst">&#123;model_name&#125;</span>.onnx&quot;</span></span><br><span class="line"></span><br><span class="line">dummy_input = torch.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">240</span>, <span class="number">320</span>).to(<span class="string">&quot;cuda&quot;</span>)</span><br><span class="line"><span class="comment"># dummy_input = torch.randn(1, 3, 480, 640).to(&quot;cuda&quot;) #if input size is 640*480</span></span><br><span class="line">torch.onnx.export(net, dummy_input, model_path, verbose=<span class="literal">False</span>, input_names=[<span class="string">&#x27;input&#x27;</span>], output_names=[<span class="string">&#x27;scores&#x27;</span>, <span class="string">&#x27;boxes&#x27;</span>])</span><br></pre></td></tr></table></figure></p><p>得到onnx模型后便可以进行Tengine模型的转换和部署，该部分将在下一篇文章继续讨论。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>[1].<a href="https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB">https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB</a><br></p><p>[2].<a href="https://github.com/onnx/onnx">https://github.com/onnx/onnx</a><br></p>]]></content>
      
      
      <categories>
          
          <category> 项目实践 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人脸识别 </tag>
            
            <tag> 人脸检测 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>详细解读Transformer那些有趣的特性</title>
      <link href="/2021/09/05/%E8%AF%A6%E7%BB%86%E8%A7%A3%E8%AF%BBTransformer%E9%82%A3%E4%BA%9B%E6%9C%89%E8%B6%A3%E7%9A%84%E7%89%B9%E6%80%A7/"/>
      <url>/2021/09/05/%E8%AF%A6%E7%BB%86%E8%A7%A3%E8%AF%BBTransformer%E9%82%A3%E4%BA%9B%E6%9C%89%E8%B6%A3%E7%9A%84%E7%89%B9%E6%80%A7/</url>
      
        <content type="html"><![CDATA[<p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210525/1.png" alt=""></p><blockquote><p>本文发现了Transformer的一些重要特性，如<strong>Transformer对严重的遮挡，扰动和域偏移具有很高的鲁棒性</strong>、<strong>与CNN相比，ViT更符合人类视觉系统，泛化性更强</strong>，等等…  代码即将开源！<br><br><strong>作者单位</strong>：澳大利亚国立大学, 蒙纳士大学, 谷歌等7家高校/企业</p><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2></blockquote><p>近期Vision Transformer（ViT）在各个垂直任务上均表现出非常不错的性能。这些模型基于multi-head自注意力机制，该机制可以灵活地处理一系列图像patches以对上下文cues进行编码。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210525/2.png" alt=""></p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210525/3.png" alt=""></p><p>一个重要的问题是，在以给定patch为条件的图像范围内，如何灵活地处理图像中的干扰，例如严重的遮挡问题、域偏移问题、空间排列问题、对抗性和自然扰动等等问题。作者通过涵盖3个ViT系列的大量实验，以及与高性能卷积神经网络（CNN）的比较，系统地研究了这些问题。并通过分析得出了ViT的以下的特性：</p><p>1) Transformer对严重的遮挡，扰动和域偏移具有很高的鲁棒性，例如，即使随机遮挡80％的图像内容，其在ImageNet上仍可保持高达60％的top-1精度; </p><p>2) Transformer对于遮挡的良好表现并不是由于依赖局部纹理信息，与CNN相比，ViT对纹理的依赖要小得多。当经过适当训练以对基于shape的特征进行编码时，ViT可以展现出与人类视觉系统相当的shape识别能力;</p><p>3) 使用ViT对shape进行编码会产生有趣的现象，在即使没有像素级监督的情况下也可以进行精确的语义分割;</p><p>4) 可以将单个ViT模型提取的特征进行组合以创建特征集合，从而在传统学习模型和少量学习模型中的一系列分类数据集上实现较高的准确率。实验表明，ViT的有效特征是由于通过自注意力机制可以产生的灵活和动态的感受野所带来的。</p><h2 id="本文讨论主题"><a href="#本文讨论主题" class="headerlink" title="本文讨论主题"></a>本文讨论主题</h2><h3 id="2-1-ViT对遮挡鲁棒否？"><a href="#2-1-ViT对遮挡鲁棒否？" class="headerlink" title="2.1 ViT对遮挡鲁棒否？"></a>2.1 ViT对遮挡鲁棒否？</h3><p>这里假设有一个网络模型$f$，它通过处理一个输入图像$x$来预测一个标签$y$，其中$x$可以表示为一个patch $x={x<em>i}</em>{i=1}^N$的序列，$N$是图像patch的总数。</p><p>虽然可以有很多种方法来建模遮挡，但本文还是选择了采用一个简单的掩蔽策略，选择整个图像patch的一个子集，$M &lt; N$，并将这些patch的像素值设为0，这样便创建一个遮挡图像$x’$。</p><p>作者将上述方法称为<strong>PatchDrop</strong>。目的是观察鲁棒性$f(x’)_{argmax}=y$。</p><p>作者总共实验了3种遮挡方法:<br>1) Random PatchDrop<br>2) Salient(foreground) PatchDrop<br>3) Non-salient (background) PatchDrop</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210525/4.png" alt=""></p><h4 id="1、Random-PatchDrop"><a href="#1、Random-PatchDrop" class="headerlink" title="1、Random PatchDrop"></a><strong><em>1、Random PatchDrop</em></strong></h4><p>ViT通常将图像划分为196个patch，每个patch为14x14网格，这样一幅224x224x3大小的图像分割成196个patches，每个patch的大小为16x16x3。例如，随机从输入中删除100个这样的补丁就相当于丢失了51%的图像内容。而这个随机删除的过程即为<strong>Random PatchDrop</strong>。</p><h4 id="2、Salient-foreground-PatchDrop"><a href="#2、Salient-foreground-PatchDrop" class="headerlink" title="2、Salient(foreground) PatchDrop"></a><strong><em>2、Salient(foreground) PatchDrop</em></strong></h4><p>对于分类器来说，并不是所有的像素都具有相同的值。为了估计显著区域，作者利用了一个自监督的ViT模型DINO，该模型使用注意力分割图像中的显著目标。按照这种方法可以从196个包含前n个百分比的前景信息的patches中选择一个子集并删除它们。而这种通过自监督模型删除显著区域的过程即为<strong>Salient (foreground) PatchDrop</strong>。</p><h4 id="3、Non-salient-background-PatchDrop"><a href="#3、Non-salient-background-PatchDrop" class="headerlink" title="3、Non-salient(background) PatchDrop"></a><strong><em>3、Non-salient(background) PatchDrop</em></strong></h4><p>采用与SP（Salient(foreground) PatchDrop）相同的方法选择图像中最不显著的区域。包含前景信息中最低n%的patch被选中并删除。同样，而这种通过自监督模型删除非显著区域的过程即为<strong>Non-salient(background) PatchDrop</strong>。</p><h4 id="鲁棒性分析"><a href="#鲁棒性分析" class="headerlink" title="鲁棒性分析"></a>鲁棒性分析</h4><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210525/5.png" alt=""></p><p>以Random PatchDrop为例，作者给出5次测试的平均值和标准偏差。对于显著性和非显著性Patchdrop，由于获得的遮挡掩模是确定性的，作者只给出了1次运行的精度值。</p><p>Random PatchDrop 50%的图像信息几乎完全破坏了CNN的识别能力。例如，当去掉50%的图像内容时ResNet50的准确率为0.1%，而DeiT-S的准确率为70%。一个极端的例子可以观察到，当90%的图像信息丢失，但Deit-B仍然显示出37%的识别精度。这个结果在不同的ViT体系结构中是一致的。同样，ViT对前景(显著)和背景(非显著)内容的去除也有很不错的表现。</p><h4 id="Class-Token-Preserves-Information"><a href="#Class-Token-Preserves-Information" class="headerlink" title="Class Token Preserves Information"></a>Class Token Preserves Information</h4><p>为了更好地理解模型在这种遮挡下的性能鲁棒的原有，作者将不同层的注意力可视化(图4)。 通过下图可以看出浅层更关注遮挡区域，而较深的层更关注图像中的遮挡以外的信息。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210525/6.png" alt=""></p><p>然后作者还研究这种从浅层到更深层次的变化是否是导致针对遮挡的Token不变性的原因，而这对分类是非常重要的。作者测量了原始图像和遮挡图像的特征/标记之间的相关系数。在ResNet50的情况下测试在logit层之前的特性，对于ViT模型，Class Token从最后一个Transformer block中提取。与ResNet50特性相比，来自Transformer的Class Token明显更鲁棒，也不会遭受太多信息损失(表1)。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210525/7.png" alt=""></p><p>此外，作者还可视化了ImageNet中12个选择的超类的相关系数，并注意到这种趋势在不同的类类型中都存在，即使是相对较小的对象类型，如昆虫，食物和鸟类。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210525/8.png" alt=""></p><h3 id="2-2-ViT能否同时学习Shape和Texture这2种特性？"><a href="#2-2-ViT能否同时学习Shape和Texture这2种特性？" class="headerlink" title="2.2 ViT能否同时学习Shape和Texture这2种特性？"></a>2.2 ViT能否同时学习Shape和Texture这2种特性？</h3><p>Geirhos等人引入了Shape vs Texture的假设，并提出了一种训练框架来增强卷积神经网络(CNNs)中的shape偏差。</p><p>首先，作者对ViT模型进行了类似的分析，得出了比CNN更强的shape偏差，与人类视觉系统识别形状的能力相当。然而，这种方法会导致自然图像的精度显著下降。</p><p>为了解决这种问题，在第2种方法中，作者将shape token引入到Transformer体系结构中，专门学习shape信息，使用一组不同的Token在同一体系结构中建模Shape和Texture相关的特征。为此，作者从预训练的高shape偏差CNN模型中提取shape信息。而作者的这种蒸馏方法提供了一种平衡，既保持合理的分类精度，又提供比原始ViT模型更好的shape偏差。</p><h4 id="Training-without-Local-Texture"><a href="#Training-without-Local-Texture" class="headerlink" title="Training without Local Texture"></a>Training without Local Texture</h4><p>在训练中首先通过创建一个SIN风格化的ImageNet数据（从训练数据中删除局部纹理信息）。在这个数据集上训练非常小的DeiT模型。通常情况下，vit在训练期间需要大量的数据增强。然而，由于较少的纹理细节，使用SIN进行学习是一项困难的任务，并且在风格化样本上进行进一步的扩展会破坏shape信息，使训练不稳定。因此，在SIN上训练模型不使用任何augmentation、label smoothing或Mixup。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210525/9.png" alt=""></p><p>作者观察到，在ImageNet上训练的ViT模型比类似参数量的CNN模型表现出更高的shape偏差，例如，具有2200万个参数的DeiT-S比ResNet50表现更好(右图)。当比较SIN训练模型时，ViT模型始终优于cnn模型。有趣的是，DeiT-S在SIN数据集上训练时达到了人类水平(左图)。</p><h4 id="Shape-Distillation"><a href="#Shape-Distillation" class="headerlink" title="Shape Distillation"></a>Shape Distillation</h4><p>通过学习Teacher models 提供的soft labels，知识蒸馏可以将大teacher models压缩成较小的Student Model。本文作者引入了一种新的shape token，并采用 Adapt Attentive Distillation从SIN dataset(ResNet50-SIN)训练的CNN中提取Shape特征。作者注意到，ViT特性本质上是动态的，可以通过Auxiliary Token来控制其学习所需的特征。这意味着单个ViT模型可以同时使用单独的标记显示high shape和texture bias(下表)。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210525/10.png" alt=""></p><p>当引入shape token时在分类和形状偏差度量方面获得了更平衡的性能(图6)。为了证明这些不同的token(用于分类和shape)可以确实模型独特的特征，作者计算了所蒸馏的模型DeiT-T-SIN和DeiT-S-SIN的class和shape token之间的余弦相似度，结果分别是0.35和0.68。这明显低于class和distill token之间的相似性；DeiT-T和Deit-S分别为0.96和0.94。这证实了关于在Transformer中使用单独的Token可以用来建模不同特征的假设，这是一种独特的能力，但是不能直接用在CNN模型中。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210525/11.png" alt=""></p><h4 id="Shape-biased-ViT-Offers-Automated-Object-Segmentation"><a href="#Shape-biased-ViT-Offers-Automated-Object-Segmentation" class="headerlink" title="Shape-biased ViT Offers Automated Object Segmentation"></a>Shape-biased ViT Offers Automated Object Segmentation</h4><p>有趣的是，没有局部纹理或形状蒸馏的训练可以让ViT专注于场景中的前景物体而忽略背景(图4)。这为图像提供了自动语义分割的特征，尽管该模型从未显示像素级对象标签。这也表明，在ViT中促进shape偏差作为一个自监督信号，模型可以学习不同shape相关的特征，这有助于定位正确的前景对象。值得注意的是，没有使用shape token的训练中ViT表现得比较差(Table 3)。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210525/12.png" alt=""></p><h3 id="2-3-位置编码是否真的可以表征Global-Context？"><a href="#2-3-位置编码是否真的可以表征Global-Context？" class="headerlink" title="2.3 位置编码是否真的可以表征Global Context？"></a>2.3 位置编码是否真的可以表征Global Context？</h3><p>Transformer使用self-attention(而不是RNN中的顺序设计)并行处理长序列，其序列排序是不变的。但是它的明显缺点是忽略了输入序列元素的顺序，这可能很重要。</p><p>在视觉领域patch的排列顺序代表了图像的整体结构和整体构成。由于ViT对图像块进行序列处理，改变序列的顺序，例如对图像块进行shuffle操作但是该操作会破坏图像结构。</p><p>当前的ViT使用位置编码来保存Context。在这里问题是，如果序列顺序建模的位置编码允许ViT在遮挡处理是否依然有效?</p><p>然而，分析表明，Transformer显示排列不变的patch位置。位置编码对向ViT模型注入图像结构信息的作用是有限的。这一观察结果也与语言领域的发现相一致。</p><h4 id="Sensitivity-to-Spatial-Structure"><a href="#Sensitivity-to-Spatial-Structure" class="headerlink" title="Sensitivity to Spatial Structure"></a>Sensitivity to Spatial Structure</h4><p>通过对输入图像patch使用shuffle操作来消除下图所示的图像(空间关系)中的结构信息。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210525/13.png" alt=""></p><p>作者观察到，当输入图像的空间结构受到干扰时，DeiT模型比CNN模型保持了更高程度的准确性。这也一方面证明了位置编码对于做出正确的分类决策并不是至关重要的，并且该模型并没有使用位置编码中保存的patch序列信息来恢复全局图像context。即使在没有这种编码的情况下，与使用位置编码的ViT相比，ViT也能够保持其性能，并表现出更好的排列不变性(下图)。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210525/14.png" alt=""></p><p>最后，在ViT训练过程中，当patch大小发生变化时，对自然图像进行非混叠处理时，其排列不变性也会随着精度的降低而降低(下图)。作者将ViT的排列不变性归因于它们的动态感受野，该感受野依赖于输入小patch，可以与其他序列元素调整注意，从而在中等变换速率下，改变小patch的顺序不会显著降低表现。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210525/15.png" alt=""></p><blockquote><p>从上面的分析可以看出，就像texture bias假设是错误的一样，依赖位置编码来在遮挡下表现良好也是不准确的。作者得出这样的结论，这种鲁棒性可能只是由于ViT灵活和动态的感受野所带来的，这同时也取决于输入图像的内容。</p></blockquote><h3 id="2-4-ViT对对抗信息和自然扰动的鲁棒性又如何？"><a href="#2-4-ViT对对抗信息和自然扰动的鲁棒性又如何？" class="headerlink" title="2.4 ViT对对抗信息和自然扰动的鲁棒性又如何？"></a>2.4 ViT对对抗信息和自然扰动的鲁棒性又如何？</h3><p>作者通过计算针对雨、雾、雪和噪声等多种综合常见干扰的平均损坏误差(mCE)来研究这一问题。具有类似CNN参数的ViT(例如，DeiT-S)比经过增强训练的ResNet50(Augmix)对图像干扰更加鲁棒。有趣的是，在ImageNet或SIN上未经增强训练的卷积和Transformer模型更容易受到图像干扰的影响(表6)。这些发现与此一致，表明数据增强对于提高常见干扰的鲁棒性是很必要的。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210525/16.png" alt=""></p><p>作者还观察到adversarial patch攻击的类似问题。ViT的鲁棒性高于CNN，通用adversarial patch在白盒设置(完全了解模型参数)。在SIN上训练的ViT和CNN比在ImageNet上训练的模型更容易受到adversarial patch攻击(图10)，这是由于shape偏差与鲁棒性权衡导致的。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210525/17.png" alt=""></p><h3 id="2-5-当前ViT的最佳Token是什么？"><a href="#2-5-当前ViT的最佳Token是什么？" class="headerlink" title="2.5 当前ViT的最佳Token是什么？"></a>2.5 当前ViT的最佳Token是什么？</h3><p>ViT模型的一个独特特征是模型中的每个patch产生一个class token，class head可以单独处理该class token(下图所示)。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210525/18.png" alt=""></p><p>使得可以测量一个ImageNet预先训练的ViT的每个单独patch的区分能力，如图12所示，由更深的区块产生的class token更具鉴别性，作者利用这一结果来识别其token具有best downstream transferability最优patch token集合。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210525/19.png" alt=""></p><h4 id="Transfer-Methodology"><a href="#Transfer-Methodology" class="headerlink" title="Transfer Methodology"></a>Transfer Methodology</h4><p>如图12所示，作者分析了DeiT模型的block的分类精度，发现在最后几个block的class token中捕获了最优的判别信息。为了验证是否可以将这些信息组合起来以获得更好的性能，作者使用DeiT-S对细粒度分类数据集上现成的迁移学习进行了消融研究(CUB)，如下表所示。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210525/20.png" alt=""></p><p>在这里，作者从不同的块连接class token(可选地结合平均补丁标记)，并训练一个线性分类器来将特征转移到下游任务。</p><p>请注意，一个patch token是通过沿着patch维度进行平均生成的。最后4个块的class token连接得到了最好的迁移学习性能。</p><p>作者将这种迁移方法称为<strong>DeiT-S(ensemble)</strong>。来自所有块的class token和averaged patch tokens的拼接表现出与来自最后4个块的token相似的性能，但是需要显著的大参数来训练。作者进一步在更大范围的任务中使用DeiT-S(集成)进行进一步实验，以验证假设。在接下来的实验中，同时还将CNN Baseline与在预训练的ResNet50的logit层之前提取的特征进行比较。</p><h4 id="General-Classification"><a href="#General-Classification" class="headerlink" title="General Classification"></a>General Classification</h4><p>作者还研究了几个数据集的现成特征的可迁移性，包括Aircraft, CUB, DTD, GTSRB, Fungi, Places365和iNaturalist数据集。这些数据集分别用于细粒度识别、纹理分类、交通标志识别、真菌种类分类和场景识别，分别有100、200、47、43、1394、365和1010类。在每个数据集上训练一个线性分类器，并在测试分割上评估其性能。与CNN Baseline相比，ViT特征有了明显的改善(图13)。事实上，参数比ResNet50少5倍左右的DeiT-T性能更好。此外，本文提出的集成策略在所有数据集上都获得了最好的结果。</p><h4 id="Few-Shot-Learning"><a href="#Few-Shot-Learning" class="headerlink" title="Few-Shot Learning"></a>Few-Shot Learning</h4><p>在few-shot learning的情况下，元数据集是一个大规模的benchmark，包含一个不同的数据集集覆盖多个领域。作者使用提取的特征为每个query学习support set上的线性分类器，并使用标准FSL协议评估。ViT特征在这些不同的领域之间转移得更好(图13)。作者还强调了QuickDraw的一个改进，包含手绘草图的数据集，这与研究结果一致。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210525/21.png" alt=""></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>[1].Intriguing Properties of Vision Transformers.<br></p>]]></content>
      
      
      <categories>
          
          <category> Transformer </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>在ResNet与Transformer均适用的Skip Connection解读</title>
      <link href="/2021/09/05/%E5%9C%A8ResNet%E4%B8%8ETransformer%E5%9D%87%E9%80%82%E7%94%A8%E7%9A%84Skip%20Connection%E8%A7%A3%E8%AF%BB/"/>
      <url>/2021/09/05/%E5%9C%A8ResNet%E4%B8%8ETransformer%E5%9D%87%E9%80%82%E7%94%A8%E7%9A%84Skip%20Connection%E8%A7%A3%E8%AF%BB/</url>
      
        <content type="html"><![CDATA[<p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210521/1.png" alt=""></p><blockquote><p>该文主要是分析和讨论了跳跃连接的一些局限，同时分析了BN的一些限制，提出了通过递归的Skip connection和layer normalization来自适应地调整输入scale的策略，可以很好的提升跳Skip connection的性能，该方法在CV和NLP领域均适用。</p></blockquote><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>Skip connection是一种广泛应用于提高深度神经网络性能和收敛性的技术，它通过神经网络层传播的线性分量，缓解了非线性带来的优化困难。但是，从另一个角度来看，它也可以看作是输入和输出之间的调制机制，输入按预定义值1进行缩放。</p><p>在本文中，作者通过研究Skip connection的有效性和scale factors显示，一个微不足道的调整将导致spurious gradient爆炸或消失，这可以通过normalization来解决，特别是layer normalization。受此启发作者进一步提出通过递归的Skip connection和layer normalization来自适应地调整输入scale，这大大提高了性能，并且在包括机器翻译和图像分类数据集在内的各种任务中具有很好的泛化效果。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210521/2.png" alt="图1 常用skip connections"></p><h3 id="这项工作的特点："><a href="#这项工作的特点：" class="headerlink" title="这项工作的特点："></a>这项工作的特点：</h3><p>1) 主要关注LN和skip connection的结合；<br>2) 重新思考了层归一化的作用，选择不进行缩放；<br>3) 在具有代表性的计算机视觉和自然语言处理任务上进行实验；<br>4) 摆脱了泛化了所有以前工作的残差块的一般形式，并提出了一种新的递归残差块结构，它具有层归一化，优于本工作中检查的所有一般形式的变体；</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><h3 id="connection-problem"><a href="#connection-problem" class="headerlink" title="connection problem"></a>connection problem</h3><p>在进行尺度scaling时，会出现梯度爆炸或消失的问题，阻碍了深度神经网络的高效优化。</p><h3 id="optimization-problem"><a href="#optimization-problem" class="headerlink" title="optimization problem"></a>optimization problem</h3><p>由于早期的工作已经确定，将Skip connection直接结合到神经网络的前向传播中就足够了，不需要任何尺度，后续的优化问题研究大多遵循Skip connection结构。</p><h3 id="架构说明"><a href="#架构说明" class="headerlink" title="架构说明"></a>架构说明</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210521/3.png" alt="图2 常见LN与skip connections组合"></p><h4 id="Expanded-Skip-Connection-xSkip-："><a href="#Expanded-Skip-Connection-xSkip-：" class="headerlink" title="Expanded Skip Connection (xSkip)："></a><strong>Expanded Skip Connection (xSkip)</strong>：</h4><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210521/4.png" alt=""></p><p>其中，$x$和$y$分别为残差块的输入和输出。$F$为weighted neural network layer，$\lambda$为modulating scalar。</p><p>考虑到神经网络层可能具有不同的表示能力和优化难度，这种结构自然调整了跳跃的重要性。然而，需要注意的是，在这项工作中$\lambda$是固定的，目的是隔离缩放的影响。虽然学习过的$\lambda$可能更好地捕捉到这2个部分之间的平衡，但是学习$\lambda$变成了另一个变量。</p><h4 id="Expanded-Skip-Connection-with-Layer-Normalization-xSkip-LN-："><a href="#Expanded-Skip-Connection-with-Layer-Normalization-xSkip-LN-：" class="headerlink" title="Expanded Skip Connection with Layer Normalization (xSkip+LN)："></a><strong>Expanded Skip Connection with Layer Normalization (xSkip+LN)</strong>：</h4><p>在Transformer将跳跃连接与层规范化相结合的激励下，作者进一步研究了层规范化对扩展跳跃连接的影响：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210521/5.png" alt=""></p><p>实验表明层归一化有助于缓解调制因子在优化过程中引起的梯度畸变。不同于作用于“样本空间”的BN，LN则是作用于“特征空间”。同时在神经网络难以优化的情况下，LN仍然可以帮助学习shortcut，而BN可能会失败。</p><h4 id="Recursive-Skip-Connection-with-Layer-Normalization-rSkip-LN-："><a href="#Recursive-Skip-Connection-with-Layer-Normalization-rSkip-LN-：" class="headerlink" title="Recursive Skip Connection with Layer Normalization (rSkip+LN)："></a><strong>Recursive Skip Connection with Layer Normalization (rSkip+LN)</strong>：</h4><p>另一种稳定梯度的方法是每次保持$\lambda$=1，但重复添加带有LN的shortcut，这样更多的输入信息也被建模。它被递归定义为：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210521/6.png" alt=""></p><p>$\lambda$应该是一个不小于1的整数。例如，当$\lambda$=1时，上式便回归到Transformer中使用的block，并符合跳过不需要缩放的结果。</p><p>通过recursive skip connection with layer normalization，该模型鼓励多次使用层归一化来改进优化，通过跳跃连接可以包含更多的x信息。此外，与一次性简单地合并比例跳跃相比，该模型可能获得更强的表达能力，因为每一个递归步骤本质上构建了一个不同的特征分布，递归结构可以学习自适应的x与F(x,W)。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="实验1：PreAct-ResNet-110-on-cifar10"><a href="#实验1：PreAct-ResNet-110-on-cifar10" class="headerlink" title="实验1：PreAct-ResNet-110 on cifar10"></a>实验1：PreAct-ResNet-110 on cifar10</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210521/7.png" alt=""></p><h3 id="实验2：EN-VI-machine-translation"><a href="#实验2：EN-VI-machine-translation" class="headerlink" title="实验2：EN-VI machine translation"></a>实验2：EN-VI machine translation</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210521/8.png" alt=""></p><h3 id="实验3：BN代替LN"><a href="#实验3：BN代替LN" class="headerlink" title="实验3：BN代替LN"></a>实验3：BN代替LN</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210521/9.png" alt=""></p><p>可以看出，与LN结合跳跃连接相比，BN的效果较差。而本文所提出的递归策略可以帮助BN提升效果。</p><h3 id="实验结论"><a href="#实验结论" class="headerlink" title="实验结论"></a>实验结论</h3><p>作者通过对不同任务的实验（Transformer和ResNet），得出如下结论:</p><ul><li><p>没有经过任何归一化的expanded skip connection确实会造成梯度畸形，导致神经网络的学习效果不理想。层归一化在一定程度上有助于解决 expanded skip connection带来的优化问题。</p></li><li><p>本文提出的带有LN的recursive skip connection，通过将expanded skip connection划分为多个阶段，以更好地融合转换输入的效果，进一步简化了优化过程。</p></li><li><p>利用Transformer在WMT-2014 EN-DE机器翻译数据集上的实验结果进一步证明了递归架构的有效性和效率，模型性能甚至优于3倍大的模型。</p></li></ul><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>[1].Rethinking Skip Connection with Layer Normalization in Transformers and ResNets<br></p>]]></content>
      
      
      <categories>
          
          <category> 卷积CNN </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 残差连接 </tag>
            
            <tag> CNN </tag>
            
            <tag> Tansformer </tag>
            
            <tag> ResNet </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
