<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>如何用Transformer一步一步改进Unet?</title>
      <link href="/2021/09/27/18/"/>
      <url>/2021/09/27/18/</url>
      
        <content type="html"><![CDATA[<p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210927/1.png" alt=""></p><blockquote><p>本文提出了一种用于医学图像分析的基于Transformer和UNet的神经网络，Transformer直接处理原始图像而不是提取的特征图，性能优于Trans-Unet等网络。<br><strong>作者单位</strong>:北京Zoezen机器人,北航</p></blockquote><h2 id="简介">简介</h2><p>医学图像分割在生物医学图像分析中占有重要地位，也引起了人们的广泛关注。良好的分割结果可以帮助医生进行判断，进一步改善患者体验。</p><p>在医学图像分析的众多可用方法中，UNet是最受欢迎的神经网络之一，它通过在编码器和解码器之间添加级联来保持原始特征，这使得它在工业领域仍有广泛的应用。同时，Transformer作为一种主导自然语言处理任务的模型，现已被广泛地引入到计算机视觉任务中，并在目标检测、图像分类和语义分割等任务中取得了良好的效果。因此，Transformer和UNet的结合应该比2种方法单独工作更有效。</p><p>在本文中，作者提出了Transformer-UNet，通过在原始图像中添加Transformer Block而不是在UNet中添加Feature map，并在CT-82数据集中测试本文的网络来进行胰腺分割。在实验中，形成了一个端到端的网络，并获得了比以往许多基于Unet的算法更好的分割结果。</p><h2 id="本文方法">本文方法</h2><p>首先设计一个典型的UNet作为CNN结构，使用双线性插值作为上采样方法，max-pooling作为下采样方法。为了方便实现，作者设计了一个几乎对称的网络，它可以很容易修改注意力模块和Transformer模块。然而，在T-Unet中，编码器和解码器并不直接连接，这将在本节中解释。</p><p>Transformer作为一个以序列数据为输入的模型，对于分割任务Transformer则需要1D数据。因此，需要将一幅$C×H×W$原始图像平展成$C×n^2$维的数组，其中n×n为图像patch的大小，$\frac{HW}{n^2}$为数组序列的长度。遵循Dosovitskiy等人所提的方法，将整个图像分割成不同的平方块，n是正方形边缘的长度。为了简化实现过程，在大多数情况下假设H=W和H，W可以被n整除。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210927/2.png" alt="图1"></p><p>与NLP Transformer略有不同，如图1所示。ViT将LayerNorm放在Multi-Head Attention和MLP之前，以确保输入值不会太大而无法处理。此外，ViT保留了Vaswani等人(2017)的主要设计，如Multi-Head Self-Attention和MLP层。Dosovitskiy等人(2021)进一步添加了一个可学习的数组tensor，用于在将整个序列输入到存储在T-Unet中的Transformer之前进行位置嵌入。</p><p>作者进一步修改ViT，用ELU代替GELU作为在 Transformer MLP层的激活函数，因为作者观察到ELU在实验中表现更好。与RELU和GELU相比，ELU在Transformer中使用较少，其定义为:</p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210927/3.png" width = "300" align=center />  <p>作者认为ELU是有用的，因为CT图像中的负值与正值同样重要。在实验中将超参数α设为1。</p><p>用上面解释的方法，用下列方程形成Transformer模型:</p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210927/4.png" width = "300" align=center />  <img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210927/5.png" width = "300" align=center />  <p>式中，MHA为Multi-Head Attention layers, LN为layer normalization, $x_1,…,x_n$为image patches, $l\in {1,2,…,m}$为Transformer layer number。对于原始图像的处理，在ViT中通过在整个图像上应用一个核大小为$1\times 1$的卷积操作进行Position Embedding过程。</p><p>Transformer在提取局部特征方面不如CNN高效，所以作者遵循UNet的设计在T-Unet中添加了一个附加的编码器。此编码器不直接与解码器连接。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210927/6.png" alt="图2"></p><p>相反，它输出具有不同接受域的特征映射，并将它们与解码器中的特征映射连接起来，如图2所示。解码器以Transformer的输出作为输入，具体来说，对于采用大小为$\frac{HW}{n^2}\times Cn^2$的序列的Transformer，将其输出Reshape为大小为$C\frac{HW}{n^2}\times n\times n$并将其直接送入解码器。通过这样做保证了解码器的输入包含了不同图像patch的信息，因此对最终预测更好。</p><h2 id="复现">复现</h2><p>由于在TUnet中处理原始图像，原始图像和图像patch的大小非常重要，因为它们决定了Transformer模型的大小和运行速度。由于选择CT82作为实验数据集，其中包含大小为$512\times 512$的高分辨率CT切片，因此选择$16\times 16$作为图像patch大小，因此构建的序列长度为1024。因此，在实验中解码器的输入尺寸为$1024\times 16\times 16$，进一步通过双线性插值将其Reshape为尺寸为$1\times 512\times 512$。作者按照Ronneberger等人的方法在解码器中添加了连接部分，并相应地构建了编码器。为了最小化模型，同时保持其效率，作者设计的Transformer模块中的注意力头和总层数分别为8和6。</p><h2 id="损失函数">损失函数</h2><p>为了评价模型，通过与其他算法的比较，本文选择了在Binary分割任务中最常用的损失函数binary Cross Entropy(BCE) Loss作为主要损失函数。这个损失函数比较简单，在最终的预测概率图中并不能反映像素之间的关系，所以它更能说明模型是如何连接图片的不同部分的。一般来说，BCE Loss定义为:</p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210927/7.png" width = "400" align=center />  <p>其中N是像素个数，$y_i$是像素$i$的标签，$p_i$是像素$i$的标签在最终的预测映射中为真的概率。根据定义，很明显，这个函数只计算最终预测ixel-bypixel的损失，而不是区域的损失。</p><h2 id="实现细节">实现细节</h2><ul><li><p>数据集的大小对Transformer很重要。通过对CT切片而不是整个CT序列进行处理，可以扩大数据集的大小。</p></li><li><p>基于MLP的Transformer占用了大量的图形存储空间。因此，Transformer不会大量增加权重文件的大小，因此更适合于2D图像。</p></li></ul><p>因此，在实验中处理CT切片，并将TUnet与现有模型Unet、Attention Unet和TransUnet进行比较。为了使模型更好地处理数据，作者将整个图像用1024进行分割，1024是数据集中所有CT切片的近似最大绝对值。</p><h2 id="实验">实验</h2><h3 id="结果分析">结果分析</h3><p>作者的主要评价方法是多个验证指标，包括mIOU值和最终预测的Dice score。CT82数据集被分离为60/22进行训练和测试。在模型中，最低分辨率为16×16，这也适用于Unet, Attention Unet和TransUnet。</p><p>为了证明结果，作者将阈值设置为0.8(即，最终预测图中值大于0.8的像素将被视为胰腺点)，在计算mIOU和像素精度值时，不仅要考虑胰腺分割的准确性，还要考虑背景的识别。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210927/8.png" alt="表1（上） 表2（下）"></p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210927/9.png" alt="图3"></p><p>图3显示了Transformer的一个主要优点，这使得模型可以使用几个Transformer层在全局和局部进行特性提取工作。</p><p>表1显示了Unet的性能和它的方差，包括TUNet。以深层Unet模型为Backbone，本文的模型能够超越UNet及其相关网络，包括目前流行的Attention Unet。</p><p>表2显示了不同模型的大小和推理时间，本文模型并没有带来特别大的参数量和推理速度。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210927/10.png" alt="图4"></p><p>图4显示了不同网络的可视化结果，TUnet由于使用了Transformer，能够对长距离像素对进行很好的分割，因此优于以往其他基于Unet的网络。</p><h3 id="方差分析">方差分析</h3><p>在实验中，选择n=16作为图像patch的大小。然而，还有许多其他选项，这表明16可能不是TUnet的理想值，进一步对n=32，32进行实验。</p><p>TUnet的另一个重要特征是deep and large Unet backbone。然而，Unet和Attention Unet在浅层模型中仍然有用。由于深度模型不像浅模型那样方便，因为它们自然需要更好的硬件，如gpu，所以进一步尝试浅模型Unet Backbone。在较浅的模型中Unet中减少了1/3层CNN，并将kernel数量减少到1/4。整个模型仍然是端到端的从头到尾地训练原始模型。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210927/11.png" alt="表3"></p><p>从表3可以看出，对于T-Unet来说，$16\times 16$是Transformer的最佳分辨率，而高分辨率会降低Transformer的效率，因为同时阵列序列的长度也在减少，而这对于Transformer的自注意力层是必不可少的。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210927/12.png" alt="表4"></p><p>从表4中可以看出，当使用浅层网络作为Backbone时，T-Unet没有明显的优势。因此，Transformer提取的抽象特征可能需要更深层次的模型进行解码。</p><h2 id="参考">参考</h2><p>[1].Transformer-Unet: Raw Image Processing with Unet<br></p>]]></content>
      
      
      <categories>
          
          <category> Transformer </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
            <tag> Transformer-Unet </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>85FPS！CNN+Transformer语义分割的又一境界，真的很快！</title>
      <link href="/2021/09/23/17/"/>
      <url>/2021/09/23/17/</url>
      
        <content type="html"><![CDATA[<p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210923/1.png" alt=""></p><blockquote><p>本文提出了一种用于城市场景语义分割的高效混合Transformer(EHT),其利用CNN和Transformer结合学习全局-局部上下文来加强特征表征,性能优于ABCNet等网络,速度高达83.4FPS!代码将开源!<br><strong>作者单位</strong>:武汉大学,兰卡斯特大学等</p></blockquote><h2 id="简介">简介</h2><p>高分辨率城市场景图像的语义分割在土地覆盖制图、城市变化检测、环境保护和经济评估等广泛的实际应用中起着至关重要的作用。卷积神经网络采用分层特征表示,具有很强的局部上下文特征提取的能力。然而,卷积层的局部特性限制了网络捕获全局信息,而这个特点对于改善高分辨率图像分割至关重要。</p><p>最近, Transformer成为计算机视觉领域的热门话题。Vision Transformer也展示了其全局信息建模的强大能力,推动了许多视觉任务,例如图像分类、目标检测,尤其是语义分割。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210923/2.png" alt=""></p><p>在本文中提出了一种用于城市场景图像语义分割的高效混合Transformer(EHT)。EHT利用CNN和ransformer结合设计学习全局-局部上下文来加强特征表示。</p><p>大量实验表明,与最先进的方法相比, EHT具有更高的效率和具有竞争力的准确性。具体来说,所提出的EHT在UAVid测试集上实现了67.0%的mloU,并且明显优于其他轻量级模型。</p><h2 id="本文方法">本文方法</h2><p>所提出的efficient hybrid Transformer如图所示。将Global-Local Transformer Block附加到ResNet18 Backbone的顶部，就像BottleNeck Transformer一样。利用3个具有3个跨尺度连接的跨尺度融合模块来聚合多层特征。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210923/3.png" alt=""></p><h3 id="Global-local-Transformer-Block">Global-local Transformer Block</h3><p>提出的Global-local Transformer Block(GLTB)的细节如下图所示。主要模块global-local attention block是一种混合结构，采用linear multi-head self-attention捕获全局上下文信息，采用卷积层提取局部上下文信息。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210923/4.png" alt=""></p><p>最后，对全局上下文和局部上下文应用一个add操作来提取全局-局部上下文。</p><h4 id="Linear-multi-head-self-attention">Linear multi-head self-attention</h4><p>本文提出了一种线性注意力机制，用泰勒展开的一阶近似来代替softmax函数。本文将线性注意力改进为线性多头自注意力，以获得更高的效率和更强的序列建模。具体公式推导过程如下:</p><p>设归一化函数为softmax，则自注意力注意产生的结果矩阵的第$i$行可表示为:<br><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210923/5.png" width = "300" align=center /></p><p>其中$v_j$是第$j$个特征。根据泰勒的扩展:</p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210923/6.png" width = "300" align=center /><p>为了保证上述近似是非负的，$𝒒_𝑖$和$𝒌_𝑗$被归一化$𝑙_2 -norm$,从而确保$q_i^Tk_j≥−1$:</p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210923/7.png" width = "400" align=center /><p>因此，(1)式可以重写为(4)式，并简化为(5)式:</p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210923/8.png" width = "400" align=center /><p>进而有：<br><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210923/9.png" width = "400" align=center /></p><p>上式可以转化为矢量形式：<br><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210923/10.png" width = "400" align=center /></p><p>$\sum_{j=1}^N(\frac{k_j}{||k_j||_2})v_j^{T}$   和</p><p>$\sum_{j=1}^N(\frac{k_j}{||k_j||_2})$</p><p>可以计算得到并可以为每个query重用。</p><p><strong>注意：</strong> 在线性多头自注意力的输出上部署了一个可训练的尺度因子，以实现稳定的上下文聚合。</p><h4 id="Locality-enhanced模块">Locality-enhanced模块</h4><p>采用2个并行卷积层，然后是一个BN操作来提取局部上下文信息。</p><p>生成的全局局部上下文进一步进行深度卷积、批归一化操作和$1\times 1$卷积，以增强泛化能力。</p><h3 id="Cross-scale融合模块">Cross-scale融合模块</h3><h4 id="Cross-scale连接">Cross-scale连接</h4><p>采用两个并行卷积层，然后是一个BN操作来提取局部上下文信息。Cross-scale连接的细节如下图所示。上采样操作的比例因子为2。L为重复次数。3个跨尺度连接对应3个跨尺度融合模块。3个跨尺度连接的Atrous卷积扩张率分别为6、12和18。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210923/11.png" alt=""></p><h4 id="加权特征融合">加权特征融合</h4><p>将Cross-scale连接生成的3种语义特征通过加权元素求和运算与相应的残差特征和上采样的全局局部语义特征进行聚合，以增强泛化能力。公式如下:<br><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210923/12.png" width = "600" align=center /></p><p>其中$f_{\mu}$为Resize操作，用来统一$GRF_{i+1}$和$CSF_i$；$f_{\delta}$为$1\times 1$卷积操作,用来统一$RF_i$和$CSF_i$通道的数量；而$\alpha_1,\alpha_2,\alpha_3$为3个特征的权重系数，其中$\alpha_1+\alpha_2+\alpha_3=1$。</p><p>进一步聚合$GFL_1,GFL_@,GFL_3,GFL_4$作为Head的输入，用于最终的分割。</p><h2 id="实验">实验</h2><p><strong>Backbone</strong>：可以通过ResNet-18和像UNet一样的逐层特征融合来构建。</p><p><strong>Backbone+CFM</strong>：用跨尺度融合模块代替逐层特征融合来构建一个简单的变体。利用该变体验证了跨尺度融合模块的有效性。</p><p><strong>Backbone+CFM+GLTB</strong>：将Global-Local Transformer块插入到Baseline+CFM来生成整个EHT，可以证明所提方法的有效性。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210923/13.png" alt=""></p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210923/14.png" alt=""></p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210923/15.png" alt=""></p><p>可以看出本文所提模块可以很好的兼顾全局和局部的上下文信息，值得小伙伴们进行学习和借鉴。</p><h2 id="参考">参考</h2><p>[1].Efficient Hybrid Transformer: Learning Global-local Context for Urban Sence Segmentation<br></p>]]></content>
      
      
      <categories>
          
          <category> Transformer </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CNN </tag>
            
            <tag> Transformer </tag>
            
            <tag> 语义分割 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>详解分析 | ViT如何在医疗图像领域替代CNNs？</title>
      <link href="/2021/08/23/37/"/>
      <url>/2021/08/23/37/</url>
      
        <content type="html"><![CDATA[<p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210823/1.png" alt=""></p><h2 id="简介">简介</h2><p>在自动医学图像诊断的领域中卷积神经网络(CNN)方法已经统治了将近十年之久。最近，vision transformers(ViTs)作为CNN的一个有竞争力的替代方法出现了，它具有差不多的性能，同时还具有一些有趣的特性，同时也已经被证明对医学成像任务有益。</p><p>在这项工作中，作者探讨了是时候用基于transformer的模型了？还是应该继续使用CNN，还是可以简单地切换到transformer？</p><p>如果是，那么切换到vit进行医学影像诊断有哪些优点和缺点？作者在3种主流医学图像数据集上进行了一系列实验来考虑这些问题。</p><p>-研究结果表明，虽然CNN在从头开始训练时表现更好，但在ImageNet上预训练时，使用默认超参数的vision transformer与CNN相当，而在使用自监督预训练时vision transformer则优于CNN。</p><h2 id="介绍">介绍</h2><p>对于vision transformer来说，注意力机制提供了几个关键的优势:</p><ol><li>它捕获了long-range relationships；</li><li>它具有通过动态进行自适应建模的能力；</li><li>它提供了一种内置的显著性，可以洞察模型关注于的是什么。</li></ol><p>然而，有证据表明，vision transformer需要非常大的数据集才能超过CNN，ViT的性能只有在谷歌私有图像数据集JFT-300M进行预训练才能够得到体现。这个问题在医学成像领域尤其严重，因为该领域的数据集更小，往往伴有不太可靠的标签。</p><p>与ViT一样，当数据匮乏时，CNN的性能会更差。标准的解决方案是使用迁移学习:通常，模型在ImageNet等较大的数据集上进行预训练，然后使用较小的专门数据集对特定任务进行微调。</p><p>在医学领域，在ImageNet进行预训练的模型在最终表现和减少的训练时间方面都优于从零开始训练的模型。</p><p>自监督是一种处理未标记数据的学习方法，近年来受到了广泛关注。已有研究表明，在进行微调之前，在目标域进行自监督预训练可以提高CNN的性能。同时从ImageNet初始化有助于自监督CNN收敛更快，通常也具有更好的预测性能。</p><p>这些处理医学图像领域数据匮乏的技术已被证明对CNN有效，但目前尚不清楚vision transformer是否同样受益。一些研究表明，使用ImageNet进行医学图像分析的预训练CNN并不依赖于特征重用，而是由于更好的初始化和权重缩放。那么vision transformer是否能从这些技术中获益？如果可以，就没有什么能阻止vit成为医学图像的主导架构。</p><p>在这项工作中，作者探索了vit是否可以替代CNNs，同时考虑到易用性、数据集限制以及计算限制，作者着眼于“即插即用”解决方案。为此，作者在3个主流的公开数据集上进行了实验。通过这些实验发现:</p><ol><li>在数据有限时，CNNs与ViTs在ImageNet上预训练的性能差不多；</li><li>迁移学习有利于ViTs；</li><li>当使用自监督预训练之后再用有监督的微调时，ViTs比CNNs表现更好。</li></ol><p>这些发现表明，医学图像分析可以从CNN无缝过渡到ViTs，同时获得更好的可解释性。</p><h2 id="本文方法">本文方法</h2><p>作者研究的主题是，ViTs是否可以直接替代CNNs用于医疗诊断任务。为此，作者进行了一系列实验，在类似条件下比较ViTs和CNNs，保持超参数调优到最小。为了确保比较的公平性和可解释性，作者选择ResNet50作为CNN模型，使用$16\times 16$ token作为ViT的DEIT-S。之所以选择这些模型，是因为它们在参数数量、内存需求和计算方面具有可比性。</p><p>如上所述，当数据不够丰富时，CNNs依赖于初始化策略来提高性能，医学图像就是如此。标准的方法是使用迁移学习（用ImageNet上预训练的权值初始化模型），并在目标域上进行微调。</p><p>因此，作者考虑3种初始化策略:</p><ol><li>随机初始化权值</li><li>使用ImageNet预训练权值进行迁移学习</li><li>初始化后对目标数据集进行自监督预训练学习</li></ol><p>数据增强策略：</p><ul><li>normalization;</li><li>color jitter：<ul><li>brightness</li><li>contrast</li><li>saturation</li><li>hue</li></ul></li><li>horizontal flip</li><li>vertical flip</li><li>random resized crops</li></ul><p>数据集：</p><ul><li>APTOS 2019</li></ul><blockquote><p>在这个数据集中，任务是将糖尿病视网膜病变图像分类为疾病严重程度的5类。APTOS 2019包含3662张高分辨率视网膜图像。</p></blockquote><ul><li>ISIC 2019</li></ul><blockquote><p>这里的任务是将25333张皮肤镜图像在9种不同的皮肤病变诊断类别中进行分类。</p></blockquote><ul><li>CBIS-DDSM</li></ul><blockquote><p>该数据集包含10239张乳房x线照片，任务是检测乳房x线照片中肿块的存在。</p></blockquote><p>数据集被分为train/test/valid(80/10/10)，除了APTOS，由于其规模小，APTOS被分为70/15/15。所有监督训练都使用ADAM优化器，基本学习率为$10^{-4}$，warm-up周期为1000次迭代。当验证指标达到饱和时，学习率会下降10倍，直到达到最终值。重复每个实验5次，并选择每次运行中验证分数最高的checkpoint。</p><h2 id="实验">实验</h2><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210823/2.png" alt="表1"></p><h3 id="4-1-随机初始化Transformer模型是否有效？">4.1 随机初始化Transformer模型是否有效？</h3><p>将DEIT-S与具有随机初始化权值(Kaiming初始化)的ResNet50进行比较。在这些实验中，通过网格搜索将基础学习率设置为0.0003。</p><p><strong>表1的结果表明，在这种设置下，CNNs在各方面都大大优于ViTs。</strong></p><p>这些结果与之前在自然图像领域的观察结果一致，在有限的数据上训练CNNs优于ViTs，这一趋势归因于ViT缺乏归纳偏差。由于大多数医学影像数据集大小适中，随机初始化的ViTs的用处似乎有限。</p><h3 id="4-2-ImageNet上预训练ViTs是否适用于医学图像领域">4.2 ImageNet上预训练ViTs是否适用于医学图像领域?</h3><p>在医学图像数据集中，随机初始化在实际应用中很少使用。标准步骤是使用ImageNet预训练网络，然后对来自目标域的数据进行微调。</p><p>在这里，作者也研究了这种方法是否可以有效地应用于ViTs。为了测试这一点，作者用在ImageNet上预训练过权重初始化所有模型。然后进行微调。表1中的结果表明，CNNs和ViTs都从ImageNet初始化中得到了显著提升。事实上，ViTs受益更多，表现与CNN相当。</p><p>这表明，当使用ImageNet初始化时，可以用普通的ViTs替换CNNs，而不会影响使用中等规模训练数据的医学成像任务的性能。</p><h3 id="4-3-ViT是否能从医疗图像领域的自监督中获益">4.3 ViT是否能从医疗图像领域的自监督中获益?</h3><p>表1中结果显示，ViTs和CNNs在自监督的预训练中表现得更好。在这种情况下，ViTs的表现优于CNNs，尽管差距很小。对自然图像的研究表明ViTs和CNNs将随着更多的数据增长。</p><h2 id="讨论">讨论</h2><p>作者比较了3种初始化策略下的医学图像任务cnn和vit的性能。实验结果证实了之前的发现，并提供了新的见解。</p><p>在医学图像中，正如之前在自然图像领域所说的那样，作者发现，在低数据模式下从零开始训练时，cnn优于vit。这一趋势在所有数据集上都是一致的，并且很好地符合“Transformer缺乏归纳偏差”的论点。</p><p>令人惊讶的是，当使用监督ImageNet预训练权重初始化时，CNN和ViT性能之间的差距在医疗任务中消失了。在cnn上进行ImageNet预训练的好处是众所周知的，但出乎意料的是，ViTs的受益也如此之大。这表明，可以通过与任务更密切相关的其他领域的迁移学习获得进一步的改进，cnn的情况就是如此。</p><p>作者研究了自监督预训练对医学图像域的影响。研究结果表明，vit和cnn有微小但一致的改善。而最佳的整体性能是使用自监督+ViTs获得的。</p><p>总结发现，对于医学图像领域:</p><ul><li>如果从零开始训练，那么在低数据下，vit比cnn更糟糕；</li><li>迁移学习在cnn和vit之间架起了桥梁;性能是相似的；</li><li>最好的表现是通过自监督预训练+微调获得的，其中ViTs比CNNs有小的优势。</li></ul><h2 id="可解释性">可解释性</h2><p>在医学图像任务中，vit似乎可以取代cnn，还有其他选择vit而不是cnn的原因吗?</p><p>我们应该考虑可视化transformer attention maps的额外好处。transformer的自注意机制内置了一个attention maps，它提供了模型如何做出决策的新方式。</p><p>cnn自然不适合把自己的突出形象表现出来。流行的CNN可解释性方法，如类激活映射(CAM)和grada-CAM，由于池化层的存在，提供了粗糙的可视化。与CNN有限的接受域相比，transformer token提供了更精细的注意力图像，而自注意映射明确地模拟了图像中每个区域之间的交互。虽然可解释性的质量差异还有待量化，但许多人已经注意到transformer的注意力在可解释性方面所带来的质量改进。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210823/3.png" alt="图1"></p><p>图1中展示了来自每个数据集的示例，以及ResNet-50的grade-cam可视化和$16\times 16$ DEIT-S CLS token的前50%自注意。注意ViTs的自注意如何提供一个清晰的、局部的注意力图，例如ISIC的皮肤病变边界的注意力，APTOS的出血和渗出物的注意力，以及CBIS-DDSM的乳腺致密区域的注意力。这种关注粒度很难通过cnn实现。</p><h2 id="参考">参考</h2><p>[1].Is it Time to Replace CNNs with Transformers for Medical Images?<br></p>]]></content>
      
      
      <categories>
          
          <category> 论文解读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ViT </tag>
            
            <tag> 医疗图像领域 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>详细解读 | 如何让你的DETR目标检测模型快速收敛</title>
      <link href="/2021/08/16/36/"/>
      <url>/2021/08/16/36/</url>
      
        <content type="html"><![CDATA[<p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210816/1.png" alt=""></p><blockquote><p>本文作者提出了<strong>一种conditional cross-attention mechanism用于快速训练DETR</strong>，从decoder embedding中学习了一个conditional spatial query用于decoder multi-head cross-attention；实验结果表明，对于Backbone R50和R101，条件DETR收敛速度快6.7倍;对于backboone DC5-R50和DC5-R101，条件DETR收敛速度快10倍。</p></blockquote><h2 id="简介">简介</h2><p>最近发展起来的DETR方法将transformer编解码器体系结构应用于目标检测并取得了很好的性能。在本文中，作者解决了训练收敛速度慢这一关键问题，并提出了一种conditional cross-attention mechanism用于快速训练DETR。作者动机是cross-attention在DETR中高度依赖content embeddings定位的4端和预测框，这增加了对高质量content embedding的需求进而增加了训练的难度。</p><p>本文方法被命名为<strong>条件DETR</strong>，从decoder embedding中学习了一个conditional spatial query用于decoder multi-head cross-attention。其好处是，通过conditional spatial query每个cross-attention head能够关注包含不同区域的band（例如，一个目标端点或目标box内的一个区域）。这缩小了目标分类和box回归的不同区域定位的空间范围，从而减轻了对content embedding的依赖，减轻了训练。</p><p>实验结果表明，对于Backbone R50和R101，条件DETR收敛速度快6.7倍;对于backboone DC5-R50和DC5-R101，条件DETR收敛速度快10倍。</p><h2 id="背景">背景</h2><p>DETR方法将transformer应用于目标检测取得了良好的性能。它有效地消除了许多手工制作组件的需要，包括NMS和Anchor生成。</p><p>DETR方法在训练上收敛缓慢，需要500个epoch才能取得良好的效果。Deformable DETR通过使用高分辨率和多尺度编码器将global dense attention(self-attention和cross-attention)替换为deformable attention来解决这个问题。相反，本文仍然使用global dense attention并提出了一个改进的 decoder cross-attention mechanism以加速训练收敛的过程。</p><p>本文方法的动机是高度依赖content embeddings和spatial embeddings in cross-attention。实验结果表明，如果从第2解码器层去除key和query中的位置嵌入，只使用key和query中的content embeddings，检测AP略有下降（1%）。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210816/2.png" alt="图1"></p><p>图1(第2行)显示了50个epoch训练的DETR cross-attention的spatial attention weight maps。可以看到，4个映射中有2个没有正确地突出对应端点的波段，因此在缩小内容查询的空间范围以精确定位端点方面很弱。其原因是:</p><ol><li>spatial query：即目标query，只给出general attention weight map，而没有利用具体的图像信息;</li><li>由于训练时间短content embeddings不够强，不能很好地匹配spatial key，因为它们也用于匹配content key。这增加了对高质量content embeddings的依赖性，从而增加了训练难度。</li></ol><p>本文提出了一种有条件的DETR方法，该方法从之前对应的解码器输出嵌入中学习每个query的条件spatial embedding，形成decoder multi-head<br>cross-attention的条件spatial query。通过将用于回归目标框的信息映射到嵌入空间来预测条件spatial query。</p><h2 id="条件DETR">条件DETR</h2><h3 id="方法概览">方法概览</h3><p>该方法采用端到端目标检测器(detection transformer, DETR)，无需生成NMS或Anchor即可一次性预测所有目标。该体系结构由CNN Backbone、transformer encoder、transformer decoder、目标分类器和边界框位置预测器组成。transformer encoder的目的是改进CNN Backbone的content embeddings输出。它是由多个编码器层组成的堆栈，其中每一层主要由self-attention层和feed-forward层组成。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210816/3.png" alt="图3"></p><p>transformer decoder是一堆decoder layer。每个decoder layer如图3所示，由3个主要层组成:</p><ol><li><strong>self-attention layer</strong>：用于去除重复预测，执行前一解码器层输出的嵌入之间的交互，用于类和box的预测;</li><li><strong>cross-attention layer</strong>：该层聚合编码器输出的embedding以细化解码器embedding改进类和box预测;</li><li><strong>feed-forward layer</strong></li></ol><h4 id="Box回归">Box回归</h4><p>从每个decoder embedding中预测一个候选框，如下所示:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210816/4.png" alt=""></p><p>这里，$f$是decoder embedding。$b$是一个4维矢量$[b_{cx} b_{cy} b_{w} b_{h}]^T$，由框的中心、框的宽度和框的高度组成。Sigmoid()用于将预测b归一化到范围[0,1]。FFN()的目的是预测非规范化框。在原始DETR中为(0,0),s为参考点的非归一化二维坐标。在本方法中，作者考虑2个选择:将参考点s作为每个候选框预测的参数学习，或者从相应的目标query中生成它。</p><h4 id="类别预测">类别预测</h4><p>每个候选框的分类score也通过FNN预测：<br>$$e=FFN(f)$$</p><h4 id="Main-work">Main work</h4><p>cross-attention mechanism的目的是定位不同的区域（用于box检测的4个端点和box内用于目标分类的区域）并聚合相应的嵌入。本文提出了一种条件cross-attention mechanism，通过引入conditional spatial query来提高定位能力和加速训练的收敛过程。</p><h3 id="DETR-Decoder-Cross-Attention">DETR Decoder Cross-Attention</h3><p>DETR解码器cross-attention mechanism有3个输入:query、key和value。每个key都是通过添加一个content key $c_k$(编码器的content embedding输出)和一个spatial key $p_k$(对应的标准化2D坐标的positional embedding)形成的。该value是由编码器输出的content embedding(与content key相同)形成的。</p><p>在原始的DETR方法中，每个query是通过添加content query $c_q$(decoder self-attention embedding)和spatial query $p_q$(即object query $o_q$)形成的。在实现中，有N=300个object queries，相应地有N个query，每个query在一个解码器层输出一个候选检测结果。</p><p>attention weight是基于query与key的点积:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210816/5.png" alt=""></p><h3 id="Conditional-Cross-Attention">Conditional Cross-Attention</h3><p>提出的Conditional Cross-Attention将解码器self-attention输出的content query $c_q$和spatial query $p_q$串联起来形成query。因此，同理，key由content key $c_k$和spatial key $p_k$拼接而成。</p><p>cross-attention weight由content attention weight和spatial attention weight两部分组成。这两个权重来自两个点积，content和spatial点积：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210816/6.png" alt=""></p><p>与原来的DETR Cross-Attention不同，本文所提的机制分离了content query和spatial query的角色，使spatial query和content query分别关注spatial和content的attention weight。</p><p>另外一个重要的任务是从前一个解码器层的embedding $f$计算spatial query $p_q$。首先识别出不同区域的空间信息是由解码器 embedding 和参考点这两个因素共同决定的。然后展示了如何将它们映射到embedding space形成query $p_q$，使spatial位于key的2D坐标映射到的同一空间。</p><p>解码器embedding包含不同区域相对于参考点的位移。式1中的box预测过程包括2个步骤:</p><ol><li><p>对非归一化空间中的参考点进行预测;</p></li><li><p>将预测框归一化到范围[0,1];</p></li></ol><p>步骤(1)表示decoder embedding f包含了构成方框的4个端点相对于非归一化空间中的参考点s的位移。这意味着，无论是embedding f还是参考点s，都需要确定不同区域、4个极值以及预测分类评分的区域的空间信息。</p><h4 id="Conditional-spatial-query-prediction"><strong>Conditional spatial query prediction</strong></h4><p>通过embedding f和参考点s预测条件空间查询，</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210816/7.png" alt=""></p><p>以便与key的标准化2D坐标映射到的位置空间对齐。这个过程如图3的灰色框区所示。</p><p>这里将参考点归一化，然后将其映射到256维正弦位置嵌入，方法与key的位置嵌入相同:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210816/8.png" alt=""></p><p>然后通过可学习线性投影+ReLU+可学习线性投影组成的FFN将解码器embedding f中包含的位移信息映射到同一空间中的线性投影:</p><p>$$T=FFN(f)$$</p><p>conditional spatial query通过转换embedding空间中的参考点来计算:<br>$$p_q = Tp_s$$</p><p>作者选择简单和计算效率高的对角矩阵。256个对角线元素被表示为一个向量$\lambda_q$。conditional spatial query通过逐元素乘法计算:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210816/9.png" alt=""></p><h4 id="Multi-head-cross-attention"><strong>Multi-head cross-attention</strong></h4><p>与DETR一样作者采用标准的Multi-head cross-attention机制。目标检测通常需要隐式或显式定位目标的4个端点以实现精确的box回归，并定位目标区域以实现精确的目标分类。multi-head mechanism有利于解决定位任务的纠缠问题。</p><p>作者通过将query、key和value M=8次投影到低维的线性投影来执行multi-head parallel attentions。spatial和content query(key)分别以不同的线性投影投影到每个head。value投影与原始的DETR相同，仅用于content。</p><h3 id="可视化分析">可视化分析</h3><p>图4可视化了每个attention weight maps:</p><ul><li>spatial attention weight maps</li><li>content attention weight maps</li><li>combined attention weight maps</li></ul><p>对spatial dot-products $p_q^Tp_k$、 content dot-products $c_q^Tc_k$和combined dot-products $c_q^Tc_k+p_q^Tp_k$进行softmax normalized。图中显示了8个map中的5个其他3个是重复的，对应于底部和顶部的端点，以及目标框内的一个小区域。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210816/10.png" alt="图4"></p><p>可以看到，每个head的spatial attention weight maps能够定位一个不同的区域（包含一个极点的区域或物体box内的区域）。有趣的是，每个spatial attention weight maps对应的一个极点突出了一个空间带，该空间带与目标框的相应边缘重叠。目标框内区域的另一个spatial attention weight maps仅仅突出显示了一个小区域，该区域的表示可能已经编码了足够的目标分类信息。</p><p>content attention weight maps还突出了分散的区域。空间和内容映射的组合过滤掉了其他高亮部分，并保留了极端高亮部分以实现精确的box回归。</p><h4 id="Comparison-to-DETR">Comparison to DETR</h4><p>图1显示了条件式DETR(第1行)和经过50个epoch训练的原始DETR(第2行)的spatial attention weight maps。本文方法的映射是通过spatial key和query之间的dot$(p_q^Tp_k)$的softmax normalized来计算的:</p><p>$$(o_q + c_q)^T p_k$$</p><p>可以看出spatial attention weight maps准确定位了不同的区域。相比之下，原始的DETR中包含50个epoch的map不能准确定位2个极点，而500个训练epoch(第3行)使得content query更强，从而实现了精确定位。这意味着学习content query $c_q$作为2个角色(同时匹配content key和spatial key)是非常困难的，因此需要更多的训练epoch。</p><h4 id="分析">分析</h4><p>图4所示的spatial attention weight maps暗示用于形成spatial query的conditional spatial query至少有2种效果：</p><ol><li>将突出显示的位置转换为4个端点和目标框内的位置:有趣的是，突出显示的位置在目标框内的空间分布相似；</li><li>缩放顶端亮点的空间扩展:大目标的空间扩展大，小目标的空间扩展小。</li></ol><p>这2种效果是在spatial embedding space中通过T/ps变换实现的(通过cross-attention中包含的独立于图像的线性投影进一步分离，并分布到每个head)。这说明变换T不仅包含前面讨论的位移，还包含目标尺度。</p><h2 id="实验">实验</h2><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210816/11.png" alt="表1"></p><p>表1给出了DETR和条件DETR的结果。具有50个训练期的DETR比500个训练期的表现差得多。</p><p>对于R50和R101具有50个训练周期的条件DETR作为backbone，其表现略低于具有500个训练周期的DETR。</p><p>对于DC5-R50和DC5-R101，带有50个训练周期的条件DETR的性能与带有500个训练周期的DETR相似。</p><p>4个backbone 75/108个训练周期的条件DETR优于500个训练周期的DETR。</p><p>总之，高分辨率backbone DC5-R50和DC5-R101的有条件DETR比原始的DETR快10倍，低分辨率backbone R50和R101快6.67倍。换句话说，有条件的DETR对于更强大的backbone和更好的性能表现得更好。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210816/12.png" alt="表2"></p><p>表2中显示，在DC5-R50(16×)上的方法与可变形的方法表现相同DETR-R50(多尺度、8×)。考虑到单尺度可变形DETR-DC5-R50-SS的AP为41.5(低于43.8)(表1)，可以看到，可变形的DETR受益于多尺度和高分辨率编码器。</p><p>本文方法的性能也与TSP-FCOS TSP-RCNN。这2种方法包含一个在少量选定位置/区域上的transformer编码器(在TSP-FCOS和TSP-RCNN区域提议中感兴趣的特性)，而不使用transformer解码器是FCOS和Faster RCNN的扩展。</p><h2 id="参考">参考</h2><p>[1].Conditional DETR for Fast Training Convergence<br></p>]]></content>
      
      
      <categories>
          
          <category> 论文解读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DETR目标检测模型 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Mobile-Former | MobileNet+Transformer轻量化模型(精度速度秒杀MobileNet)</title>
      <link href="/2021/08/14/35/"/>
      <url>/2021/08/14/35/</url>
      
        <content type="html"><![CDATA[<p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210814/1.png" alt=""></p><blockquote><p>微软提出Mobile-Former，MobileNet和Transformer的并行设计，可以实现局部和全局特征的双向融合，在分类和下游任务中，性能远超MobileNetV3等轻量级网络！<br><strong>作者单位</strong>：微软, 中科大</p></blockquote><h2 id="背景">背景</h2><p>最近，Vision Transformer(ViT)展示了全局处理的优势，与cnn相比实现了显著的性能提升。然而，当将计算预算限制在1G FLOPs内时，增益维特减少。如果进一步挑战计算成本，基于depthwise和pointwise卷积的MobileNet和它的扩展仍然占据着一席之地(例如，少于300M的FLOPs图像分类),这又自然而然地提出了一个问题:</p><h4 id="如何设计有效的网络来有效地编码局部处理和全局交互">如何设计有效的网络来有效地编码局部处理和全局交互?</h4><p>一个简单的想法是将卷积和Vision Transformer结合起来。最近的研究表明，将卷积和Vision Transformer串联在一起，无论是在开始时使用卷积，还是将卷积插入到每个Transformer块中都是有益的。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210814/2.png" alt="图1"></p><p>在本文中，作者将设计范式从串联向并联转变，提出了一种新的MobileNet和Transformer并行化，并在两者之间建立双向桥接(见图)。将其命名为<strong>Mobile-Former</strong>，其中Mobile指MobileNet, Former指transformer。Mobile以图像为输入堆叠mobile block(或inverted bottleneck)。它利用高效的depthwise和pointwise卷积来提取像素级的局部特征。前者以一些可学习的token作为输入，叠加multi-head attention和前馈网络(FFN)。这些token用于对图像的全局特征进行编码。</p><p>Mobile-Former是MobileNet和Transformer的并行设计，中间有一个双向桥接。这种结构利用了MobileNet在局部处理和Transformer在全局交互方面的优势。 并且该桥接可以实现局部和全局特征的双向融合。与最近在视觉Transformer上的工作不同，Mobile-Former中的Transformer包含非常少的随机初始化的token（例如少于6个token），从而导致计算成本低。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210814/3.png" alt=""></p><p>结合提出的轻量级交叉注意力对桥接进行建模，Mobile-Former不仅计算效率高，而且具有更强的表示能力，在ImageNet分类上从25M到500MFLOPs的低 FLOPs机制下优于MobileNetV3。 例如，它在294M FLOPs下实现了77.9%的top-1准确率，比MobileNetV3提高了1.3%，但节省了17%的计算量。在转移到目标检测时，Mobile-Former 比MobileNetV3高8.6 AP。</p><h2 id="相关工作">相关工作</h2><h3 id="2-1-轻量化CNN模型">2.1 轻量化CNN模型</h3><p>mobilenet提出了一种在inverted bottleneck结构中使用depthwise和pointwise卷积对局部处理建模的有效方法。使用group卷积和channel shuffle来简化pointwise卷积的shuffle。此外，MicroNet提出了micro-factorized 卷积，优化了inverted bottleneck和group卷积的组合，在极低的FLOPs下实现了可靠的性能。其他有效的操作包括傅里叶变换、GhostNet中的线性变换，以及在AdderNet中使用廉价的加法替代大规模乘法。此外，还研究了不同的体系结构。MixConv探索了混合多个核大小，Sandglass inverted residual block的结构。EfficientNet和TinyNet研究深度、宽度和分辨率的复合缩放。</p><h3 id="2-2-Vision-Transformers">2.2 Vision Transformers</h3><p>最近，ViT及其后续在多个视觉任务上取得了令人印象深刻的表现。原始的ViT需要在大型数据集(如JFT-300M)上进行训练才能表现良好。后来，DeiT通过引入几个重要的训练策略，证明了在较小的ImageNet-1K数据集上可以获得良好的性能。为实现高分辨率图像的ViT，提出了几种分层Transformer。</p><p>例如，Swin提出了在局部窗口内计算自注意力的移位窗口方法，CSWin通过引入十字形窗口自注意力进一步改进了该方法。T2T-ViT通过递归聚合相邻的token逐步将图像转换为token从而可以很好地建模局部结构。HaloNet开发了两种注意力扩展(blocked local attention和attention downsampling)从而提高了速度、内存使用以及准确性。</p><h3 id="2-3-CNNs与ViT结合">2.3 CNNs与ViT结合</h3><p>近研究结果表明，卷积与Transformer相结合在预测精度和训练稳定性上都有提高。</p><p>通过在ResNet的最后3个bottleneck block中使用全局自注意力替换空间卷积，BoTNet在实例分割和目标检测方面有了显著的改进。</p><p>通过引入门控位置自注意力(GPSA)，ConViT通过soft卷积归纳偏差改进了ViT。</p><p>CvT在每个multi-head attention之前引入了depthwise/pointwise卷积。</p><p>LeViT和ViTC使用convolutional stem (stacking $3\times 3$ convolutions)代替patchify stem。LeViT和ViTC在低FLOP状态下有明显改善。在本文中作者提出了一个不同的设计，并行MobileNet和Transformer之间的双向交叉注意力。本文的方法既高效又有效，在低FLOP状态下优于高效CNN和ViT变种。</p><h2 id="Mobile-Former">Mobile-Former</h2><p>Mobile-Former将MobileNet和transformer并行化，并通过双向交叉注意力将两者连接起来(见图1)。Mobile-former中，Mobile(简称MobileNet)以一幅图像作为输入$(X_0 \in R^{H\times W\times 3})$，采用inverted bottleneck block提取局部特征。前者(指transformer)以可学习参数(或token)作为输入，记为$Z_0\in R^{M\times d}$，其中d和M分别为token的维数和数量。这些token被随机初始化，每个token表示图像的全局先验。这与Vision Transformer(ViT)不同，在ViT中，token线性地投射局部图像patch。这种差异非常重要，因为它显著减少了token的数量从而产生了高效的Former。</p><h3 id="3-1-Low-Cost-Two-Way-Bridge">3.1 Low Cost Two-Way Bridge</h3><p>作者利用cross attention的优势融合局部特性(来自Mobile)和全局token(来自Former)。这里为了降低计算成本介绍了2个标准cross attention计算:</p><ol><li>在channel数较低的MobileNet Bottlneck处计算cross attention;</li><li>在Mobile position数量很大的地方移除预测$(W_i^Q, W_i^K, W_i^V)$，但让他们在Former之中。</li></ol><p>将局部特征映射表示为x，全局token表示为z。它们被分割为$x=[x_h]$和$z=[z_h](1\leq h\leq H)$表示有H个头的多头注意力。从局部到全局的轻量级cross attention定义如下：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210814/4.png" alt=""></p><p>其中$W^Q_h$是第h个head的query投影矩阵，$W^O$用于将多个head组合在一起，Attention($Q,K,V$)是query Q、key K和value V上的标准自注意力函数，如下所示：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210814/5.png" alt=""></p><p>注意，全局特性$z$是query，而局部feat x是key和value。$W_h^Q$和$W^O$应用于全局token z上。这个cross attention如图3(Mobile$\rightarrow$Former)所示。</p><p>以类似的方式，从全局到局部的cross attention计算如下:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210814/6.png" alt=""></p><p>其中$W^K_h$和$W_h^V$是key和value的投影矩阵。在这里，局部feat x是query，而全局feat z是key和value。这种cross attention的图表如图3(Mobile$\leftarrow$Former)所示。</p><h3 id="3-2-Mobile-Former-Block">3.2 Mobile-Former Block</h3><p>Mobile-Former可以解耦为Mobile-Former块的堆栈(见图1)。每个块包括Mobile sub-block、Former sub-block和双向桥接(Mobile$\rightarrow$Former和Mobile$\leftarrow$Former)。Mobile-Former块的细节如图3所示。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210814/7.png" alt="图3"></p><h4 id="1-输入和输出">1 输入和输出</h4><p>Mobile-Former块有2个输入:</p><ol><li><p>局部特征图$X_i \in R^{L×C}$，具有C通道和L空间位置(L=hw，其中h和w为特征图的高度和宽度);</p></li><li><p>全局token $Z_i\in R^{M×d}$，其中M和d分别是token的数量和维数。</p></li></ol><p>Mobile-Former块输出更新后的局部特征映射为$X_{i+1}$和全局token $Z_{i+1}$，用作下一个(i+1)块的输入。注意，全局token的数量和维度在所有块中都是相同的。</p><h4 id="2-Mobile-sub-block">2 Mobile sub-block</h4><p>Mobile sub-block以feature map $X_i$为输入。与MobileNet中的inverted bottleneck block略有不同，在第一次pointwise卷积和$3\times 3$深度卷积后用dynamic ReLU代替ReLU作为激活函数。</p><p>与原来的dynamic ReLU不同，在平均池化特征上使用两个MLP层生成参数，而在前者输出的第一个全局token上使用2个MLP层(图3中的θ)保存平均池化。注意，对于所有块，深度卷积的核大小是$3\times 3$。将Mobile sub-block的输出表示为$X_i^{hidden}$，作为Mobile Former的输入(见图3)，其计算复杂度为O(2LEC^2 + 9LEC)，其中L为空间位置数，E为通道展开比，C为展开前通道数。</p><h4 id="3-Former-sub-block">3 Former sub-block</h4><p>Former sub-block是带有多头注意(MHA)和前馈网络(FFN)的标准transform block。在这里，作者遵循ViT使用后层标准化。为了节省计算，作者在FFN中使用的扩展比为2而不是4。</p><p>Former sub-block之间处理是双向交叉注意力，即(Mobile$\rightarrow$Former和Mobile$\leftarrow$Former)(见图3)。其复杂性为O(M^2d + Md^2)。第1项涉及到计算query和key的点积，以及根据注意力值聚合值。第2项涉及到线性投影和FFN。由于Former只有几个token(m6)，所以第1项M^2d是可以忽略的。</p><h4 id="4-Mobile-rightarrow-Former">4 Mobile$\rightarrow$Former</h4><p>采用所提出的轻量cross attention(式1)将局部特征$X_i$融合到全局token $Z_i$。与标准自注意力相比去掉了key $W^K$和value $W^V$(在局部特征上)的投影矩阵，以节省计算量(如图3所示)。其计算复杂度为O(LMC + MdC)，其中第1项涉及计算局部特征和全局特征之间的cross attention以及为每个全局token聚合局部特征，第2项是将全局特征投影到局部特征C的同一维度并在聚合后返回到维度d的复杂性。</p><h4 id="5-Mobile-leftarrow-Former">5 Mobile$\leftarrow$Former</h4><p>在这里cross attention(公式3)位于移动方向的相反方向。它融合了全局token $Z_i$和局部特征$X_i$。局部特征$X_i$是query，全局token $Z_i$是key和value。因此，保留key $W^K$和value的投影矩阵$W^V$，但在query $W^Q$时去掉投影矩阵以节省计算，如图3所示。计算复杂度为O(LMC + MdC)。</p><h4 id="6-计算复杂度">6 计算复杂度</h4><p>Mobile-Former块的4个支柱有不同的计算成本。Mobile sub-block消耗的计算量最多(O(2LEC^2 + 9LEC))，它与空间位置数L呈线性增长，与局部特征c中通道数呈二次增长。Former sub-block和双向Bridge具有较高的计算效率，消耗小于所有Mobile-Former模型总计算量的20%。</p><h3 id="3-3-网络配置说明">3.3 网络配置说明</h3><h4 id="1-架构">1 架构</h4><p>表1显示了在294M FLOPs上的Mobile-Former架构，它以不同的输入分辨率堆叠11个Mobile-Former块。所有Mobile-Former区块都有6个维度为192的全局token。它以$3\times 3$卷积作为stem开始，随后是lite bottleneck block在stage-1。lite bottleneck block使用$3\times 3$深度卷积来扩展channel数量，并使用pointwise卷积来压缩channel数量。分类head对局部特征应用平均池化，与第一个全局token连接，然后通过2个完全连接的层(层之间使用h-swish)。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210814/8.png" alt=""></p><h4 id="2-Downsample-Mobile-Former-Block">2 Downsample Mobile-Former Block</h4><p>staget 2-5有一个Mobile-Former块的downsample变体(表示为Mobile-Former$\downarrow$)来处理空间下采样。在Mobile-former$\downarrow$中，只有Mobile sub block中的卷积层从3层(点向!深度向!点向)改变为4层(pointwise$\rightarrow$depth$\rightarrow$pointwise)，其中第一个深度卷积层有stride=2。channel的数量在每个深度卷积中扩展，并压缩在接下来的pointwise卷积中。这节省了计算，因为2个代价高昂的pointwise卷积在下采样后以较低的分辨率执行。</p><h4 id="3-Mobile-Former变体">3 Mobile-Former变体</h4><p>Mobile-Former有7个不同计算成本的模型，从26M到508M FLOPs。它们的结构相似，但宽度和高度不同。作者遵循[36]来引用我们的模型的FLOPs，例如Mobile-Former-294M, Mobile-Former-96M。这些Mobile-Former模型的网络架构细节如下表。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210814/9.png" alt=""></p><h2 id="Mobile-Former的可解释性">Mobile-Former的可解释性</h2><p>为了理解Mobile和Former之间的协作，作者将cross attention形象化在双向桥上(Mobile$\rightarrow$Former和Mobile$\leftarrow$Former)（见图4、5、6）。使用ImageNet预训练的MobileFormer-294M，其中包括6个全局token和11个Mobile-Former块。作者观察到3种有趣的现象：</p><h4 id="第1点：">第1点：</h4><p><strong>lower level token的注意力比higher level token更多样化</strong>。如图4所示，每一列对应一个token，每一行对应相应的多头交叉注意中的一个头。注意，在Mobile$\rightarrow$Former(左半部分)中，注意力是在像素上标准化的，显示每个token的聚焦区域。相比之下，Mobile$\leftarrow$Former中的注意力是在token上标准化的，比较不同token在每个像素上的贡献。显然，第3和第5区块的6个token在Mobile$\rightarrow$Former和Mobile$\leftarrow$Former中都有不同的cross attention模式。在第8块中可以清楚地观察到token之间类似的注意力模式。在第12区块，最后5个token的注意力模式非常相似。注意，第1个token是进入分类器头部的class token。最近关于ViT的研究也发现了类似的现象。</p><h4 id="第2点：">第2点：</h4><p>全局token的重点区域从低到高级别逐渐变化。图5显示了Mobile$\rightarrow$Former中第1个token的像素交叉注意力。这个token开始关注局部特性，例如边缘/角(在第2-4块)。然后对像素连通区域进行了更多的关注。有趣的是，聚焦区域在前景(人和马)和背景(草)之间转换。最后，定位识别度最高的区域(马身和马头)进行分类。</p><h4 id="第3点：">第3点：</h4><p>Mobile$\leftarrow$Former的中间层(例如第8块)出现了前景和背景的分离。图6显示了特征图中每个像素在6个token上的cross attention。显然，前景和背景被第一个token和最后一个token分开。这表明，一些全局token学习有意义的原型，聚类相似的像素。</p><h4 id="局限性">局限性</h4><p>Mobile-Former的主要限制是模型大小。这有2个原因：</p><p>首先，由于Mobile，Former和bridge都有各自的参数，因此并行设计在参数共享方面效率不高；虽然Former由于token数量少，计算效率高，但它并不节省参数的数量。</p><p>其次，在执行ImageNet分类任务时，Mobile-Former在分类头(2个全连接层)中消耗了很多参数。例如，Mobile-Former-294M在分类头中花费了40% (11.4M中的4.6M)参数。当从图像分类切换到目标检测任务时，由于去掉了分类头，模型大小问题得到了缓解。</p><h2 id="实验">实验</h2><h3 id="5-1-ImageNet-Classification">5.1 ImageNet Classification</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210814/10.png" alt=""></p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210814/11.png" alt=""></p><h3 id="5-2-Object-Detection">5.2 Object Detection</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210814/12.png" alt=""></p><h2 id="参考">参考</h2><p>[1].Mobile-Former: Bridging MobileNet and Transformer<br></p>]]></content>
      
      
      <categories>
          
          <category> Mobile-Former </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
            <tag> 轻量化模型 </tag>
            
            <tag> MobileNet </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>牛津大学提出PSViT | Token池化+Attention Sharing让Transformer模型不在冗余！！！</title>
      <link href="/2021/08/10/34/"/>
      <url>/2021/08/10/34/</url>
      
        <content type="html"><![CDATA[<p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210810/1.png" alt=""></p><h2 id="简介">简介</h2><p>在本文中，作者观察到在应用ViT进行图像识别时存在两级冗余。首先，固定整个网络的Token数量会在空间层面产生冗余特征；其次，不同Transformer之间的注意力图是冗余的。</p><p>基于上述观察，作者提出了一种PSViT:一种带有Token pooling和Atention共享的ViT以减少冗余的同时有效增强特征表示能力，实现更好的速度-精度权衡。</p><p>具体来说，在PSViT中，Token pooling可以定义为在空间级别上减少token数量的操作。并在相邻的Transformer层之间建立Atention共享，以重用相邻层之间相关性较强的Attention Map。然后，为不同的Token pooling和Atention共享机制构建一个紧凑的可能性组合集。基于所提出的compact set可以将每一层的token数量和Atention共享层的选择视为从数据中自动学习的超参数。</p><p>实验结果表明，与DeiT方法相比，该方法在ImageNet分类中准确率提高了6.6%</p><h4 id="主要贡献">主要贡献</h4><ol><li>提出了一种基于Token pooling和Share Attention的PSViT方案，设计更好的视觉Transformer可以有效地增强特征表示；</li><li>为Token pooling和Share Attention提出了一组紧凑的设计选择，并在此基础上利用现成的AutoML方法学习最佳设计选择；</li><li>综合实验结果表明，与DeiT相比，该算法在ImageNet分类中的准确率提高了6.6%。</li></ol><h2 id="Token-Pooling-and-Sharing-for-Transformer">Token Pooling and Sharing for Transformer</h2><p>![图3]<img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210810/2.png" alt=""></p><h3 id="Token-Pooling">Token Pooling</h3><p>Token Pooling机制调整每个阶段的token数量，如图3所示。当网络深度增加时，本文减少token数量以消除空间冗余，增加特征维数以容纳更多不同的高级特征。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210810/3.png" alt=""></p><p>Token Pooling有2种设计选项：</p><ul><li><strong>第1种方法</strong>：是将图像patch视为1D token，利用额外的CLS token进行分类任务。</li><li><strong>第2种方法</strong>：是去除CLS token，将图像patch保持在一个2D数组中，这与ResNet中的池化策略相同。</li></ul><p>对于第1个策略，通过卷积和Max-Pooling来实现Token Pooling。与只减少token数量不同，本文目标是增强特性表示能力。这里首先利用一维卷积来改变特征维数(即每个token的维数)，然后通过一维最大池化来减少token的数量。</p><p>作者将采用上述池化策略的网络命名为<strong>PSViT-1D</strong>。</p><p>在第2种策略中，采用stride=2的二维卷积层进行Token Pooling，这在许多卷积网络中得到了广泛的应用。</p><p>这里将具有第2种池策略的网络命名为<strong>PSViT-2D</strong>。</p><p>简单地在每一层之后添加一个Token Pooling层会迅速降低模型的表示能力。受到简历深层网络原则的影响，例如VGGNet, ResNet和MobileNet，作者在几层之后添加了一个Pooling层，并用相同的token number作为一个阶段来命名这些层。</p><h4 id="Token-维度研究">Token 维度研究</h4><p>为了增加ViT的表示能力同时保持其计算开销的FLOPs，可以有两种选择来修改网络结构:</p><ul><li>增加标记尺寸而减少令牌号码或保持标记尺寸,</li><li>增加变压器层数。</li></ul><p>基于这2种选择对表1中的1D策略进行了一些初步实验，结果表明<strong>增加特征维数同时减少token数量是更好的选择</strong>。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210810/4.png" alt=""></p><p>具体来说，在实验中，baseline模型固定特征尺寸，而表1中的Dimension1方案固定token维数，并在各个阶段减少token数量。为了平衡总计算量，增加了几个编码器层。表1中的Dimension2方案增加了token的维数，同时减少了token数量。</p><p>这2种方案设计的体系结构遵循计算均匀分布在阶段的原则。从表1中可以看出，这2种token维设计的性能要比通过整个网络固定特征尺寸的DeiT-Tiny模型好得多。在增加特征维数(维数2)的同时减少token数的方法性能最好。</p><h4 id="设计选择分析">设计选择分析</h4><p>在CV任务中有2个特点。首先，正如第1节所讨论的，High-level特征在空间层面上具有冗余。因此，通过Token Pooling在空间层面上降低冗余是合理的。表1中的结果支持这一点，其中2个Token Pooling设计比没有Token Pooling的DeiT-Tiny模型性能要好得多。</p><p>深层网络中的不同层对不同层次的信息进行编码。低层次的特征，例如边缘和纹理，在浅层可以很少，可以共享来代表高级特征。相反，高层次的特征，例如不同视角的属性或对象，在更深层次上更难以共享。因此，大多数CNN的设计，如VGGNet、ResNet和MobileNet遵循的规则是越深层特征维度越高。表1中的结果也验证了这一点，其中在维度2中增加更深层次的特征维比在维度1中固定特征维表现得更好。</p><h3 id="Attention-Sharing">Attention Sharing</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210810/5.png" alt=""></p><p>如图所示，连续多注意层次中的attention map是相似的。其中一个原因是，在transformer中使用了标识映射，并使用残差。在这种情况下，主要特征虽然被残差改变了，但不会有很大的变化。因此，相邻层的特征会彼此相似，从而产生相似特征之间的attention map。这种相似性导致了冗余。因此，采用一些共享机制来减少冗余，同时减少计算量。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210810/6.png" alt=""></p><p>具体来说，通过图©所示的相邻层之间的注意力计算过程来实现注意力共享的目的。如果一层重用了前一层的注意力分数，那么就不计算注意力分数以及Q和K。前面一层计算出的attention作为这一层的输入attention计算的整个过程。self-attention被简化为V与输入注意力分数的点积。</p><p>Attention Sharing可以帮助消除相邻transformer层之间attention map冗余。另一方面，一些相邻的图层可能具有非常不同的功能，共享它们的注意力图就不那么有效了。考虑到这一点，应该提供灵活性，使整个ViT仍然可以选择使用原来的多头注意力模块，而不sharing attention map。因此，在设计transformer整体架构时将Attention Sharing模块作为对原有独立多头注意力模块的可选方案。</p><h2 id="AutoML-Enhanced-Transformer">AutoML Enhanced Transformer</h2><h3 id="Search-Space-for-PSViT">Search Space for PSViT</h3><p>如前所述，token pooling是指在空间层面上减少token数量，增加每个token的特征通道数量的操作，这种操作可能会极大地影响特征的表示能力。因此，在ViT中特别考虑它的不同形式。</p><p>每个阶段的设计选择主要包括3个因素:</p><ul><li>token数量$N_t$，</li><li>token维数$N_f$，</li><li>每个阶段的层数$N_b$。</li></ul><p>为了得到这3个因素的最优选择构建了一个对它们有多个选择的搜索空间。搜索空间中的每个元素都将产生一个新的transformer候选网络体系结构。</p><p>在每一层中，token数量有$S_t$种可能的选择，token维度有$S_f$种可能的选择，每个map使用attention map有$S_s$种可能的选择。对于L层，仅考虑token pooling的搜索空间将包含$(S_t·S_f·S_s)^L$候选设计。例如，当$S_t$= 4, $S_f$= 4, $S_s$= 4, L= 36时，大约有$1.1\times 10^{65}$个选项，约$9.6\times 10^{52}$倍的SPOS方法的大小。相比之下，SPOS每层有4个选择，而这个搜索空间每层有64个。这个搜索空间太大，每一层都有太多的选择。使用搜索算法从这个搜索空间中获得最好的架构花费了太多的时间。此外，每一层的选择过多可能会使现有的快速搜索算法失效。因此，需要缩小搜索空间。</p><p>设计搜索空间的原因如下:</p><ul><li>根据3.2.1节的分析，随着深度的增加，token维数增加，token数减少。这有助于通过删除不符合此规则的网络架构来减少搜索空间；</li><li>遵循成熟的CNN设计，限制一个阶段中的多个层具有相同数量的token和相同的token维度。为了减少搜索算法所需的搜索空间和计算量，只使用了token pooling的3个阶段；</li><li>对于每个阶段的层数，只考虑有限的层数，进一步减少搜索空间和计算量；</li><li>为不同的层次提供了应用注意力共享或不共享的灵活性。</li></ul><h4 id="网络设计搜索空间">网络设计搜索空间</h4><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210810/7.png" alt=""></p><p>如果将每个阶段的层次数量和注意力图共享设置作为网络的超参数，那么学习它们的自然选择就是基于RL或EA的方法，这在搜索过程中非常缓慢。</p><p>因此，作者采用基于权值共享的方法作为本文的搜索算法，这需要定义一个supernet来囊括搜索空间中的所有候选架构。一种可能的supernet设计如图所示。supernet有3个阶段。在阶段之间放置一个token pooling层，以更改token number和特性维度。</p><p>每个阶段有6个单元，其中一个单元有3个路径选择，1个基本的transformer层，2个共享层(第1个transformer层的注意力映射被复制到第2层)和identity映射。</p><p>候选体系结构可以为每个单元独立地从3个选择中选择一条路径。通过包含identity映射，候选网络可以有0到36(所有单元选择共享层)transformer层。这种supernet设计为transformer架构提供了更大的可行性。</p><p>例如，候选对象只能为所有层选择基本层，这相当于2个token pooling层增强的transformer。作为另一个例子，一个候选架构可以在第一阶段为所有的前6个单元选择共享层路径，并为其余单元选择token，它有12个transformer层，每2层共享注意力图，但没有token pooling。</p><h3 id="AutoML-for-the-Searching">AutoML for the Searching</h3><h4 id="Training-the-supernet">Training the supernet</h4><p>前面构建的supernet包含了所有候选网络。对于supernet的每个单元都有多种选择。通过激活每个细胞中的一个选择，就可以构建一个候选网络。在训练过程中，SPOS对每次训练迭代进行统一采样，只选择一个候选网络，并在supernet中更新所选候选网络的参数。候选网络将继承supernet的训练参数，并不断更新这些参数。一旦supernet被训练，所有的候选网络都可以继承它的权值，而不需要从头开始进行搜索训练。</p><h4 id="Searching-from-the-trained-supernet">Searching from the trained supernet</h4><p>对supernet进行训练后，将进化方法进一步应用于supernet中的所有候选网络，得到最佳的候选网络结构。</p><h2 id="实验">实验</h2><h3 id="Classification实验">Classification实验</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210810/8.png" alt=""></p><h3 id="Object-Detection-Instance-Segmentation">Object Detection &amp; Instance Segmentation</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210810/9.png" alt=""></p><h3 id="可视化">可视化</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210810/10.png" alt=""></p><h2 id="参考">参考</h2><p>[1].PSViT: Better Vision Transformer via Token Pooling and Attention Sharing<br></p>]]></content>
      
      
      <categories>
          
          <category> PSViT </category>
          
          <category> Transformer </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Token池化 </tag>
            
            <tag> Attention Sharing </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>YOffleNet | YOLO V4 基于嵌入式设备的轻量化改进设计</title>
      <link href="/2021/08/05/33/"/>
      <url>/2021/08/05/33/</url>
      
        <content type="html"><![CDATA[<p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210805/1.png" alt=""></p><h2 id="简介">简介</h2><p>最新的基于CNN的目标检测模型相当精确，但需要高性能GPU实时运行。对于内存空间有限的嵌入式系统来说，它们在内存大小和速度方面依旧不是很好。</p><p>由于目标检测是在嵌入式处理器上进行的，因此在保证检测精度的同时，最好尽可能地压缩检测网络。有几个流行的轻量级检测模型，但它们的准确性太低。因此，本文提出了一种新的目标检测模型 <strong>YOffleNet</strong>，该模型在压缩率高的同时，将精度损失降到最小，可用于自动驾驶系统上的实时安全驾驶应用。该模型的Backbone架构是基于YOLOv4实现，但是可以用ShuffleNet的轻量级模块代替CSP的高计算负荷的DenseNet，从而大大压缩网络。</p><p>在KITTI数据集上的实验表明，提出的YOffleNet比YOLOv4-s压缩了4.7倍，在嵌入式GPU系统(NVIDIA Jetson AGX Xavier)上可以达到46FPS的速度。与高压缩比相比，精度略有降低，为85.8% mAP，仅比YOLOv4-s低2.6%。因此，提出的网络具有很高的潜力部署在嵌入式系统。</p><h2 id="YOLO-V4简述">YOLO V4简述</h2><p>在YOLOv4的主干网络CSPDarknet-53中，CSP将特征卷积一定次数后复制使用与前一层特征cat起来，然后利用DenseNet模块。</p><p>在Neck中，输入特征图有3种大小。SPP最大池化后concat技术提高了各种尺寸输入的准确性。此外，它通过自底向上的路径增强技术平滑特征。</p><p>YOLOv4引入PANet以促进信息流和它弥补了权重带来的精度损失问题。</p><p>YOLO v4的Head依旧采用YOLOv3的物体检测方法。</p><h2 id="YOLO-V4轻量化设计">YOLO V4轻量化设计</h2><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210805/2.png" alt=""></p><p>YOLOv4中使用的主要模块是下图中的CSP DenseNet；此外为了防止初始特征图中的信息丢失的问题，作者还设计了PANet结构，其是通过自下而上的路径增强特征表达的。它促进信息的流动的同时也增加了特征图中的通道数、增加参数的数量，这也是YOffleNet模型为它改进了上述YOLOv4模型的缺点。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210805/3.png" alt="CSP DenseNet"></p><h4 id="改进点-1">改进点 1</h4><p>主干层CSP DenseNet是一种随着深度增加而不可避免地增加计算量的结构。在本研究中，主干网络层被配置为ShuffleNet模块。</p><p>![ShuffleNet模块]<img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210805/4.png" alt=""></p><h4 id="改进点-2">改进点 2</h4><p>YOLOv4网络中使用的SPP+PANet结构简化和减轻模型的大小。现有YOLOv4模型的PANet从主干网络分为3层作为输入的。然而，常见对象检测情况与自动驾驶环境不同，有限类别中的物体检测（汽车、行人等，更小的目标也就少了）。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210805/5.png" alt=""></p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210805/6.png" alt=""></p><p>基于这个原因，改进PANet可以接收来自backbone网络的只有2层的输入。Upsample, Downsample层的位置和数量变少了。计算量相对也就减少了。</p><h2 id="实验">实验</h2><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210805/7.png" alt=""></p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210805/8.png" alt=""></p><p>没啥好评价的，确实变快了，但是这个改进确实有点。。。。你懂的！！！</p><h2 id="参考">参考</h2><p>[1].Developing a Compressed Object Detection Model based on YOLOv4 for Deployment on Embedded GPU Platform of Autonomous System<br></p>]]></content>
      
      
      <categories>
          
          <category> YOffleNet </category>
          
      </categories>
      
      
        <tags>
            
            <tag> YOLO V4 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>详细解读GraphFPN | 如何用图模型提升目标检测模型性能？</title>
      <link href="/2021/08/03/32/"/>
      <url>/2021/08/03/32/</url>
      
        <content type="html"><![CDATA[<p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210803/1.png" alt=""></p><blockquote><p>本文提出了图特征金字塔网络：GraphFPN，其能够使其拓扑结构适应不同的内在图像结构，并支持跨所有尺度的同步特征交互，与Faster R-CNN+FPN搭配！性能优于Sparse R-CNN等网络，单位：复旦大学, 香港大学等</p></blockquote><h2 id="简介">简介</h2><p>特征金字塔在需要多尺度特征的图像理解任务中已被证明是强大的。多尺度特征学习的最新方法侧重于使用具有固定拓扑结构的神经网络跨空间和尺度执行特征交互。</p><p>在本文中提出了<strong>图特征金字塔网络</strong>，该网络能够使其拓扑结构适应不同的内在图像结构，并支持跨所有尺度的同步特征交互。首先为每个输入图像定义一个特定于图像的超像素层次结构来表示其固有的图像结构。图特征金字塔网络从这个超像素层次结构继承了它的结构。上下文层和层次层旨在实现相同尺度内和不同尺度之间的特征交互。为了使这些层更鲁棒，作者通过概括卷积神经网络的全局通道注意力，为图神经网络引入了2种类型的局部通道注意力。提出的图特征金字塔网络可以增强卷积特征金字塔网络的多尺度特征。</p><p>作者通过将其集成到Faster R-CNN算法中来评估在目标检测任务中的图特征金字塔网络。修改后的算法不仅在MS-COCO 2017验证和测试数据集上以明显的优势优于先前最先进的基于特征金字塔的方法，而且还优于其他流行的检测方法。</p><h4 id="本文主要贡献">本文主要贡献</h4><ul><li><p>提出了一种新的图特征金字塔网络，利用固有的图像结构，支持所有尺度的同时特征交互。该图特征金字塔网络继承了输入图像的超像素层次结构。上下文层和层次层的设计分别是为了促进相同规模内和跨不同规模的特性交互；</p></li><li><p>在现有的卷积神经网络全局通道注意机制的基础上，进一步引入了图神经网络的2种局部通道注意机制；</p></li><li><p>在MS-COCO 2017验证和测试数据集上的大量实验表明，无论是否基于特征金字塔，图特征金字塔网络都可以帮助实现比现有的最先进的目标检测方法明显更好的性能。消融研究进一步验证了所提网络组件的有效性。</p></li></ul><h2 id="图特征金字塔网络">图特征金字塔网络</h2><p>图特征金字塔网络旨在通过在超像素层次上构建多尺度图神经网络来增强卷积特征金字塔网络。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210803/2.png" alt=""></p><h3 id="Superpixel-Hierarchy">Superpixel Hierarchy</h3><p>在分层分割中，像素(或更小的超像素)通过相似性度量递归地分组为更大的像素。给定一幅图像I，依靠面向卷积的边界(COB)来获得分层分割，这是一组图像分区${S^0,S^1,…,S^L}$。注意,每个超像素$S^0$的原始输入图像中的一个像素,$S^L$只有一个超像素代表整个图像的超像素$S^L$和$S^{L−1}$只相差一个。</p><p>在本文中，从$S^0$中选择了一个分区子集${S^0,S^1,…,S^L}$定义超像素层次${S^{l_1},S^{l_2},S^{l_3},S^{l_4},S^{l_5}}$，其中S的上标表示分割层次中的划分级别，$S^{l_1}$是该层次中最精细的超像素集合，$S^{l_{i+1}}$中的超像素是$S^{l_1}$中超像素的并集。匹配卷积神经网络的下采样率，选取${l_1,l_2,l_3,l_4,l_5}$，使 $S^{l_{i+1}}$ 的超像素数为 $S^{l_i}$ 超像素数的1/4。然后用超像素层次S表示输入图像的部分-整个层次，并跟踪超像素之间的ancestor-descendant关系。</p><h3 id="Multi-scale-Graph-Pyramid">Multi-scale Graph Pyramid</h3><p>本文构建了一个图金字塔$g^1,g^2,g^3,g^4,g^5$，其级别对应于超像素层次的级别。超像素层次中的每个超像素在图金字塔的相应层次上都有一个对应的图节点。因此，当从图金字塔的一层移动到下一层时，节点的数量也会减少4倍。</p><p>作者为图金字塔定义了2种类型的边。它们被称为<strong>上下文边缘</strong>和<strong>层次边缘</strong>。</p><p>上下文边缘连接同一层次上的2个相邻节点，而层次边缘连接不同层次上的2个节点，如果它们对应的超像素之间存在ancestor-descendant关系。<strong>上下文边缘用于传播层次边缘用于弥合不同层次之间的语义差距</strong>，而<strong>同一层次内的上下文信息</strong>。</p><p>请注意，层次边缘是密集的，因为在每个节点和它的每个ancestor和descendant之间都有这样的边缘。这些密集的连接会产生很大的计算和内存成本。因此，每个层次边缘都与其节点特征之间的余弦相似度关联，作者根据它们的余弦特征相似度对层次边缘进行修剪。在所有关联到节点的分层边缘中，排在最后50%的边缘将被删除。</p><h3 id="Graph-Neural-Network-Layers">Graph Neural Network Layers</h3><p>在图金字塔的基础上构造了一个<strong>图神经网络GraphFPN</strong>。在GraphFPN中有2种类型的层，上下文层和层次层。这2种类型的层在图金字塔中使用相同的节点集，但不同的图边集。上下文层只使用上下文边缘，而层次层只使用修剪过的层次边缘。GraphFPN在最开始有L1上下文层，在中间有L2层次层，在最后有L3上下文层。更重要的是，每一层都有自己的可学习参数，这些参数不会与任何其他层共享。</p><p>虽然上下文层和层次层使用不同的边缘，但这2种类型的层中的GNN操作是完全相同的。这2种类型的层次共享相同的空间和通道注意机制。简单地采用图注意网络中的self-attention作为空间注意。给定节点i及其邻居集合$N_i$，节点i的空间注意力更新特征如下:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210803/3.png" alt=""></p><p>其中M为single-head self-attention，$\vec h_{j\in N_i}$为节点i的邻居收集的特征向量集，$\vec h_i$和$\vec {h’_i}$分别为节点i更新前后的特征向量。</p><p>通道注意机制由基于平均池化的局部通道注意力模块和局部信道自注意力模块组成。在基于平均池化的局部通道注意力算法中，首先对节点i及其邻居的特征向量进行平均，得到特征向量$\vec a’_i \in R^C$。然后将平均的特征向量通过一个sigmoid激活的完全连接层，并在结果和$\vec {h’_i}$之间执行元素乘法，</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210803/4.png" alt=""></p><p>其中，$\sigma$为sigmoid函数，$W_1 \in R^{C×C}$为全连通层可学习权重矩阵，为元素乘法。在局部通道自注意力模块中，首先获取节点i及其邻居的特征向量集合A，并进行reshape 为$R^{(|N_i|+1)×C}$。其中$|N_i|$为节点i的邻域大小。接下来得到通道相似矩阵$X=A^TA\in R^{C×C}$，并对X的每一行应用softmax函数，局部通道自注意力模块的输出为：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210803/5.png" alt=""></p><p>其中$\beta$是初始化为0的可学习权值.</p><p>local channel-wise attention和local channel self attention的灵感来自于SENet和Dual attention Network。主要的区别在于本文的通道注意力是在local neighborhoods内定义的，因此从节点到节点在空间上是不同的，而SENet和Dual attention Network则在所有空间位置上使用相同的通道注意力。局部通道注意力在图神经网络中的优势包括更低的计算成本和更高的空间自适应，因此非常适合于GraphFPN这样的大型网络。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210803/6.png" alt=""></p><p>表5中的消融研究表明，双局部通道注意力在GraphFPN中相当有效。</p><h3 id="Feature-Mapping-between-GNN-and-CNN">Feature Mapping between GNN and CNN</h3><p>卷积神经网络可以保留局部和目标的位置信息，对目标检测有明显的好处，而图神经网络可以跨多个语义尺度灵活地建模局部和目标之间的依赖关系。</p><p>需要注意的是，卷积神经网络的主干和FPN分别负责多尺度编码和解码，而GraphFPN主要负责多尺度解码。因此，来自主干的特征作为GraphFPN的输入。为了利用这2种类型的特征金字塔网络，作者还融合了GraphFPN和卷积FPN的最终特征。因此，需要从主干映射特征来初始化GraphFPN，也需要在特征融合之前将最终的特征从GraphFPN映射到卷积FPN。主干和卷积FPN中的多尺度特征映射分别表示为$C={C^1,C^2,C^3,C^4,C^5}$ 和$P={P^1,P^2,P^3,P^4,P^5}$ 。注意，C中的特征映射是主干中5个卷积阶段的最终特征映射。</p><h4 id="Mapping-from-CNN-to-GNN">Mapping from CNN to GNN</h4><p>作者将主干$C_i$的第i个特征映射到s中的第i个$S_i$。$C_i$中的特征位于一个矩形网格上，每个网格单元对应原始输入图像中的一个矩形区域，而$S_i$中的超像素通常具有不规则的形状。如果$C_i$中多个超像素与同一网格单元部分重叠，如图1©所示，将网格单元分配给重叠最大的超像素。这样的分配会导致一个小集合$C_i$的网格单元分配给$S_i$中相同的超像素$R^k_i$。在集合上执行最大池化和最小池化，并通过ReLU激活将连接池化结果提供给一个完全连接的层。$R^k_i$的映射特性可以写成：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210803/7.png" alt=""></p><p>其中，$\delta$表示ReLU激活，$W_2$为全连接层的可学习权值矩阵，k为级联算子，$∆max(C^k_i)$和$∆min(C^k_i)$分别为max-pooling和min-pooling算子。</p><h4 id="Mapping-from-GNN-to-CNN">Mapping from GNN to CNN</h4><p>向前通过GraphFPN，就将其最后一层的特征映射到卷积特征金字塔P；设$P_k^i$为$P_i$中分配给$S_i$中超像素$R_k^i$的网格单元的集合。简单地将$R_k^i$的最终特性复制到$P_k^i$中的每个网格单元。这样就得到了卷积第i层的一个新的特征映射$\overline P^i$。然后将$\overline P^i$与$P^i$连接，并将连接的特征映射提供给1×1卷积层，以确保融合的特征映射$\tilde P^i$与$P^i$具有相同数量的通道。最后，提出融合后的特征金字塔:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210803/8.png" alt=""></p><h3 id="目标检测">目标检测</h3><p>本文提出的图特征金字塔网络可以用融合后的特征金字塔代替传统的FPN，集成到的目标检测中。采用Faster RCNN作为检测算法，并进行相同的端到端训练。</p><h2 id="实验">实验</h2><h3 id="SOTA对比">SOTA对比</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210803/9.png" alt=""></p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210803/10.png" alt=""></p><h3 id="可视化对比">可视化对比</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210803/11.png" alt=""></p><h2 id="参考">参考</h2><p>[1].GraphFPN: Graph Feature Pyramid Network for Object Detection<br></p>]]></content>
      
      
      <categories>
          
          <category> GraphFPN </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GraphFPN </tag>
            
            <tag> 图模型 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>实验分析非常精彩 | Transformer中的位置嵌入到底改如何看待？</title>
      <link href="/2021/07/30/31/"/>
      <url>/2021/07/30/31/</url>
      
        <content type="html"><![CDATA[<p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210730/1.png" alt=""></p><h2 id="简介">简介</h2><p>相对位置编码(Relative position encoding, RPE)是Transformer获取输入Token序列顺序的重要方法。在自然语言处理中已证实了其有效性。</p><p>然而，在计算机视觉中，相对位置编码的有效性还没有得到很好的研究，甚至还存在争议，<strong>如相对位置编码是否能与绝对位置编码同样有效</strong>?</p><p>为了澄清这一点，作者首先回顾了现有的相对位置编码方法，并分析了它们在Vision Transformer中应用的优缺点。然后提出了一种新的二维图像相对位置编码方法，称为<strong>图像RPE(iRPE)</strong>。</p><p>本文的方法考虑了定向相对距离建模以及query和相对位置嵌入在自注意机制中的交互作用。提出的iRPE方法简单、轻量级。它们可以很容易地插入Transformer。</p><p>实验表明，仅由于提出的编码方法，DeiT和DETR在ImageNet和COCO上分别获得了高达1.5% (top-1 Acc)和1.3%(mAP)的提升，而不需要调整任何额外的超参数，如学习率和权重衰减。作者也进行了消融实验和分析得到了有趣的发现，其中一些与之前的理解恰恰相反。</p><h4 id="本文主要贡献">本文主要贡献:</h4><ol><li><p>分析了相对位置编码中的几个关键因素，包括相对方向、上下文的重要性、query、key、value和相对位置嵌入之间的交互以及计算成本。对相对位置编码进行了全面的理解，为新方法的设计提供了经验指导;</p></li><li><p>引入了一种有效的相对编码实现，它将计算成本从原始的$O(n^2d)$降低到$O(nkd)$，其中k&lt;&lt;n。该实现适用于高分辨率输入图像，如目标检测和语义分割，其中token数可能非常大;</p></li><li><p>提出了4种新的相对位置编码方法，称为图像RPE(iRPE)，专门用于Vision Transformer，考虑了效率和泛化。这些方法很简单，可以很容易地插入到Self-attention中。实验表明,在不调整任何超参数和设置的情况下，该方法在ImageNet和COCO上的DeiTS和detr-resnet50分别比原模型提高了1.5%(top-1 Acc)和1.3%(mAP);</p></li><li><p>回答了先前有争议的问题。实验证明相对位置编码可以代替绝对位置编码进行图像分类。同时，目标检测需要绝对编码，像素位置对目标定位也是至关重要。</p></li></ol><h2 id="相关工作总结">相关工作总结</h2><p>Transformer由于其具有较强的性能和捕获远程依赖关系的优越能力，近年来在计算机视觉领域引起了广泛的关注。Transformer的核心是Self-attention，它能够对序列中的Token关系进行建模。然而，<strong>Self-attention有一个固有的缺陷就是它不能捕获输入Token的顺序</strong>。</p><p>因此，合并位置信息的显式表示对于Transformer特别重要，因为模型对序列顺序是完全不变的，这对于结构化数据建模是不可取的。</p><p>主要有2类方法对Transformer的位置表示进行编码。一个是绝对位置编码，另一个是相对位置编码。</p><p>绝对位置方法将输入Token的绝对位置从1编码到最大序列长度。也就是说，每个位置都有一个单独的编码向量。然后将编码向量与输入Token组合，以向模型公开位置信息。</p><p>相对位置方法对输入元素之间的相对距离进行编码，学习符号之间的成对关系。相对位置编码(RPE)通常是通过带有可学习参数的query表与Self-attention模块中的query和key进行交互来计算的。这种模式允许模块捕获Token之间非常长的依赖关系。相对位置编码在自然语言处理中的有效性得到了验证。</p><p>然而，在计算机视觉中，这种效果还不清楚。最近很少有作品对它进行阐释，但在Vision Transformer中却得到了有争议的结论。有研究者观察到相对位置编码与绝对位置编码相比没有带来任何增益。相反，也有学者发现相对位置编码可以诱导表观增益，优于绝对位置编码。</p><p>此外，最近的研究表明相对位置编码不能像绝对位置编码一样好。这些工作对模型中相对位置编码的有效性得出了不同的结论，这促使本文作者重新思考和改进相对位置编码在Vision Transformer中的应用。</p><p>另一方面，提出了原始相对位置编码的语言建模方法，其中输入数据为1D单词序列。但对于视觉任务，输入通常是2D图像或视频序列，其中像素是高度空间结构的。尚不清楚的是:从1D到2D的朴素扩展是否适用于视觉模型;方向性信息在视觉任务中是否重要?</p><h2 id="Self-Attention与位置嵌入">Self-Attention与位置嵌入</h2><h3 id="Self-Attention">Self-Attention</h3><p>Self-Attention在Transformer中起着基础性的作用。如下图，它将query和一组key和value对映射到输出。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210730/2.png" alt=""></p><p>更具体地说，对于输入序列，如单词或图像块的嵌入，$x=(x_1,…,x_n)$，其中$x_i\in R^{d_x}$，Self-Attention计算一个输出序列$z=(z_1,…,z_n)$,其中$z\in R^{d_z}$。每个输出元素$z_i$是作为输入元素的加权和计算的：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210730/3.png" alt=""></p><p>每个权重系数$\alpha _{ij}$使用softmax计算:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210730/4.png" alt=""></p><p>其中$e_{ij}$是使用一个scaled dot-product attention来计算的:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210730/5.png" alt=""></p><p>这里，投影$W^Q, W^K, W^V \in R^{d_x×d_z}$是参数矩阵，每层都是唯一的。</p><p>MultiHead Self-Attention(MHSA)不是计算一次Self-Attention，而是并行运行多次Self-Attention，即使用H个Attention Head。Attention Head的输出被简单地连接起来并线性地转换成设计的维度。</p><h3 id="位置嵌入">位置嵌入</h3><h4 id="绝对位置编码">绝对位置编码</h4><p>由于Transformer不包含递归和卷积，为了使模型能够利用序列的顺序，需要注入一些关于Token位置的信息。原始Self-Attention考虑绝对位置，并添加绝对位置编码$p=(p_1,…,p_n)$的输入Token嵌入x为：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210730/6.png" alt=""></p><p>这里的位置编码是$p_i,x_i\in R^d_x$。绝对位置编码有几种选择，如采用不同频率的正弦和余弦函数的固定编码和通过训练参数的可学习编码。</p><h4 id="相对位置编码">相对位置编码</h4><p>除了每个输入元素的绝对位置，最近的一些工作还考虑了元素之间的成对关系，即相对位置。对于元素的相对顺序或距离很重要的任务，相对关系可能很重要。这种类型的方法将输入元素$x_i$和$x_j$之间的相对位置编码为向量$p^V_{ij},p^Q_{ij},p^K_{ij}\in R^{d_z}$，其中$d_z=d_x$。将编码向量嵌入到Self-Attention模块中，将式(1)和式(3)重新表示为:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210730/7.png" alt=""></p><p>通过这种方法，可以在Transformer训练过程中学习成对的位置关系。这种相对位置编码可以在Attention Head之间共享，也可以不共享。</p><h2 id="本文方法">本文方法</h2><p>这里，首先回顾以往的相对位置编码方法，并分析它们之间的差异。在此基础上，提出了4种用于Vision Transformer的新方法，并对其进行了有效的实现。</p><h3 id="先前的相对位置编码方法">先前的相对位置编码方法</h3><h4 id="Shaw’s-RPE">Shaw’s RPE</h4><p>Shaw等人提出了Self-Attention的相对位置编码。输入Token被建模为一个有向和全连接图。任意位置i和j之间的每条边由一个可学习的向量$p_{ij}\in R^{d_z}$表示，即相对位置编码。此外，作者认为精确的相对位置信息在超过一定距离后就没有用处了，因此引入了clip函数来减少参数的数量。<br>编码形式为:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210730/8.png" alt=""></p><p>其中$p^V$和$p^K$分别为value和key的相对位置编码的可训练权值。$p^V=(p^V_{−k},…,p^K_k)$ $p^K=(p^K_{−k},…,p^K_k)$，其中$p^V_i,p^K_i\in R^{d_z}$。标量k是最大相对距离。</p><h4 id="RPE-in-Transformer-XL">RPE in Transformer-XL</h4><p>Dai等人为query引入了额外的偏置项，并使用正弦公式进行相对位置编码，其表达式为:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210730/9.png" alt=""></p><p>其中$u,v\in R^{d_z}$是2个可学习的向量。</p><p>正弦编码矢量$s$提供了相对位置的先验值。$W^R\in R^{d_z×d_z}$是一个可训练矩阵，它可以将$s_{i−j}$投射到一个基于位置的关键向量中。</p><h4 id="Huang’s-RPE">Huang’s RPE</h4><p>Huang等人提出了一种同时考虑query、key和相对位置交互的新方法。方程如下所示：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210730/10.png" alt=""></p><p>其中$p_{ij}\in R^{d_z}$是query和key共享的相对位置编码。</p><h4 id="RPE-in-SASA">RPE in SASA</h4><p>以上3种方法都是针对语言建模中的一维词序列而设计的。Ramachandran等人提出了一种二维图像编码方法。这个想法很简单。它将二维相对编码分为水平方向和垂直方向，使得每个方向都可以通过一维编码进行建模。方法公式如下所示:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210730/11.png" alt=""></p><p>其中，$\delta \tilde x = \tilde x_i - \tilde x_j$和$\delta \tilde y = \tilde y_i - \tilde y_j$表示x轴和y轴上的相对位置，图像的坐标分别$p^K\delta \tilde x$和$p^K\delta \tilde y$是可学习的向量, concat操作连接2个编码形式最终相对编码和$d_z$的长度。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210730/12.png" alt=""></p><p>换句话说，x轴或y轴上相同的偏移量共享相同的相对位置编码，因此这种方法可以减少可学习参数的数量和计算成本。但是，编码只应用于key。在实验中观察到对key、query和value同时施加RPE是最有效的，如表4和表5所示。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210730/13.png" alt=""></p><h4 id="RPE-in-Axial-Deeplab">RPE in Axial-Deeplab</h4><p>Wang等人引入了一种位置敏感方法，将qkv依赖的位置偏差加入到Self-Attention中。位置灵敏度应用于沿高度轴和宽度轴依次传播信息的轴向注意。然而，当相对距离大于一个阈值时，编码设置为零。作者观察到远程相对位置信息是有用的，如表6所示。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210730/14.png" alt=""></p><p>在标准的Self-Attention基础上，这种位置敏感性可能具有竞争性。如果采用所提出的分段函数，它可以进一步改进，更有效地建模长期依赖关系。</p><h3 id="提出新的相对位置编码">提出新的相对位置编码</h3><p>作者设计了自己的图像RPE(iRPE)方法来分析几个在以前的工作中没有很好研究的因素。首先，为了研究编码是否可以独立于输入嵌入，作者引入了2种相对位置模式:<strong>偏差模式</strong>和<strong>上下文模式</strong>。</p><p>作者提出了一个分段函数来映射相对位置到编码，这与传统的clip函数不同。然后，为了研究方向性的重要性，作者设计了2种无向和2有向方法。</p><h4 id="偏差模式和上下文模式">偏差模式和上下文模式</h4><p>以前的相对位置编码方法都依赖于输入Embedding。它带来了一个问题，即编码是否可以独立于输入?</p><p>作者引入相对位置编码的偏差模式和上下文模式来研究这一问题。前者独立于输入Embedding，而后者考虑与query、key或value的交互。更具体地说，作者引入一个统一的公式为：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210730/15.png" alt=""></p><p>其中$b_{ij}\in R$为二维相对位置编码，定义偏差或上下文模式。</p><p>对于偏差模式:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210730/16.png" alt="偏差模式"></p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210730/17.png" alt=""></p><p>其中$r_{ij}\in R$是一个可学习的标量，表示位置$i$和$j$之间的相对位置权值。</p><p>对于上下文模式:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210730/18.png" alt="上下文模式"></p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210730/19.png" alt=""></p><p>其中$r_{ij}\in R^{d_z}$是一个可训练的向量，与query嵌入交互。在上下文模式下，$b_{ij}$有多种变体。例如，在query和key上操作的相对位置编码可以表示为:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210730/20.png" alt=""></p><p>其中$r^K_{ij},r^Q_{ij}\in R^{d_z}$都是可学习的向量。</p><p>此外，上下文模式也可以应用于value Embedding，</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210730/21.png" alt=""></p><p>其中$r^V_{ij}\in R^{d_z}$。相对位置权值$r^Q_{ij},r^K_{ij},r^V_{ij}$可以用同样的方法构造。对于一个统一的表示，在下面的讨论中使用$r_{ij}$来表示偏差模式和上下文模式。</p><h4 id="分段索引函数">分段索引函数</h4><p>在描述二维相对位置权值$r_{ij}$之前，首先引入多对一函数，将有限集中的相对距离映射为一个整数，然后$r_{ij}$可以用这个整数作为索引，并在不同的关系位置之间共享约束。这种索引函数可以大大减少长序列(如高分辨率图像)的计算成本和参数数量。</p><p>虽然有学者使用的clip函数$h(x)=max(\beta,min(\beta,x))$也降低了成本，但相对距离大于β的位置被赋给相同的编码。这种方法不可避免地忽略了长期相对位置的上下文信息。预适作者引入了一个分段函数$g(x):R{y\in Z|- \beta &lt;= y &lt;= \beta}$来索引到相应编码的相对距离。这个函数是基于一个假设，即更近的邻居比更远的邻居更重要，并根据相对距离分配注意力。它被描述为：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210730/22.png" alt=""></p><p>其中[·]是一个舍入运算，sign()确定数字的符号，即输入为正返回1，输入为负返回-1，输入为负返回0。α决定分段点，β控制输出在[−β,β]范围内， γ调整对数部分的曲率。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210730/23.png" alt=""></p><p>作者比较分段函数g(x)与clip函数$h(x)= min(−\beta,max(\beta,x))$，如图2，clip函数h(x)分布均匀的注意力，忽略了长距离的位置，但分段函数g(x)通过相对距离分布不同的注意力水平。假设需要保留远程位置的潜在信息，特别是对于高分辨率图像或需要远程特征依赖的任务，因此选择g(x)来构建$r_{ij}$映射方法。</p><h4 id="二维相对位置计算">二维相对位置计算</h4><p>为了计算二维图像平面上的相对位置和定义相对权重$r_{ij}$，作者提出了2种无向映射方法，即<strong>欧式映射</strong>和<strong>量化映射</strong>，以及2种有向映射方法，即<strong>交叉映射</strong>和<strong>乘积映射</strong>。</p><h5 id="欧式映射">欧式映射</h5><p>在图像平面上，相对位置$(\tilde x_i−\tilde x_j;\tilde y_i−\tilde y_j)$为二维坐标。计算2个位置之间的欧氏距离，并将距离映射到相应的编码中。该方法是无向的，数学表达为:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210730/24.png" alt=""></p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210730/25.png" alt=""></p><p>其中$p_I(i,j)$为偏置模式下的可学习标量或上下文模式下的向量。这里将$p_I(i,j)$看作一个存储相对位置权值的桶。桶的数量为$2\beta+1$。</p><h5 id="量化映射">量化映射</h5><p>在上述欧式映射方法中，相对距离不同的2个近邻可以映射到同一个索引，例如二维相对位置(1,0)和(1,1)都映射到索引1。则认为近邻应该分开。因此，对欧氏距离进行量化，即不同的实数映射为不同的整数。I(I,j)修正为：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210730/26.png" alt=""></p><p>操作$quant(·)$将一组实数${0, 1，1.41, 2, 2.24,…}$量化为整数集合${0, 1, 2, 3, 4,…}$,这个方法也是无向的。</p><h5 id="交叉映射">交叉映射</h5><p>像素的位置方向对图像也很重要，因此提出了有向映射方法。这种方法称为<strong>交叉法</strong>，它分别计算水平和垂直方向上的编码，然后对它们进行总结。方法如下:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210730/27.png" alt=""></p><p>其中，$p^{\tilde x}<em>{I~(i,j)}$和$p^{\tilde y}</em>{I~(i,j)}$都是偏置模式下的可学习标量，或上下文模式下的可学习向量。</p><p>类似于SASA中的编码，x轴或y轴上相同的偏移量共享相同的编码，但主要的区别是这里使用了一个分段函数来根据相对距离分配注意力。桶的数量是$2\beta+1$。</p><h5 id="乘积映射">乘积映射</h5><p>交叉映射将不同的相对位置编码为相同的嵌入，如果在一个方向上的距离是相同的，无论是水平的还是垂直的。</p><p>此外，加法运算带来了额外的计算成本。为了提高效率和包含更多的方向性信息，作者设计了<strong>乘积映射</strong>，公式如下：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210730/28.png" alt=""></p><p>方程的右侧为偏置模式下的可训练标量，或上下文模式下的可训练向量。</p><h4 id="二维相对位置映射实验">二维相对位置映射实验</h4><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210730/29.png" alt=""></p><h5 id="分析">分析</h5><ul><li><p><strong>直接 vs 间接</strong>：如表所示，有向方法(交叉和乘积)总体上优于无向方法(欧几里得和量化)。这一现象<strong>说明方向性对于vision transformer是重要的，因为图像像素是高度结构化的和语义相关的</strong>；</p></li><li><p><strong>偏置 vs 上下文</strong>：从表可以看出，无论采用哪种方法，上下文模式的表现都优于偏置模式。<strong>潜在的原因可能是上下文模式用输入特征改变编码，而偏置模式保持不变</strong>；</p></li></ul><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210730/30.png" alt=""></p><ul><li><strong>共享 vs 不共享</strong>：Self-attention包含多个Head。相对位置编码可以在不同的head共享或不共享。分别在表2中展示了这2种方案在偏置和上下文模式下的效果。对于偏置模式，当在head共享编码时，准确率显著下降。相比之下，在上下文模式下，2个方案之间的性能差距可以忽略不计。两者的平均准确率均为80.9%。作者<strong>推测不同的head需要不同的相对位置编码来获取不同的信息。在上下文模式下，每个head可以计算出自己的RPE，而在偏置模式下，共享的RPE则迫使所有head对patch给予同样的关注。为了节省参数，在最终的方法中采用了共享方案</strong>；</li></ul><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210730/31.png" alt=""></p><ul><li><p><strong>分段 vs Clip</strong>：作者比较了分段函数g(x)与clip函数h(x)的效果。在图像分类任务中，这2种功能之间的性能差距很小，甚至可以忽略不计。然而，在目标检测任务中，clip函数比表6所示的分段函数差。<strong>潜在的原因</strong>是<strong>当序列长度较短时，这两个函数非常相似。分段函数是有效的，特别是当序列的大小远远大于桶的数量时。与分类相比，目标检测使用更高的分辨率输入，导致输入序列更长</strong>。因此，作者推测，当输入序列较长时，由于分段函数能够将不同的Attention分散到距离相对较大的位置，所以应该使用分段函数，而当相对距离大于β时，Clip函数可以分配相同的编码；</p></li><li><p><strong>桶的数量</strong>：桶的数量在很大程度上影响模型参数、计算复杂性和性能。为了找到一个平衡，作者探索了不同桶的数量对上下文方法的影响。图3显示了top-1精度随桶数的变化情况。在50桶之前，精度从79.9提高到80.9。在那之后，没有显著的改善。结果表明，对于DeiT-S中14×14的特征图，桶数50可以很好地平衡计算代价和精度。</p></li></ul><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210730/32.png" alt=""></p><ul><li><strong>特定组件分析</strong>：这里主要是研究不同的位置编码对vision transformer模型的影响。选择DeiT-S模型作为基线，只改变了位置编码方法。原始模型采用了可学习的绝对位置编码。用上下文产品法计算了50个桶的相对位置编码。</li></ul><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210730/33.png" alt=""></p><p>从表中得到如下结论:</p><ol><li>从原始DeiT-S中去除绝对位置编码会导致Top-1的精度从79.9下降到77.6(#1 vs . #2)；</li><li>仅采用相对位置编码的模型优于仅采用绝对位置编码的模型(#3-5 vs . #1)。结果表明，相对位置编码比绝对位置编码效果好；</li><li>当配置相对位置编码时，绝对编码不带来任何增益(#3-5 vs . #8-10)。在分类任务中，作者认为局部信息比全局信息更重要；</li><li>query或key的相对位置编码比value带来更多的收益(#3,4 vs . #5)；</li><li>对query、key和value的编码组合可以带来了进一步的提升(#6、7、11、12 vs . others)。</li></ol><ul><li><strong>复杂度分析</strong>：<br>作者评估了所提出的方法在不同输入分辨率下的计算成本。baseline模型为DeiT-S，仅采用绝对位置编码。采用We adopt contextual product<br>shared-head relative position encoding to the baseline with 50 buckets。图4显示了我们的方法在有效实现的情况下最多需要1%的额外计算成本。</li></ul><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210730/34.png" alt=""></p><h2 id="实验">实验</h2><h3 id="ImageNet">ImageNet</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210730/35.png" alt=""></p><h3 id="COCO">COCO</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210730/36.png" alt=""></p><h2 id="参考">参考</h2><p>[1].Rethinking and Improving Relative Position Encoding for Vision Transformer<br></p>]]></content>
      
      
      <categories>
          
          <category> Transformer </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 位置嵌入 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>详细解读 Transformer的即插即用模块 | MoE插件让ViT模型更宽、更快、精度更高</title>
      <link href="/2021/07/28/30/"/>
      <url>/2021/07/28/30/</url>
      
        <content type="html"><![CDATA[<p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210728/1.png" alt=""></p><blockquote><p>本文提出Transformer更宽而不是更深，以实现更高效的参数部署，并将此框架实现为<strong>WideNet</strong>。首先通过在Transformer块之间共享参数来压缩可训练参数和深度，并用MoE层替换了FFN层。实验表明，WideNet通过较少的可训练参数实现了最佳性能，优于ViT等网络。<br><strong>作者单位</strong>：新加坡国立大学</p></blockquote><h2 id="简介">简介</h2><p>Transformer最近在各种任务上取得了令人瞩目的成果。为了进一步提高Transformer的有效性和效率，现有工作中有2种思路：</p><ol><li>通过扩展到更多可训练的参数来扩大范围；</li><li>通过参数共享或模型压缩随深度变浅。</li></ol><p>然而，<strong>当可供训练的Token较少时，较大的模型通常无法很好地扩展，而当模型非常大时，则需要更高的并行性</strong>。由于表征能力的损失，与原始Transformer模型相比，较小的模型通常会获得较差的性能。</p><p>在本文中，为了用更少的可训练参数获得更好的性能，作者提出了一个通过更宽的模型框架来有效地部署可训练参数。特别地，作者通过用MoE替换前馈网络（FFN）来沿模型宽度进行缩放。然后，使用单个层归一化跨Transformer Block共享MoE层。这种部署起到了转换各种语义表征的作用，这使得模型参数更加高效和有效。</p><p>为了评估所提的框架，作者设计了WideNet并在ImageNet-1K上对其进行评估。最好的模型在0.72倍可训练参数下的性能比ViT高1.46%。使用0.46×和0.13×参数的WideNet仍然可以分别超过ViT和ViT-MoE0.83%和2.08%。</p><h3 id="本文主要贡献">本文主要贡献</h3><ol><li><p>为了提高参数效率，提出了跨Transformer Block的共享MoE层。MoE层可以在不同的Transformer Block中接收到不同的Token表示，这使得每个MoE都能得到充分的训练；</p></li><li><p>在Transformer Block之间保持单独的标准化层。单独的层具有少量额外的可训练参数可以将输入隐藏向量转换为其他语义。然后，将不同的输入输入到同一Attention层或更强的MoE层，以建模不同的语义信息。</p></li><li><p>结合以上2种思路，作者提出了更广、更少参数、更有效的框架。然后将用这个框架构建了<strong>WideNet</strong>，并在ImageNet上对其进行评估。WideNet在可训练参数少得多的情况下，性能大大优于Baseline。</p></li></ol><h2 id="Methodology">Methodology</h2><p>本文研究了一种新的可训练参数部署框架，并在Transformer上实现了该框架。总体结构如图所示。在这个例子中使用Vision Transformer作为Backbone，这意味着在Attention或FFN层之前进行规范化。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210728/2.png" alt=""></p><p>WideNet可以很容易地扩展到其他Transformer模型。在WideNet中用MoE层代替FFN层。采用跨Transformer Block的参数共享，以实现更有效的参数部署。在每个MoE层中有一个路由器来选择K个专家来学习更复杂的表示。请注意，为了更多样化的语义表示，层标准化中的可训练参数不是共享的。</p><h3 id="MoE条件计算">MoE条件计算</h3><p>核心理念是沿着宽度部署更多的可训练参数，沿着深度部署更少的可训练参数。为此，作者使用MoE将Transformer缩放到大量的可训练参数。MoE作为一种典型的条件计算模型，只激活少数专家，即网络的子集。对于每个输入只将需要处理的隐藏表示的一部分提供给选定的专家。</p><p>根据Shazeer，给定E个可训练专家，输入表示$x\in R^D$, MoE模型的输出可以表示为:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210728/3.png" alt=""></p><p>其中$e(·)_i$为非线性变换, $g(·)_i$为可训练路由器$g(·)$输出的第$i$个元素，通常$e(·)$和$g(·)$都是由神经网络参数化的。</p><p>由上式可知，当$g(·)_i$为稀疏向量时，在训练过程中，只有部分专家会被反向传播激活和更新。在本文中，对于普通的MoE和WideNet，每个专家都是一个FFN层。</p><h3 id="Routing">Routing</h3><p>为了保证稀疏Routing $g(·)$，使用TopK()来选择排名最高的专家。根据Riquelme论文$g(·)$可表示为:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210728/4.png" alt=""></p><p>式中$f(·)$为路由线性变换，$\epsilon$为专家路由的高斯噪声。在$f(·)$之后使用softmax，以获得更好的性能和更稀疏的专家。当K&lt;&lt;E时，$g(x)$的大部分元素为零，从而实现稀疏条件计算。</p><h3 id="Balanced-Loading">Balanced Loading</h3><p>在基于MoE的Transformer中将每个Token分派给K个专家。在训练期间，如果MoE模型没有规则，大多数Token可能会被分派给一小部分专家。这种不平衡的分配会降低MoE模型的吞吐量。</p><p>此外，更重要的是，大多数附加的可训练参数没有得到充分的训练，使得稀疏条件模型在缩放时无法超越相应的稠密模型。因此，为了平衡加载需要避免2件事:</p><ol><li>分配给单个专家的Token太多，</li><li>单个专家收到的Token太少。</li></ol><p>为了解决第1个问题，需要缓冲容量B。也就是说，对于每个专家，最多只保留B个Token，不管分派给该专家多少Token。如果分配了超过B个Token，那么左边的Token将被丢弃:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210728/5.png" alt=""></p><p>其中C是容量比，这是一个预定义的超参数，用于控制为每个专家保留的Token的比例。通常，$C \in [1,2]$，在没有特别说明的情况下，设C为1.2。K是每个token选择的专家数量。N为每个设备上的batch-size。L是序列的长度。对于计算机视觉任务，L表示每幅图像中patch Token的数量。</p><p>缓冲区容量B帮助每个专家去除冗余token以最大化吞吐量，但它不能确保所有专家都能收到足够的token进行训练。换句话说，直到现在，路由仍然是不平衡的。因此，使用可微分的负载均衡损失，而不是在路由器中均衡负载时单独的负载均衡和重要性权重损失。对于每个路由操作，给定E专家和N批带有NL token，在训练时模型总损失中加入以下辅助损失:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210728/6.png" alt=""></p><p>其中$m_i$是向量。第$i4元素为分配给专家$i$的token的比例:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210728/7.png" alt=""></p><p>式中$h(·)$为Eq.2中TopK选取的指标向量。$H(x_j)_i$是$H(x_j)$的第$i$个元素。值得注意的是，与Eq.2中的$g(x_i)_i$不同，$m_i$和$H(x_j)_i$是不可微的。然而，需要一个可微分的损失函数来优化端到端的MoE。因此，将式4中的$P_i$定义为:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210728/8.png" alt=""></p><p>可以看出$P_i$是softmax激活函数后路由线性变换的第$i$个元素，并且$P_i$是可微的。</p><p>负载均衡损失的目的是实现均衡分配。当最小化$l_{balance}$时,可以看到m和P都接近均匀分布。</p><h3 id="Cross-Transformer-blocks共享MoE">Cross Transformer blocks共享MoE</h3><p>如图1所示，WideNet采用跨Transformer blocks参数共享的方式。使用参数共享有2个原因。首先，在本文中目标是一个更参数有效的框架。其次，由于使用MoE层来获得更强的建模能力，为了克服稀疏条件计算带来的过拟合问题，需要给每个专家提供足够的token。为此，WideNet使用相同的路由器和专家在不同的Transformer blocks。</p><p>形式上，给定隐藏表示$H^1={h_1^1,h_2^1,…,h_L^1}$作为第1个Transformer blocks的输入，可以将参数共享定义为$H^{i+1}=MoE(H^i)$，这与现有的基于MoE的模型$H^{i+1}=MoE^i(H^i)$不同。</p><p>请注意，虽然在包括路由器在内的MoE层共享可训练参数，但对应于同一Token的Token表示在每个Transformer blocks中是不同的。也就是说，$h^j_i$和$h^{j+1}_i$可以派给不同的专家。因此，每个专家将接受更多不同的Token的训练，以获得更好的性能。</p><h3 id="Individual-Layer-Normalization">Individual Layer Normalization</h3><p>虽然现有的工作表明不同Transformer blocks的激活是相似的，但余弦距离仍然远远大于零。因此，不同于现有的作品在Transformer blocks之间共享所有权重，为了鼓励不同块更多样化的输入表示，这里只共享multi-head attention layer和FFN(或MoE)层，这意味着层标准化的可训练参数在块之间是不同的。</p><p>综上所述，框架中的第$i$个Transformer blocks可以写成:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210728/9.png" alt=""></p><p>标准化层LayerNormal(·)为:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210728/10.png" alt=""></p><p>其中$\gamma \in R^D$和$\beta \in R^D$是2个可训练的参数。</p><p>层归一化只需要这2个小向量，所以单独的归一化只会在框架中添加一些可训练的参数。可以发现共享层标准化和单个标准化之间的差异是输出的平均值和大小。对于共享层归一化，MHA和MoE层的输入在不同的Transformer blocks中更相似。由于共享了可训练矩阵，鼓励更多样化的输入以在不同的Transformer blocks中表示不同的语义。</p><h2 id="Optimization">Optimization</h2><p>虽然在每个Transformer block中重用了路由器的可训练参数，但由于输入表示的不同，分配也会不同。因此，给定T次具有相同可训练参数的路由操作，需要优化的损失如下:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210728/11.png" alt=""></p><p>其中λ是一个超参数，以确保平衡分配，将其设置为一个相对较大的数，即在本工作中为0.01。与现有的基于MoE的模型相似，作者发现该模型的性能对λ不敏感。$L_{main}$是Transformer的主要目标。例如，在有监督图像分类中，$l_{main}$是交叉熵损失。</p><h2 id="实验">实验</h2><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210728/12.png" alt=""></p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210728/13.png" alt=""></p><h2 id="参考">参考</h2><p>[1].Go Wider Instead of Deeper<br></p>]]></content>
      
      
      <categories>
          
          <category> Transformer </category>
          
          <category> 即插即用模块 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MoE插件 </tag>
            
            <tag> ViT模型 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>超越MobileNet V3 | 详解SkipNet+Bias Loss=轻量化模型新的里程碑</title>
      <link href="/2021/07/26/29/"/>
      <url>/2021/07/26/29/</url>
      
        <content type="html"><![CDATA[<p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210726/1.png" alt=""></p><h2 id="简介">简介</h2><p>近年来，Compact卷积神经网络(CNNs)的性能有了显著的提高。然而，在参数众多的情况下，它们仍然无法提供与普通CNN相同的预测能力。这些层捕获的多样且丰富的特征是这些CNN的一个重要特征。</p><p>然而，大型CNN和小型CNN在这一特性上的差异很少被研究。在Compact CNN中，由于参数数量有限，不太可能获得丰富的特征，特征多样性成为本质特征。在模型推断期间，从数据点派生的激活映射中呈现的不同特征可能表明存在一组惟一描述符，这是区分不同类的对象所必需的。</p><p>相比之下，特征多样性较低的数据点可能无法提供足够数量的描述符来进行有效预测;作者称之为<strong>随机预测</strong>。随机预测会对优化过程产生负面影响，并损害最终性能。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210726/2.png" alt=""></p><p>本文提出通过重塑标准交叉熵来解决随机预测带来的问题，使其偏向具有有限数量独特描述特征的数据点。本文所提出的<strong>新型Bias Loss</strong>将训练重点放在一组有价值的数据点上，防止大量学习特征差的样本误导优化过程。此外，为了说明多样性的重要性，作者提出了一系列<strong>SkipNet模型</strong>，其体系结构增加了最后一层的唯一描述符的数量。实验表明，所提出的损失函数优于交叉熵损失。此外，与MobileNetV3 Large相比，Skipnet-M在相似的计算条件下，在ImageNet上分类准确率提高了1%。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210726/3.png" alt=""></p><p>总结起来，本文的贡献有3个方面:</p><ol><li>设计了损失函数，以减少随机预测在Compact CNN优化中的误导;</li><li>提出了一个有效的神经结构<strong>SkipNet模型</strong>，以增加数据点的数量与大量独特的描述特征;</li><li>在资源受限的条件下，<strong>SkipNet模型</strong>在ImageNet分类任务上达到了最先进的性能。</li></ol><h2 id="相关工作">相关工作</h2><h3 id="Mobile-Architectures">Mobile Architectures</h3><p>在已经开发的几种CNN架构中，MobileNet和ShuffleNet系列是比较优秀的工作，因为他们具有好性能的同时实现了更少的FLOPs。</p><p>MobileNetV2引入了inverted residual blocks，以改进MobileNetV1的性能。此外，MobileNetV3利用NAS(神经体系结构搜索)技术，以更少的FLOPs实现更高的性能。</p><p>ShuffleNet引入了通道shuffle操作，以提高通道组内的信息流动。ShuffleNetV2进一步提高了硬件上的实际速度。尽管用很少的flop实现了高性能，但是在网络的最后一层保持独特描述特性的重要性一直没有得到很好的利用。</p><p>为此，作者提出了SkipNet体系结构，该体系结构旨在增加最后一层中惟一描述特性的数量，并减少随机预测的数量。SkipNet与以前的高性能CNN有很多相似之处，特别是MobileNetV3中使用的inverted residual blocks和U-Net中使用的跳连的概念。作者强调通过简单修改取得了卓越的结果，该修改不是由于设计上的创新，而是由于网络与损失的结合。</p><h3 id="损失函数">损失函数</h3><p>在许多任务中，最常见的目标函数选择是交叉熵。然而，各种研究表明，旨在解决特定问题的损失函数的设计可以有显著的好处。</p><p>Focal loss提出对标准交叉熵进行重塑，以解决目标检测器在训练过程中遇到的前景-背景类不平衡的问题。</p><p><strong>标签平滑</strong>的机理建议在交叉熵计算中使用soft目标。这些soft目标是原始目标的加权混合，并在标签上均匀分布。这项技术有助于防止网络在图像分类、语言翻译和语音识别等众多任务中出现过拟合。</p><p>各种各样的研究试图解决噪音标签造成的障碍。在reweight论文中，作者引入了加权交叉熵的变化，其中权值由多层感知器学习。这些工作的重点主要是优化具有大量参数的模型的性能。</p><p>相反，本文的损失是为了解决Compact模型中缺少参数而产生的问题，即随机预测可能导致优化过程中出现误导的问题。</p><h2 id="Bias-Loss">Bias Loss</h2><p>本文所设计的Bias Loss是为了解决在深度卷积神经网络优化过程中由于随机预测而导致的误导问题。作者认为在compact神经网络中，数据点无法提供足够数量的独特特征来描述物体，迫使模型产生随机预测，也就是说，在没有特征多样性的情况下进行预测。</p><p>作者在所有的实验中，作者采用信号方差来作为多样性的一个简单度量，它可以表明特征图从平均值扩散到多远。这种选择背后的直觉是，方差越高获得大量独特特征的机会就越高。</p><p>对于方差计算，作者使用最后一个卷积层(在池化和dropout操作之前)的特征映射。这有助于避免在结果和估计更好的学习信号的数据点。设$T\in R^{b×c×h×w}$为卷积层的输出，其中b为batchsize，c为输入通道数量，h和w为张量的高度和宽度。在方差计算之前，T被展开成一个二维数组$t\in R^{b×n}$，其中$n=c×h×w$。批处理中第$i$个数据点的特征图方差为:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210726/4.png" alt=""></p><p>其中：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210726/5.png" alt=""></p><p>此外，在损失函数中方差被缩放到[0,1]范围内，即:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210726/6.png" alt=""></p><p>式中，在每次迭代时，Max和min分别为该批特征图中激活量的最大值和最小值。这样做是为了确保方差值中的异常值不会导致损失的大变化，也不会使模型不稳定。</p><p>此外，作者建议将关于缺乏唯一描述特征的知识注入优化过程中，为此，作者提出了新的损失函数，即<strong>Bias Loss</strong>。Bias Loss是一种动态缩放的交叉熵损失，其尺度随着数据点方差的减小而衰减。</p><p>设$X\in R^{c×h×w}$为特征空间，其中c为若干输入通道，h,w为输入数据的高度和宽度，$Y={1,…,K}$为标签空间，其中k为类的数量。在一个标准场景中有一个数据集$D=(x_i,y_i)^N_{i=1}$，其中每个$(x_i,y_i)\in X×Y$，神经网络$f(X;\theta)$，其中θ为模型参数。通常，训练的目的是通过最小化训练集的期望损失来学习模型。一般来说，分类问题的交叉熵损失为:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210726/7.png" alt=""></p><p>其中，作者认为神经网络的输出层是一个softmax。为了校准每个数据点对累积损失的贡献，作者建议添加一个非线性尺度函数，其目的是在低方差和高方差的数据点之间产生bias。bias loss定义为：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210726/8.png" alt=""></p><p>式中，α和β为可调参数，v为卷积层输出的缩放方差。下图显示了几个α和β值的偏置函数。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210726/9.png" alt=""></p><p>作者注意到偏差函数的2个性质:</p><ol><li><p>当方差较低时，函数值达到最小值(1−β)，这些数据点的影响是向下加权的。随着方差的增加，z(v)的值随着数据点的影响呈指数增加。</p></li><li><p>参数α平滑地调整高方差示例的影响率。随着α的增大，高方差数据点的影响也增大。</p></li></ol><p>此外，下图给出了基于方差和预测得分的偏差损失值。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210726/10.png" alt=""></p><p>对于正确和错误预测的低置信度和低方差数据点，损失是向下加权的。此外，对于高置信度和高方差的错误预测，它是向上加权的，因为从这类具有大量独特特征的数据点学习，可以对优化过程产生积极的影响。实验结果表明，选择α=0.3;β=0.3获得最佳性能。</p><p>从直观上看，所提出的函数有助于将学习重点放在能够提供大量独特特征的样本上，并减少在优化过程中可能因随机预测而造成的误导。</p><h2 id="SkipNet">SkipNet</h2><p>作者还引入了一个新的计算块。所提出的block可以很容易地集成到现有的体系结构中，并且不需要额外的工作就可以促进向最后一层的信息流。</p><h3 id="Skip-Block">Skip Block</h3><p>skip block想法是直接将low-level features从第一层传递到最后一层。块的设计是由U-Net架构驱动的，在自动编码器风格的架构中，编码器和解码器中具有相同空间维度的层的输出通过skip connections连接起来。</p><p>一般来说，在分类网络中，层空间大小逐渐减小，无法直接使用skip connections。为了解决这一限制，作者提出了一个中间块，它将不同空间大小的层连接起来，并利用从第一层提取的low-level特征来丰富最后一层。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210726/11.png" alt=""></p><p>如图所示，skip block由池操作和卷积组成。首先，为了保持关键特征和减小空间大小，作者采用自适应平均池化，然后采用3个卷积层。批处理归一化(BN)和ReLU非线性应用在每个卷积层之后，除了最后一个没有使用ReLU的卷积层。选择自适应平均池化是因为它考虑了所有的特性，使得skip block可以处理所有的输入值。卷积层参数采用了MobileNetV3中对inverted residual blocks的setting。</p><h3 id="SkipNet-2">SkipNet</h3><p>由于主要目标是增加compact神经网络中独特描述特性的数量，同时降低计算复杂性，因此提出了一种部署skip blocks的SkipNet架构。由于MobileNetV3优越的性能，所以将其作为设计基准。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210726/12.png" alt=""></p><p>SkipNet(上图)由inverted residual blocks和MobileNetV3的分类块组成，其中包括我们的新skip块。</p><p>第一层是由15个inverted residual block进行卷积。</p><p>在第一个卷积块之后插入2个skip block(图4)，将信息传递给第6和第10个inverted residual block。</p><p>在skip和inverted residual block之后，再应用卷积层和全局平均池化，最终得到由dropout层和全连接层组成的分类块。</p><p>与MobileNetV3类似，SkipNet使用hard-swish非线性函数。从表2中可以看出，SkipNet在移动设备上的延迟与MobileNetV3相当。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210726/13.png" alt=""></p><p>尽管所描述的体系结构已经能够保证高性能和低延迟，但在某些情况下可能需要更快的模型或更高的精度。为了提供一个完全可定制的网络，作者将inverted residual block中的宽度乘法器集成到skip block中以控制每层通道的数量。</p><p>通过操纵宽度乘法器，可以改变整个网络的宽度。这将导致模型大小和计算成本的变化，以及性能的变化。通常，乘法器的增加将导致性能和延迟的增加，反之亦然。介绍的体系结构提供了一个基本的设计供参考，为了进一步改进，可以使用AutoML方法来调优skip block并提高性能。</p><h2 id="实验">实验</h2><h3 id="ImageNet-Classification">ImageNet Classification</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210726/14.png" alt=""></p><h3 id="Object-Detection">Object Detection</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210726/15.png" alt=""></p><p>很好的做到了速度与精度的平衡，是一个非常不错的工作。</p><h2 id="参考">参考</h2><p>[1].Bias Loss for Mobile Neural Networks<br></p>]]></content>
      
      
      <categories>
          
          <category> 论文解读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MobileNet V3 </tag>
            
            <tag> SkipNet+Bias Loss </tag>
            
            <tag> 轻量化模型 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>算法部署 | 万字长文带你从C++案例一步一步实操cmake（起飞系列）</title>
      <link href="/2021/07/15/28/"/>
      <url>/2021/07/15/28/</url>
      
        <content type="html"><![CDATA[<blockquote><p>从实战出发，一步一步教你如何使用cmake工具，让C++工程编译更有效率。</p></blockquote><h2 id="什么是-CMake">什么是 CMake</h2><p>你或许听过好几种Make工具，例如GNU Make ，QT的qmake ，微软的MS nmake，BSD Make（pmake），Makepp，等等。这些Make工具遵循着不同的规范和标准，所执行的Makefile格式也千差万别。这样就带来了一个严峻的问题：如果软件想跨平台，必须要保证能够在不同平台编译。而如果使用上面的 Make 工具，就得为每一种标准写一次Makefile，这将是一件让人抓狂的工作。</p><p>CMake就是针对上面问题所设计的工具：它首先允许开发者编写一种平台无关的CMakeList.txt 文件来定制整个编译流程，然后再根据目标用户的平台进一步生成所需的本地化Makefile和工程文件，如Unix的Makefile或Windows的Visual Studio工程。从而做到“Write once,run everywhere”。显然，CMake是一个比上述几种make更高级的编译配置工具。一些使用CMake作为项目架构系统的知名开源项目有VTK、ITK、KDE、OpenCV、OSG等。</p><p>在linux平台下使用CMake生成Makefile并编译的流程如下：</p><ul><li>编写 CMake 配置文件 CMakeLists.txt；</li><li>执行命令cmake PATH或者ccmake PATH生成Makefile（ccmake和cmake的区别在于前者提供了一个交互式的界面）。其中，PATH是CMakeLists.txt 所在的目录；</li><li>使用 make 命令进行编译。</li></ul><h3 id="入门案例一：单个源文件">入门案例一：单个源文件</h3><h4 id="1、编写源文件">1、编写源文件</h4><p>对于简单的项目，只需要写几行代码就可以了。例如，假设现在我们的项目中只有一个源文件 main.cpp ，该程序的用途是计算一个数的指数幂。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * power -- Calculate the power of number.</span></span><br><span class="line"><span class="comment"> * @param base: Base value.</span></span><br><span class="line"><span class="comment"> * @param exponent: Exponent value.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * @return base raised to the power exponent.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">double</span> <span class="title">power</span><span class="params">(<span class="keyword">double</span> base, <span class="keyword">int</span> exponent)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">auto</span> result = base;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> (exponent == <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; exponent; ++i)&#123;</span><br><span class="line">        result = result * base;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> *argv[])</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (argc &lt; <span class="number">3</span>)&#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Usage: %s base exponent \n&quot;</span>, argv[<span class="number">0</span>]);</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">double</span> base = <span class="built_in">atof</span>(argv[<span class="number">1</span>]);</span><br><span class="line">    <span class="keyword">int</span> exponent = <span class="built_in">atoi</span>(argv[<span class="number">2</span>]);</span><br><span class="line">    <span class="keyword">double</span> result = <span class="built_in">power</span>(base, exponent);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;%g ^ %d is %g\n&quot;</span>, base, exponent, result);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="2、编写-CMakeLists-txt">2、编写 CMakeLists.txt</h4><p>首先编写 CMakeLists.txt 文件，并保存在与 main.cpp 源文件同个目录下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> CMake 最低版本号要求</span></span><br><span class="line">cmake_minimum_required (VERSION 2.8)</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 项目信息</span></span><br><span class="line">project (Test_Demo1)</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置c++的版本并添加多线程的使用</span></span><br><span class="line">set(CMAKE_CXX_FLAGS &quot;$&#123;CMAKE_CXX_FLAGS&#125; -std=c++17 -pthread&quot;)</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 指定生成目标</span></span><br><span class="line">add_executable(Demo1 main.cpp)</span><br></pre></td></tr></table></figure><p>CMakeLists.txt 的语法比较简单，由命令、注释和空格组成，其中命令是不区分大小写的。符号 # 后面的内容被认为是注释。命令由命令名称、小括号和参数组成，参数之间使用空格进行间隔。</p><p>对于上面的 CMakeLists.txt 文件，依次出现了几个命令：</p><ul><li>cmake_minimum_required：指定运行此配置文件所需的 CMake 的最低版本；</li><li>project：参数值是Test_Demo1，该命令表示项目的名称是 Test_Demo1 。</li><li>add_executable：将名为 main.cpp 的源文件编译成一个名称为Demo1 的可执行文件。</li></ul><h4 id="3、编译项目">3、编译项目</h4><p>之后，在当前目录执行 sudo cmake . ，得到 Makefile 后再使用 sudo make 命令编译得到 Demo1 可执行文件。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">pi@raspberrypi:~/Desktop/rr/Test_Demo1 $ cmake .</span><br><span class="line">-- The C compiler identification is GNU 8.3.0</span><br><span class="line">-- The CXX compiler identification is GNU 8.3.0</span><br><span class="line">-- Check for working C compiler: /usr/bin/cc</span><br><span class="line">-- Check for working C compiler: /usr/bin/cc -- works</span><br><span class="line">-- Detecting C compiler ABI info</span><br><span class="line">-- Detecting C compiler ABI info - done</span><br><span class="line">-- Detecting C compile features</span><br><span class="line">-- Detecting C compile features - done</span><br><span class="line">-- Check for working CXX compiler: /usr/bin/c++</span><br><span class="line">-- Check for working CXX compiler: /usr/bin/c++ -- works</span><br><span class="line">-- Detecting CXX compiler ABI info</span><br><span class="line">-- Detecting CXX compiler ABI info - done</span><br><span class="line">-- Detecting CXX compile features</span><br><span class="line">-- Detecting CXX compile features - done</span><br><span class="line">-- Configuring done</span><br><span class="line">-- Generating done</span><br><span class="line">-- Build files have been written to: /home/pi/Desktop/rr/Test_Demo1</span><br><span class="line">pi@raspberrypi:~/Desktop/rr/Test_Demo1 $ sudo make clean &amp;&amp; sudo make</span><br><span class="line">Scanning dependencies of target Demo1</span><br><span class="line">[ 50%] Building CXX object CMakeFiles/Demo1.dir/main.cpp.o</span><br><span class="line"><span class="meta">[100%</span><span class="bash">] Linking CXX executable Demo1</span></span><br><span class="line"><span class="meta">[100%</span><span class="bash">] Built target Demo1</span></span><br><span class="line">pi@raspberrypi:~/Desktop/rr/Test_Demo1 $ ./Demo1 5 2</span><br><span class="line">5 ^ 2 is 25</span><br><span class="line">pi@raspberrypi:~/Desktop/rr/Test_Demo1 $ ./Demo1 2 0</span><br><span class="line">2 ^ 0 is 1</span><br><span class="line">pi@raspberrypi:~/Desktop/rr/Test_Demo1 $ ./Demo1 9 2</span><br><span class="line">9 ^ 2 is 81</span><br><span class="line">pi@raspberrypi:~/Desktop/rr/Test_Demo1 $ </span><br></pre></td></tr></table></figure><h3 id="入门案例二：多个源文件">入门案例二：多个源文件</h3><p>同一目录，多个源文件</p><p>上面的例子只有单个源文件。现在假如把 power 函数单独写进一个名为 MathFunctions.cpp 的源文件里，使得这个工程变成如下的形式：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">./Test_Demo2</span><br><span class="line">    |</span><br><span class="line">    +--- main.cpp</span><br><span class="line">    |</span><br><span class="line">    +--- MathFunctions.cpp</span><br><span class="line">    |</span><br><span class="line">    +--- MathFunctions.h</span><br></pre></td></tr></table></figure><p>这个时候，CMakeLists.txt 可以改成如下的形式：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> CMake 最低版本号要求</span></span><br><span class="line">cmake_minimum_required (VERSION 2.8)</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 项目信息</span></span><br><span class="line">project (Test_Demo2)</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 指定生成目标</span></span><br><span class="line">add_executable(Demo2 main.cpp MathFunctions.cpp)</span><br></pre></td></tr></table></figure><p>唯一的改动只是在 add_executable 命令中增加了一个 MathFunctions.cpp 源文件。这样写当然没什么问题，但是如果源文件很多，把所有源文件的名字都加进去将是一件烦人的工作。更省事的方法是使用 aux_source_directory 命令，该命令会查找指定目录下的所有源文件，然后将结果存进指定变量名。其语法如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">aux_source_directory(&lt;dir&gt; &lt;variable&gt;)</span><br></pre></td></tr></table></figure><p>因此，可以修改 CMakeLists.txt 如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> CMake 最低版本号要求</span></span><br><span class="line">cmake_minimum_required (VERSION 2.8)</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 项目信息</span></span><br><span class="line">project (Test_Demo2)</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置c++的版本并添加多线程的使用</span></span><br><span class="line">set(CMAKE_CXX_FLAGS &quot;$&#123;CMAKE_CXX_FLAGS&#125; -std=c++17 -pthread&quot;)</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查找当前目录显得所有源文件</span></span><br><span class="line"><span class="meta">#</span><span class="bash">并将名称保存到DIR_SRCS的变量中</span> </span><br><span class="line">aux_source_directory(. DIR_SRCS)</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">指定生成目标</span></span><br><span class="line">add_executable(Demo2 $&#123;DIR_SRCS&#125;)</span><br></pre></td></tr></table></figure><p>这样，CMake会将当前目录所有源文件的文件名赋值给变量DIR_SRCS ，再指示变量DIR_SRCS中的源文件需要编译成一个名称为Demo2的可执行文件。</p><h3 id="入门案例三：多个目录，多个源文件">入门案例三：多个目录，多个源文件</h3><p>现在进一步将MathFunctions.h和MathFunctions.cpp文件移动到MyMath目录下。将main.cpp放入src文件里。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">./Test_Demo3</span><br><span class="line">├── MyMath</span><br><span class="line">│   ├── MathFunctions.cpp</span><br><span class="line">│   └── MathFunctions.h</span><br><span class="line">└── src</span><br><span class="line">    └── main.cpp</span><br></pre></td></tr></table></figure><p>对于这种情况，需要在Test_Demo3根目录下编写一个CMakeLists.txt文件， 以及分别在项目根目录 scr 和 MyMath 目录里各编写一个 CMakeLists.txt 文件。</p><p>Test_Demo3 根目录中的 CMakeLists.txt ：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> CMake 最低版本号要求</span></span><br><span class="line">cmake_minimum_required (VERSION 2.8)</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 项目信息</span></span><br><span class="line">project (Test_Demo3)</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置c++的版本并添加多线程的使用</span></span><br><span class="line">set(CMAKE_CXX_FLAGS &quot;$&#123;CMAKE_CXX_FLAGS&#125; -std=c++17 -pthread&quot;)</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查找当前目录显得所有源文件</span></span><br><span class="line"><span class="meta">#</span><span class="bash">并将名称保存到DIR_SRCS的变量中</span> </span><br><span class="line">aux_source_directory(. DIR_SRCS)</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">添加math子目录</span></span><br><span class="line">add_subdirectory(./MyMath)</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">添加src子目录</span></span><br><span class="line">add_subdirectory(./src)</span><br></pre></td></tr></table></figure><p>该文件添加了下面的内容: 使用命令 add_subdirectory 指明本项目包含一个子目录 MyMath以及一个子目录src，这样 MyMath 目录和src目录下的 CMakeLists.txt 文件和源代码也会被处理 。</p><p>MyMath 子目录中的 CMakeLists.txt：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 查找当前目录下的所有源文件</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 并将名称保存到 DIR_LIB_SRCS 变量</span></span><br><span class="line">aux_source_directory(. DIR_LIB_SRCS)</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">设置输出路径</span></span><br><span class="line">SET(LIBRARY_OUTPUT_PATH $&#123;PROJECT_BINARY_DIR&#125;/lib)</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 生成动态链接库SHARED 静态库STATIC</span></span><br><span class="line">add_library (MathFunctions SHARED $&#123;DIR_LIB_SRCS&#125;)</span><br></pre></td></tr></table></figure><p>在该文件中使用命令 add_library 将 src 目录中的源文件编译为动态链接库。</p><p>src 子目录中的 CMakeLists.txt：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">包含的文件路径</span></span><br><span class="line"><span class="meta">INCLUDE_DIRECTORIES($</span><span class="bash">&#123;PROJECT_SOURCE_DIR&#125;/MyMath)</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">设置输出文件的路径</span></span><br><span class="line">set(EXECUTABLE_OUTPUT_PATH $&#123;PROJECT_BINARY_DIR&#125;/bin)</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查找当前目录显得所有源文件</span></span><br><span class="line"><span class="meta">#</span><span class="bash">并将名称保存到DIR_SRCS的变量中</span> </span><br><span class="line">aux_source_directory(. DIR_SRCS)</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">指定生成目标</span></span><br><span class="line">add_executable(Demo3 main.cpp)</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">添加链接库</span></span><br><span class="line">target_link_libraries(Demo3 MathFunctions)</span><br></pre></td></tr></table></figure><p>使用命令 target_link_libraries 指明可执行文件Demo3 需要连接一个名为 MathFunctions 的链接库 。</p><p>之后的构建模式如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo mkdir build</span><br><span class="line">sudo cd build</span><br><span class="line">sudo cmake ..</span><br><span class="line">sudo make clean &amp;&amp; sudo make </span><br></pre></td></tr></table></figure><h3 id="入门案例四：自定义编译选项">入门案例四：自定义编译选项</h3><p>CMake 允许为项目增加编译选项，从而可以根据用户的环境和需求选择最合适的编译方案。</p><p>例如，可以将MathFunctions库设为一个可选的库，如果该选项为ON，就使用该库定义的数学函数来进行运算。否则就调用标准库中的数学函数库。</p><p>修改src目录下的CMakeLists 文件</p><p>我们要做的第一步是在src目录的 CMakeLists.txt 文件中添加该选项：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"># 设置c++的版本并添加多线程的使用</span><br><span class="line">set(CMAKE_CXX_FLAGS &quot;$&#123;CMAKE_CXX_FLAGS&#125; -std=c++17 -pthread&quot;)</span><br><span class="line"></span><br><span class="line">#设置输出文件的路径</span><br><span class="line">set(EXECUTABLE_OUTPUT_PATH $&#123;PROJECT_BINARY_DIR&#125;/bin)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 加入一个配置头文件，用于处理 CMake 对源码的设置(输入，输出)</span><br><span class="line">configure_file (</span><br><span class="line">  &quot;$&#123;PROJECT_SOURCE_DIR&#125;/config/config.hpp.in&quot;</span><br><span class="line">  &quot;$&#123;PROJECT_BINARY_DIR&#125;/config/config.hpp&quot;</span><br><span class="line">  )</span><br><span class="line"></span><br><span class="line"># 是否使用自己的 MathFunctions 库</span><br><span class="line">option (USE_MYMATH</span><br><span class="line">       &quot;Use provided math implementation&quot; ON)</span><br><span class="line"></span><br><span class="line"># 是否加入 MathFunctions 库</span><br><span class="line">if (USE_MYMATH)</span><br><span class="line">  include_directories (&quot;$&#123;PROJECT_SOURCE_DIR&#125;/MyMath&quot;)</span><br><span class="line">endif (USE_MYMATH)</span><br><span class="line"></span><br><span class="line"># 查找当前目录显得所有源文件</span><br><span class="line">#并将名称保存到DIR_SRCS的变量中 </span><br><span class="line">aux_source_directory(./ DIR_SRCS)</span><br><span class="line"></span><br><span class="line">#指定生成目标</span><br><span class="line">add_executable(Demo4 $&#123;DIR_SRCS&#125;)</span><br><span class="line"></span><br><span class="line">#添加链接库</span><br><span class="line">target_link_libraries(Demo4 MathFunctions)</span><br></pre></td></tr></table></figure><p>其中：</p><ul><li>configure_file 命令用于加入一个配置头文件 config.hpp，这个文件由 CMake <a href="http://xn--config-ht8i.hpp.in">从config.hpp.in</a> 生成，通过这样的机制，将可以通过预定义一些参数和变量来控制代码的生成。</li><li>option 命令添加了一个 USE_MYMATH 选项，并且默认值为 ON 。</li><li>USE_MYMATH变量的值来决定是否使用我们自己编写的 MathFunctions 库。</li></ul><h4 id="修改-src目录下的main-cpp-文件">修改 src目录下的main.cpp 文件</h4><p>之后修改 main.cpp 文件，让其根据 USE_MYMATH 的预定义值来决定是否调用标准库还是 MathFunctions 库：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&quot;../config/config.hpp&quot;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">ifdef</span> USE_MYMATH</span></span><br><span class="line">  <span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&quot;../MyMath/MathFunctions.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">else</span></span></span><br><span class="line">  <span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;math.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> *argv[])</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (argc &lt; <span class="number">3</span>)&#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Usage: %s base exponent \n&quot;</span>, argv[<span class="number">0</span>]);</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">double</span> base = <span class="built_in">atof</span>(argv[<span class="number">1</span>]);</span><br><span class="line">    <span class="keyword">int</span> exponent = <span class="built_in">atoi</span>(argv[<span class="number">2</span>]);</span><br><span class="line"><span class="meta">#<span class="meta-keyword">ifdef</span> USE_MYMATH</span></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Now we use our own Math library. \n&quot;</span>);</span><br><span class="line">    <span class="keyword">double</span> result = <span class="built_in">power</span>(base, exponent);</span><br><span class="line"><span class="meta">#<span class="meta-keyword">else</span></span></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Now we use the standard library. \n&quot;</span>);</span><br><span class="line">    <span class="keyword">double</span> result = <span class="built_in">pow</span>(base, exponent);</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;%g ^ %d is %g\n&quot;</span>, base, exponent, result);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="编写-config目录文件">编写 config目录文件</h4><p>在config目录下编写config.hpp.in文件<br>上面的程序值得注意的是第2行，这里引用了一个 config.hpp 文件，这个文件预定义了 USE_MYMATH 的值。但我们并不直接编写这个文件，为了方便从 CMakeLists.txt 中导入配置，我们编写一个 <a href="http://config.hpp.in">config.hpp.in</a> 文件，内容如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">#cmakedefine USE_MYMATH</span><br></pre></td></tr></table></figure><p>这样CMake会自动根据CMakeLists配置文件中的设置自动生成config.hpp文件。</p><h4 id="编译项目">编译项目</h4><p>现在编译一下这个项目，为了便于交互式的选择该变量的值，可以使用 sudo cmake-gui 命令（该命令会提供一个会话式的交互式配置界面）：</p><p><img src="https://files.mdnice.com/user/3026/9aad106b-76f5-4907-b3cc-822fcb16cb05.png" alt=""></p><p>从中可以找到刚刚定义的 USE_MYMATH 选项，打勾为勾选ON，<br>我们可以试试分别将 USE_MYMATH 设为 ON 和 OFF 得到的结果：<br>USE_MYMATH 为 ON<br>运行结果：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pi@raspberrypi:~/Desktop/rr/Test_Demo4 $ ./Demo4 3 2</span><br><span class="line">Now we use our own Math library. </span><br><span class="line">3 ^ 2 is 9</span><br></pre></td></tr></table></figure><p>此时 config.hpp的内容为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">#define USE_MYMATH</span><br></pre></td></tr></table></figure><p>USE_MYMATH 为 OFF<br>运行结果：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pi@raspberrypi:~/Desktop/rr/Test_Demo4 $ ./Demo4 3 2</span><br><span class="line">Now we use the standard library. </span><br><span class="line">3 ^ 2 is 9</span><br></pre></td></tr></table></figure><p>此时 config.hpp的内容为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/* #undef USE_MYMATH */</span><br></pre></td></tr></table></figure><h3 id="入门案例五：安装和测试">入门案例五：安装和测试</h3><p>CMake 也可以指定安装规则，以及添加测试。这两个功能分别可以通过在产生 Makefile 后使用 sudo make install 和 sudo make test 来执行。在以前的 GNU Makefile 里，你可能需要为此编写 install 和 test 两个伪目标和相应的规则，但在 CMake 里，这样的工作同样只需要简单的调用几条命令。</p><h4 id="定制安装规则">定制安装规则</h4><p>首先先在 Test_Demo5/CMakeLists.txt 文件里添加下面两行：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 指定安装路径 默认在usr/local/路径下边</span><br><span class="line">install (TARGETS Demo5 DESTINATION /home/pi/Desktop/rr/Test_Demo5/bin)</span><br><span class="line">install (FILES &quot;$&#123;PROJECT_BINARY_DIR&#125;/config/config.hpp&quot;</span><br><span class="line">         DESTINATION /home/pi/Desktop/rr/Test_Demo5/include)</span><br></pre></td></tr></table></figure><p>指明 MathFunctions 库的安装路径。之后同样修改根目录的 CMakeLists 文件，在末尾添加下面几行：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 指定 MathFunctions 库的安装路径 默认在usr/local/路径下边</span><br><span class="line">install (TARGETS MathFunctions DESTINATION /home/pi/Desktop/rr/Test_Demo5/lib)</span><br><span class="line"></span><br><span class="line">install (FILES MathFunctions.h DESTINATION /home/pi/Desktop/rr/Test_Demo5/include)</span><br></pre></td></tr></table></figure><p>通过上面的定制，生成的 Demo5 文件会被复制到/home/pi/Desktop/rr/Test_Demo5/bin中。<br>MathFunctions 函数库 <a href="http://libMathFunctions.so">libMathFunctions.so</a><br>文件将会被复制到/home/pi/Desktop/rr/Test_Demo5/lib中。<br>而 MathFunctions.h 和生成的 config.hpp文件则会被复制到<br>/home/pi/Desktop/rr/Test_Demo5/include 中。<br>我们可以验证一下（顺带一提的是，这里的 /usr/local/ 是默认安装到的根目录，可以通过修改 CMAKE_INSTALL_PREFIX 变量的值来指定这些文件应该拷贝到哪个根目录）：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">pi@raspberrypi:~/Desktop/rr/Test_Demo5 $ sudo make install</span><br><span class="line">[ 50%] Built target MathFunctions</span><br><span class="line">[100%] Built target Demo5</span><br><span class="line">Install the project...</span><br><span class="line">-- Install configuration: &quot;&quot;</span><br><span class="line">-- Installing: /home/pi/Desktop/rr/Test_Demo5/bin/Demo5</span><br><span class="line">-- Set runtime path of &quot;/home/pi/Desktop/rr/Test_Demo5/bin/Demo5&quot; to &quot;&quot;</span><br><span class="line">-- Installing: /home/pi/Desktop/rr/Test_Demo5/include/config.hpp</span><br><span class="line">-- Installing: /home/pi/Desktop/rr/Test_Demo5/lib/libMathFunctions.so</span><br><span class="line">-- Installing: /home/pi/Desktop/rr/Test_Demo5/include/MathFunctions.h</span><br></pre></td></tr></table></figure><h4 id="为工程添加测试">为工程添加测试</h4><p>添加测试同样很简单。CMake 提供了一个称为 CTest 的测试工具。我们要做的只是在项目根目录的 CMakeLists 文件中调用一系列的 add_test 命令。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"># 启用测试</span><br><span class="line">enable_testing()</span><br><span class="line"></span><br><span class="line"># 测试程序是否成功运行</span><br><span class="line">add_test (test_run ./bin/Demo5 5 2)</span><br><span class="line"></span><br><span class="line"># 测试帮助信息是否可以正常提示</span><br><span class="line">add_test (test_usage ./bin/Demo5)</span><br><span class="line">set_tests_properties (test_usage</span><br><span class="line">  PROPERTIES PASS_REGULAR_EXPRESSION &quot;Usage: .* base exponent&quot;)</span><br><span class="line"></span><br><span class="line"># 测试 5 的平方</span><br><span class="line">add_test (test_5_2 ./bin/Demo5 5 2)</span><br><span class="line"></span><br><span class="line">set_tests_properties (test_5_2</span><br><span class="line"> PROPERTIES PASS_REGULAR_EXPRESSION &quot;is 25&quot;)</span><br><span class="line"></span><br><span class="line"># 测试 10 的 5 次方</span><br><span class="line">add_test (test_10_5 ./bin/Demo5 10 5)</span><br><span class="line"></span><br><span class="line">set_tests_properties (test_10_5</span><br><span class="line"> PROPERTIES PASS_REGULAR_EXPRESSION &quot;is 100000&quot;)</span><br><span class="line"></span><br><span class="line"># 测试 2 的 10 次方</span><br><span class="line">add_test (test_2_10 ./bin/Demo5 2 10)</span><br><span class="line"></span><br><span class="line">set_tests_properties (test_2_10</span><br><span class="line"> PROPERTIES PASS_REGULAR_EXPRESSION &quot;is 1024&quot;)</span><br></pre></td></tr></table></figure><p>上面的代码包含了四个测试。第一个测试 test_run 用来测试程序是否成功运行并返回 0 值。剩下的三个测试分别用来测试 5 的 平方、10 的 5 次方、2 的 10 次方是否都能得到正确的结果。其中PASS_REGULAR_EXPRESSION 用来测试输出是否包含后面跟着的字符串。</p><p>让我们看看测试的结果：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">pi@raspberrypi:~/Desktop/rr/Test_Demo5 $ sudo make test</span><br><span class="line">Running tests...</span><br><span class="line">Test project /home/pi/Desktop/rr/Test_Demo5</span><br><span class="line">    Start 1: test_run</span><br><span class="line">1/5 Test #1: test_run .........................   Passed    0.01 sec</span><br><span class="line">    Start 2: test_usage</span><br><span class="line">2/5 Test #2: test_usage .......................   Passed    0.01 sec</span><br><span class="line">    Start 3: test_5_2</span><br><span class="line">3/5 Test #3: test_5_2 .........................   Passed    0.01 sec</span><br><span class="line">    Start 4: test_10_5</span><br><span class="line">4/5 Test #4: test_10_5 ........................   Passed    0.01 sec</span><br><span class="line">    Start 5: test_2_10</span><br><span class="line">5/5 Test #5: test_2_10 ........................   Passed    0.01 sec</span><br><span class="line"></span><br><span class="line">100% tests passed, 0 tests failed out of 5</span><br><span class="line"></span><br><span class="line">Total Test time (real) =   0.06 sec</span><br></pre></td></tr></table></figure><p>如果要测试更多的输入数据，像上面那样一个个写测试用例未免太繁琐。这时可以通过编写宏来实现：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># 定义一个宏，用来简化测试工作</span><br><span class="line">macro (do_test arg1 arg2 result)</span><br><span class="line">  add_test (test_$&#123;arg1&#125;_$&#123;arg2&#125; ./bin/Demo5 $&#123;arg1&#125; $&#123;arg2&#125;)</span><br><span class="line">  set_tests_properties (test_$&#123;arg1&#125;_$&#123;arg2&#125;</span><br><span class="line">    PROPERTIES PASS_REGULAR_EXPRESSION $&#123;result&#125;)</span><br><span class="line">endmacro (do_test)</span><br><span class="line"> </span><br><span class="line"># 使用该宏进行一系列的数据测试</span><br><span class="line">do_test (5 2 &quot;is 25&quot;)</span><br><span class="line">do_test (10 5 &quot;is 100000&quot;)</span><br><span class="line">do_test (2 10 &quot;is 1024&quot;)</span><br></pre></td></tr></table></figure><p>关于 CTest 的更详细的用法可以通过使用 man 1 ctest 命令参考 CTest 的文档。</p><p>支持 gdb</p><p>让 CMake 支持 gdb 的设置也很容易，只需要指定 Debug 模式下开启 -g 选项：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">set(CMAKE_BUILD_TYPE &quot;Debug&quot;)</span><br><span class="line">set(CMAKE_CXX_FLAGS_DEBUG &quot;$ENV&#123;CXXFLAGS&#125; -O0 -Wall -g -ggdb&quot;)</span><br><span class="line">set(CMAKE_CXX_FLAGS_RELEASE &quot;$ENV&#123;CXXFLAGS&#125; -O3 -Wall&quot;)</span><br></pre></td></tr></table></figure><p>之后可以直接对生成的程序使用 gdb 来调试。</p><h3 id="入门案例六：添加环境检查">入门案例六：添加环境检查</h3><p>有时候可能要对系统环境做点检查，例如要使用一个平台相关的特性的时候。在这个例子中，我们检查系统是否自带 pow 函数。如果带有 pow 函数，就使用它；否则使用我们定义的 power 函数。</p><p>添加 CheckFunctionExists 宏<br>首先在src目录下的CMakeLists 文件中添加 CheckFunctionExists.cmake 宏，并调用 check_function_exists 命令测试链接器是否能够在链接阶段找到 pow 函数。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"># 检查系统是否支持 pow 函数</span><br><span class="line">include ($&#123;CMAKE_ROOT&#125;/Modules/CheckFunctionExists.cmake)</span><br><span class="line">check_function_exists (pow HAVE_POW)</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">将上面这段代码放在 configure_file 命令前。</span><br><span class="line"></span><br><span class="line">预定义相关宏变量</span><br><span class="line">接下来修改 config.hpp.in 文件，预定义相关的宏变量。</span><br><span class="line"></span><br><span class="line">// does the platform provide pow function?</span><br><span class="line">#cmakedefine HAVE_POW</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">在代码中使用宏和函数</span><br><span class="line">最后一步是修改**src目录下的main.cpp** ，在代码中使用宏和函数：</span><br><span class="line"></span><br><span class="line">#include &lt;iostream&gt;</span><br><span class="line">#include &quot;../config/config.hpp&quot;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">#ifdef HAVE_POW</span><br><span class="line">    #include &lt;math.h&gt;</span><br><span class="line">#else</span><br><span class="line">    #include &quot;../MyMath/MathFunctions.h&quot;</span><br><span class="line">#endif</span><br><span class="line"></span><br><span class="line">int main(int argc, char *argv[])</span><br><span class="line">&#123;</span><br><span class="line">    if (argc &lt; 3)&#123;</span><br><span class="line">        printf(&quot;Usage: %s base exponent \n&quot;, argv[0]);</span><br><span class="line">        return 1;</span><br><span class="line">    &#125;</span><br><span class="line">    double base = atof(argv[1]);</span><br><span class="line">    int exponent = atoi(argv[2]);</span><br><span class="line">    </span><br><span class="line">#ifdef HAVE_POW</span><br><span class="line">    printf(&quot;Now we use the standard library. \n&quot;);</span><br><span class="line">    double result = pow(base, exponent);</span><br><span class="line">#else</span><br><span class="line">    printf(&quot;Now we use our own Math library. \n&quot;);</span><br><span class="line">    double result = power(base, exponent);</span><br><span class="line">#endif</span><br><span class="line"></span><br><span class="line">    printf(&quot;%g ^ %d is %g\n&quot;, base, exponent, result);</span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="入门案例七：添加版本号">入门案例七：添加版本号</h3><p>给项目添加和维护版本号是一个好习惯，这样有利于用户了解每个版本的维护情况，并及时了解当前所用的版本是否过时，或是否可能出现不兼容的情况。</p><p>首先修改顶层 CMakeLists 文件，在 project 命令之后加入如下两行：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">set (Demo_VERSION_MAJOR 1)</span><br><span class="line">set (Demo_VERSION_MINOR 0)</span><br></pre></td></tr></table></figure><p>分别指定当前的项目的主版本号和副版本号。</p><p>之后，为了在代码中获取版本信息，我们可以修改 <a href="http://config.hpp.in">config.hpp.in</a> 文件，添加两个预定义变量：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">// the configured options and settings for Tutorial</span><br><span class="line">#define Demo_VERSION_MAJOR @Demo_VERSION_MAJOR@</span><br><span class="line">#define Demo_VERSION_MINOR @Demo_VERSION_MINOR@</span><br></pre></td></tr></table></figure><p>这样就可以直接在代码中打印版本信息了：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;iostream&gt;</span><br><span class="line">#include &quot;../config/config.hpp</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">#ifdef HAVE_POW</span><br><span class="line">    #include &lt;math.h&gt;</span><br><span class="line">#else</span><br><span class="line">    #include &quot;../MyMath/MathFunctions.h&quot;</span><br><span class="line">#endif</span><br><span class="line">int main(int argc, char *argv[])</span><br><span class="line">&#123;</span><br><span class="line">    if (argc &lt; 3)&#123;</span><br><span class="line">        printf(&quot;%s Version %d.%d\n&quot;,</span><br><span class="line">            argv[0],</span><br><span class="line">            Demo_VERSION_MAJOR,</span><br><span class="line">            Demo_VERSION_MINOR);</span><br><span class="line">        printf(&quot;Usage: %s base exponent \n&quot;, argv[0]);</span><br><span class="line">        return 1;</span><br><span class="line">    &#125;</span><br><span class="line">    double base = atof(argv[1]);</span><br><span class="line">    int exponent = atoi(argv[2]);</span><br><span class="line">    </span><br><span class="line">#ifdef HAVE_POW</span><br><span class="line">    printf(&quot;Now we use the standard library. \n&quot;);</span><br><span class="line">    double result = pow(base, exponent);</span><br><span class="line">#else</span><br><span class="line">    printf(&quot;Now we use our own Math library. \n&quot;);</span><br><span class="line">    double result = power(base, exponent);</span><br><span class="line">#endif</span><br><span class="line"></span><br><span class="line">    printf(&quot;%g ^ %d is %g\n&quot;, base, exponent, result);</span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="入门案例八：生成安装包静态库">入门案例八：生成安装包静态库</h3><p>本节将学习如何配置生成各种平台上的安装包，包括二进制安装包和源码安装包。为了完成这个任务，我们需要用到 CPack ，它同样也是由 CMake 提供的一个工具，专门用于打包。</p><p>修改MyMath 子目录中的 CMakeLists.txt：*</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># 查找当前目录下的所有源文件</span><br><span class="line"># 并将名称保存到 DIR_LIB_SRCS 变量</span><br><span class="line">aux_source_directory(. DIR_LIB_SRCS)</span><br><span class="line"></span><br><span class="line">#设置输出路径</span><br><span class="line">SET(LIBRARY_OUTPUT_PATH $&#123;PROJECT_BINARY_DIR&#125;/lib)</span><br><span class="line"></span><br><span class="line"># 生成动态链接库SHARED 静态库STATIC</span><br><span class="line">add_library (MathFunctions STATIC $&#123;DIR_LIB_SRCS&#125;)</span><br></pre></td></tr></table></figure><p>将 src 目录中的源文件编译为静态链接库。</p><p>在再在顶层的 CMakeLists.txt 文件尾部添加下面几行：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># 构建一个 CPack 安装包</span><br><span class="line">include (InstallRequiredSystemLibraries)</span><br><span class="line">set (CPACK_RESOURCE_FILE_LICENSE</span><br><span class="line">  &quot;$&#123;CMAKE_CURRENT_SOURCE_DIR&#125;/License.txt&quot;)</span><br><span class="line">set (CPACK_PACKAGE_VERSION_MAJOR &quot;$&#123;Demo_VERSION_MAJOR&#125;&quot;)</span><br><span class="line">set (CPACK_PACKAGE_VERSION_MINOR &quot;$&#123;Demo_VERSION_MINOR&#125;&quot;)</span><br><span class="line">include (CPack)</span><br></pre></td></tr></table></figure><p>上面的代码做了以下几个工作：</p><p>导入 InstallRequiredSystemLibraries 模块，以便之后导入 CPack 模块；<br>设置一些 CPack相关变量，包括版权信息和版本信息，其中版本信息用了上一节定义的版本号；<br>导入 CPack 模块。接下来的工作是像往常一样构建工程，并执行 cpack 命令。<br>生成二进制安装包：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo cpack -C CPackConfig.cmake</span><br></pre></td></tr></table></figure><p>生成源码安装包：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo cpack -C CPackSourceConfig.cmake</span><br></pre></td></tr></table></figure><p>我们可以试一下。在生成项目后，执行<br>sudo cpack -C CPackConfig.cmake 命令：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">pi@raspberrypi:~/Desktop/rr/Test_Demo8 $ cpack -C CPackSourceConfig.cmake</span><br><span class="line">CPack: Create package using STGZ</span><br><span class="line">CPack: Install projects</span><br><span class="line">CPack: - Run preinstall target for: Test_Demo8</span><br><span class="line">CPack: - Install project: Test_Demo8</span><br><span class="line">CPack: Create package</span><br><span class="line">CPack: - package: /home/pi/Desktop/rr/Test_Demo8/Test_Demo8-1.0.1-Linux.sh generated.</span><br><span class="line">CPack: Create package using TGZ</span><br><span class="line">CPack: Install projects</span><br><span class="line">CPack: - Run preinstall target for: Test_Demo8</span><br><span class="line">CPack: - Install project: Test_Demo8</span><br><span class="line">CPack: Create package</span><br><span class="line">CPack: - package: /home/pi/Desktop/rr/Test_Demo8/Test_Demo8-1.0.1-Linux.tar.gz generated.</span><br><span class="line">CPack: Create package using TZ</span><br><span class="line">CPack: Install projects</span><br><span class="line">CPack: - Run preinstall target for: Test_Demo8</span><br><span class="line">CPack: - Install project: Test_Demo8</span><br><span class="line">CPack: Create package</span><br><span class="line">CPack: - package: /home/pi/Desktop/rr/Test_Demo8/Test_Demo8-1.0.1-Linux.tar.Z generated.</span><br></pre></td></tr></table></figure><p>此时会在该目录下创建 3 个不同格式的二进制包文件：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pi@raspberrypi:~/Desktop/rr/Test_Demo8 $ ls Test_Demo8-*</span><br><span class="line">Test_Demo8-1.0.1-Linux.sh  </span><br><span class="line">Test_Demo8-1.0.1-Linux.tar.Z   </span><br><span class="line">Test_Demo8-1.0.1-Linux.tar.gz</span><br></pre></td></tr></table></figure><p>这 3 个二进制包文件所包含的内容是完全相同的。我们可以执行其中一个。此时会出现一个由 CPack 自动生成的交互式安装界面：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pi@raspberrypi:~/Desktop/rr/Test_Demo8 $ sudo sh Test_Demo8-1.0.1-Linux.sh </span><br><span class="line">Test_Demo8 Installer Version: 1.0.1, Copyright (c) Humanity</span><br><span class="line">This is a self-extracting archive.</span><br><span class="line">The archive will be extracted to: /home/pi/Desktop/Test_Demo8/build</span><br></pre></td></tr></table></figure><p>If you want to stop extracting, please press <ctrl-C>.<br>就是个测试文件，不需要啥子说明，就是一个pow幂函数的实现！！！！</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Do you accept the license? [yN]: </span><br><span class="line">y</span><br><span class="line">By default the Test_Demo8 will be installed in:</span><br><span class="line">&quot;/home/pi/Desktop/Test_Demo8/build/Test_Demo8-1.0.1-Linux&quot;</span><br><span class="line">Do you want to include the subdirectory Test_Demo8-1.0.1-Linux?</span><br><span class="line">Saying no will install in: &quot;/home/pi/Desktop/Test_Demo8/build&quot; [Yn]: </span><br><span class="line">y</span><br><span class="line">Using target directory: /home/pi/Desktop/Test_Demo8/build/Test_Demo8-1.0.1-Linux</span><br><span class="line">Extracting, please wait...</span><br><span class="line">Unpacking finished successfully</span><br></pre></td></tr></table></figure><p>完成后提示安装到了 Demo8-1.0.1-Linux 子目录中，我们可以进去执行该程序：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pi@raspberrypi:~/Desktop/rr/Test_Demo8 $ sudo ./Test_Demo8-1.0.1-Linux/bin/Demo8 4 2</span><br><span class="line">Now we use our own Math library. </span><br><span class="line">4 ^ 2 is 16</span><br></pre></td></tr></table></figure><h3 id="入门案例九：生成安装包动态库">入门案例九：生成安装包动态库</h3><p>修改MyMath 子目录中的 CMakeLists.txt：*</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># 查找当前目录下的所有源文件</span><br><span class="line"># 并将名称保存到 DIR_LIB_SRCS 变量</span><br><span class="line">aux_source_directory(. DIR_LIB_SRCS)</span><br><span class="line"></span><br><span class="line">#设置输出路径</span><br><span class="line">SET(LIBRARY_OUTPUT_PATH $&#123;PROJECT_BINARY_DIR&#125;/lib)</span><br><span class="line"></span><br><span class="line"># 生成动态链接库SHARED 静态库STATIC</span><br><span class="line">add_library (MathFunctions SHARED $&#123;DIR_LIB_SRCS&#125;)</span><br></pre></td></tr></table></figure><p>将 src 目录中的源文件编译为动态链接库。</p><p>剩下的操作和静态的一样的，注意的是最后那里。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pi@raspberrypi:~/Desktop/rr/Test_Demo8 $ sudo ./Test_Demo8-1.0.1-Linux/bin/Demo8 4 2</span><br><span class="line"></span><br><span class="line">./Test_Demo8-1.0.1-Linux/bin/Demo8: error while loading shared libraries: libMathFunctions.so: cannot open shared object file: No such file or directory</span><br></pre></td></tr></table></figure><p>说明找不到库，因为采用的是动态链接库，所谓动态链接是在运行时链接，编译链接的时候是直接告诉了GCC库的位置，因此会成功，而运行是如果不告诉操作系统库在哪个位置，当然找不到这个库，程序也就不能运行。因此要告诉操作系统库在哪个地方，linux使用LD_LIBRARY_PATH告诉系统库在哪个地方。（LD_LIBRARY_PATH是Linux环境变量名，该环境变量主要用于指定查找共享库（动态链接库）时除了默认路径之外的其他路径）。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo vi /etc/ld.so.conf.d/libc.conf</span><br><span class="line"></span><br><span class="line">#在文件的末尾加入要调用的动态链接库的路径</span><br><span class="line">#这里加入动态库的默认安装路径</span><br><span class="line">/usr/local/lib</span><br></pre></td></tr></table></figure><p>保存退出后，然后在控制台执行以下命令：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo ldconfig</span><br></pre></td></tr></table></figure><p>运行程序就能够执行了。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pi@raspberrypi:~/Desktop/rr/Test_Demo8 $ sudo ./Test_Demo8-1.0.1-Linux/bin/Demo8 4 2</span><br><span class="line">Now we use our own Math library. </span><br><span class="line">4 ^ 5 is 1024</span><br></pre></td></tr></table></figure><p>关于 CPack 的更详细的用法可以通过 使用 man 1 cpack 命令参考 CPack 的文档。</p><h2 id="参考">参考</h2><p>[1].<a href="https://cmake.org/">https://cmake.org/</a><br><br>[2].<a href="https://cmake.org/documentation">https://cmake.org/documentation</a><br><br>[3].<a href="https://cmake.org/cmake/help/latest/guide/tutorial/index.html">https://cmake.org/cmake/help/latest/guide/tutorial/index.html</a><br></p>]]></content>
      
      
      <categories>
          
          <category> 算法部署 </category>
          
          <category> cmake </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cmake实操 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2CSL-YOLO | 超越Tiny-YOLO V4，全新设计轻量化YOLO模型实现边缘实时检测！！！ - 2CSL-YOLO</title>
      <link href="/2021/07/13/27/"/>
      <url>/2021/07/13/27/</url>
      
        <content type="html"><![CDATA[<p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210713/1.png" alt=""></p><blockquote><p>本文提出了一种新的轻量级卷积方法Cross-Stage Lightweight(CSL)模块，从简单的操作中生成冗余特征。在中间展开阶段用深度卷积代替逐点卷积来生成候选特征。所提出的CSL模块可以显著降低计算量。在MS-COCO上进行的实验表明，所提出的CSL-Module可以达到近似$3\times 3$卷积的拟合能力。</p></blockquote><h2 id="简介">简介</h2><p>由于计算资源有限，开发轻量级目标检测器是必要的。为了降低计算成本，如何生成冗余特征起着至关重要的作用。</p><p>本文提出了一种新的轻量级卷积方法——<strong>Cross-Stage Lightweight(CSL)模块</strong>，从简单的操作中生成冗余特征。在中间展开阶段用深度卷积代替逐点卷积来生成候选特征。所提出的CSL模块可以显著降低计算量。在MS-COCO上进行的实验表明，所提出的CSL-Module可以达到近似$3\times 3$卷积的拟合能力。</p><p>最后，利用该模块构建了轻量级检测器CSL-YOLO，在仅43% FLOPs和52%参数的情况下，实现了比TinyYOLOv4更好的检测性能。</p><h2 id="本文方法">本文方法</h2><h3 id="CSL-Module">CSL-Module</h3><p>以往的研究表明，使用更少的计算量来生成冗余特征图，可以大大减少FLOPs。CSPNet提出了一种跨阶段求解的方法，GhostNet系统地验证了cheap操作在该问题中的有效性。然而，问题是生成有价值的特征图的主要操作对于边缘计算来说仍然过于复杂。</p><p>本文建议将输入特征映射划分为2个分支。第1个分支通过像GhostNet那样的cheap操作生成一半冗余的特征图;第2个分支通过轻量级主操作生成另外一半必要的特性映射，然后将2个输出cat在一起。总体架构如下图所示。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210713/2.png" alt=""></p><p>超参数$t$表示特征扩展的比例。在CSL-Bone中将$t$设为3，在else中将$t$设为2。当下采样或扩展块后需要注意力时，插入SE模块或自适应平均池化。此外，作者还使用了Mish作为激活函数，在实验中，Mish在CNN模型中的表现优于ReLU和Swish。</p><p>本文所提出的CSL-Module通过跳过分支的操作生成半冗余特征映射。在主分支上，它不同于CSP模块和Ghost模块。作者建议一个轻量级的主操作来生成另外一半必要的特性映射。在这个分支中设计了一个类似IRB的扩展块，利用跳跃分支的输入特征图和输出特征图，通过深度卷积生成中间候选特征图。</p><p>这个块的最大优点之一是无需pointwise CNN，大家都知道深度卷积比pointwise CNN的FLOPs要少得多。它不同于IRB。IRB使用逐点卷积来生成候选特征图。这个块的其他优点是它充分考虑了所有当前可用的特性，这可以最小化冗余计算。此外，因为已经有了跳跃分支，主分支只需要生成一半的特性图，显著减少了FLOPs。</p><p>总的来说，所提出的CSL-Module通过cheap操作和跨阶段的思想减少了FLOPs。另一方面，特别对主分支进行了轻量级设计。替换了VGG-16中的卷积层来验证CSL-Module的有效性，分别将新的模型记为IRB-VGG-16、Ghost-VGG-16和CSLVGG-16。</p><p>在CIFAR-10上对它们进行了评估，训练设置和trick都是相同的(例如，flip、affine、mixup和steps learning rate)。从下表可以看出，CSL-Module比其他轻量级卷积方法更快。实验证明CSL-Module是一种非常有竞争力的轻量级卷积方法。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210713/3.png" alt=""></p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210713/4.png" alt=""></p><h3 id="构建轻量化组件">构建轻量化组件</h3><p>本文提出了2种轻量级组件CSL-Bone和CSL-FPN。这2个组件是目标检测器所必需的。CSL-Bone比其他backbone模型提取输入图像的特征值更少;CSL-FPN能更有效地预测不同尺度上的边界框。</p><p>###Lightweight Backbone<br>本文所提的CSL-Bone由几个CSL-Module组成。SE模块集成到第1个CSL-Module中，增强了整个组的特征提取能力。此外，还在适当的位置插入池化层进行降采样，以获得高级语义特征。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210713/5.png" alt=""></p><p>最后，CSL-Bone输出3种不同比例的特征图。总体架构如上图所示。作者在CIFAR-10上评估了CSL-Bone、MobileNetv2和GhostNet，并应用了相同的训练设置。由表2可以看出。尽管CSL-Bone的准确率低于MobileNetv2，但CSL-Bone的FLOPs仅比MobileNetv2低58.7%。另一方面，CSL-Bone的准确率比GhostNet高，但只略微增加了FLOPs。</p><p>###Lightweight FPN<br>以往的研究表明，大尺度特征图具有更多的物体细节，如边缘、角落或纹理，而小尺度特征图具有全面的语义理解。Vanilla FPN将小特征图向上采样，然后将它们与大特征图融合。另一方面，Vanilla FPN输出3比例尺特征图。这有助于模型检测不同尺寸的物体。</p><p>本文提出的CSL-FPN首先将FPN中的所有$3\times 3$卷积替换为CSL-Module。其次，在扩展阶段，在2个尺度层之间形成一个中尺度层，这些中尺度层可以增强模型对不同尺度目标的检测能力;第3，在重复阶段，同时有(k)th层、(k-1)th层和(k+1)th层进行特征融合，但每次只使用奇层或偶层。</p><p>例如，在第1次融合中只有第2层和第4层，而在第2次融合中，有第1层，第3层和第5层。也就是说，所提出的CSL-FPN具有与Vanilla FPN相同的卷积数，但具有更多的特征融合。总体架构如图所示。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210713/6.png" alt=""></p><p>在本文提出的CSL-FPN的实现中，为了使元素的添加更容易，作者在层扩展阶段将5个输出层的通道设置为相同的。重复阶段使用一个超参数R来表示CSL-FPN总共堆叠了几个块。较大的R可以实现更高的AP，但FLOPs也会增加，因此在速度和性能之间存在权衡。作者在基于$320\times 320$ CSL-YOLO的MS-COCO上测试了R的最佳值。表3显示了结果。随着R的增大，AP也从18.6%提高到19.8%，AP50从35.5%提高到37.2%，MFLOPs也从127下降到409。经过权衡决定将R设为3。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210713/7.png" alt=""></p><h2 id="Tricks-of-CSL-YOLO">Tricks of CSL-YOLO</h2><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210713/8.png" alt="CSL-YOLO架构"></p><h3 id="Anchors约束">Anchors约束</h3><p>YOLO系列使用K-means和IoU距离函数对ground truth的高度和宽度进行聚类，然后将中心点作为anchor box。这些锚点由k个聚类生成，并根据其规模分配到FPN的输出层。当将输出层从3层扩展到5层时，k也从9层增加到15层。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210713/9.png" alt=""></p><p>如果使用上述方法，那么由于MS-COCO中有许多小目标，这些anchor大多是小规模的。high-level的输出层将被迫使用小规模的anchor。然而，众所周知，high-level的特征图不利于小目标的检测。因此，作者在K-means前加入上式这样的尺度限制，使得生成的anchor分布更符合各个输出层的尺度。作者在下表中进行了实验，可以看到原来的3个输出层扩展到5个输出层后出现了恶化。在添加了约束方法后，它对AP有了相当大的改进。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210713/10.png" alt=""></p><h3 id="Non-Exponential预测">Non-Exponential预测</h3><p>YOLO级数实际上预测了x, y, w, h的偏移量，如下:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210713/11.png" alt=""></p><p>其中$h_{pred}$和$w_{pred}$为模型预测的目标高度和宽度的偏移量，$h_{anchor}$和$w_{anchor}$为anchor的高度和宽度。虽然对数函数可以限制模型的预测范围，但指数函数的敏感性使宽度和高度相当不稳定。因此去掉了log函数，让模型直接预测偏移量。则上式可以修改为:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210713/12.png" alt=""></p><p>从下表可以明显看出，即使在不同的图像尺寸下，非指数预测也可以提高1~2%的AP。如图1所示，然后集成所有组件构建CSL-YOLO。在推理过程中，采用soft-nms技术对重叠框进行惩罚。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210713/13.png" alt=""></p><p>CSL-YOLO在416×416的输入尺度下，所提出的CSL-YOLO使用3.2M参数和1470 MFLOPs获得42.8%的AP50，而Tiny-YOLOv4使用6.1M参数和3450 MFLOPs获得40.2%的AP50。可以说，CSL-YOLO比先进的Tiny-YOLOv4占用更少的时间(FLOPs)和空间(参数)，并能实现令人印象深刻的AP性能。此外，在224×224的输入尺度下，与最轻的YOLO-LITE相比，CSL-YOLO仍然在更低的FLOPs下获得更高的AP性能。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210713/14.png" alt=""></p><h2 id="参考">参考</h2><p>[1].CSL-YOLO: A New Lightweight Object Detection System for Edge Computing<br></p>]]></content>
      
      
      
        <tags>
            
            <tag> Tiny-YOLO V4 </tag>
            
            <tag> 轻量化YOLO模型 </tag>
            
            <tag> 边缘实时检测 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>详细解读 | Google与Waymo教你如何更好的训练目标检测模型！！！</title>
      <link href="/2021/07/12/26/"/>
      <url>/2021/07/12/26/</url>
      
        <content type="html"><![CDATA[<p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210712/1.png" alt=""></p><h2 id="简介">简介</h2><p>通过更好的模型架构、训练和推理方法的结合，目标检测系统的速度-精度Pareto曲线得到了改进。在本文中系统地评估了各种各样的技术，以理解现代检测系统的大多数改进来自哪里。</p><p>本文用RetinaNet和RCNN检测器在普通的ResNet-FPN backbone上对这些改进进行benchamrk测试。普通检测器的准确率提高了7.7%，速度提高了30%。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210712/2.png" alt=""></p><p>作者进一步提供了简单的scale策略来生成形成两条Pareto曲线的模型族，分别命名为RetinaNet-RS和Cascade RCNN-RS。这些简单的rescale检测器探索了one-stage RetinaNet检测器和two-stage RCNN检测器之间的速度-精度权衡。</p><p>最大的Cascade RCNN-RS模型使用ResNet152-FPN backbone实现了52.9%的AP，使用SpineNet143L backbone实现了53.6%。最后，展示了ResNet架构作为目标检测和实例分割系统的backbone，通过3个微小的架构变化后，其性能优于EfficientNet。</p><blockquote><p><strong>本文主要贡献</strong>：</p><ul><li>确定了关键的架构变化、训练方法和推理方法，显著提高了目标检测和实例分割系统的速度和准确性；</li><li>强调了关键的实现细节，并为RetinaNet和Cascade RCNN模型建立了新的baseline；</li><li>提供了2个目标检测模型家族作为未来研究的新baseline，<strong>RetineNetRS</strong>和<strong>Cascade RCNN-RS</strong>；</li><li>探索了one-stage RetinaNet和two-stage RCNN模型之间的速度-精度权衡。</li></ul></blockquote><h2 id="主要改进方法">主要改进方法</h2><h3 id="修改ResNet架构">修改ResNet架构</h3><p>这部分作者从3方面改进了标准的ResNet体系结构，以在较小的计算代价下提高其性能。Bello等人证明了SE Block和ResNet-D对于分类模型都是有效的。最近的检测方向也显示了像Sigmoid线性单元激活这样的非线性激活函数对提高检测性能也是有效的。</p><h4 id="Squeeze-and-Excitation">Squeeze-and-Excitation</h4><p>作者将SE模块应用于ResNet体系结构中的所有残差块中。接下来，在最后的1×1卷积层之后放置一个注意力模块，但在将残差部分与shortcut连接合并之前。所有实验都采用0.25的squeeze ratio。</p><h4 id="ResNet-D-stem">ResNet-D stem</h4><p>作者将原始的ResNet stem修改为ResNet-D stem。综上所述，作者将3个特征维数为64的7×7卷积层替换为特征维数分别为32、32、64的3×3卷积层。第1个3×3卷积的stride=2。同时在每个卷积层之后应用批处理归一化和激活层。</p><h4 id="Sigmoid-Linear-Unit-activation">Sigmoid Linear Unit activation</h4><p>$f(x) = x·\sigma (x)$计算的单元(SiLU)作为ReLU的替代可以得到良好的效果。在本研究中，作者将模型架构中的所有ReLU(backbone、FPN和检测头)替换为SiLU。</p><h3 id="训练和推理方法">训练和推理方法</h3><h4 id="训练方法"><strong>训练方法</strong></h4><h4 id="Strong-data-augmentation">Strong data augmentation</h4><p>作者应用了水平翻转和图像比例抖动，随机比例在[0.1，2.0]是主要的数据扩充策略。例如，如果输出图像的大小是640×640，首先将图像的大小调整为随机在64×64和1280×1280，然后填充或裁剪调整后的图像到640×640。</p><h4 id="Strong-regularization">Strong regularization</h4><p>应用4e-5权重衰减和初始dropout rate为0.2的随机深度进行模型正则化。根据network block在网络中的深度设置network block的dropout rate。一个block的最终dropout rate是通过将初始dropout rate乘以block的顺序除以总block数来计算的。</p><h4 id="Longer-training-schedule">Longer training schedule</h4><p>强数据增强和正则化方法与较长的训练计划相结合，以充分训练模型收敛。在不同的数据集上，不断增加训练的epoch，直到找到最佳schedule。</p><h4 id="推理方法"><strong>推理方法</strong></h4><p>对于推理，作者使用与训练相同的正方形图像大小。调整图像的较长边到目标尺寸，并填充0以保持宽高比。在batchsize为1的Tesla V100 GPU上测量推理速度，设置仅包括模型前向传递时间和前向传递加上后处理(例如，NMS)时间。作者报告了用float16精度和float32精度测量的延迟。进一步的推理时间加速可以通过TensorRT优化，这在本工作中没有使用。</p><h3 id="Model-Scaling-Method">Model Scaling Method</h3><p>作者提出了一种简单而有效的缩放方法，用于one-stage RetinaNet和two-stage RCNN检测器。EfficientDet中的复合缩放规则将输入分辨率与模型深度和所有模型组件(包括backbone、FPN和检测头)的特征维度一起进行缩放。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210712/3.png" alt=""></p><p>作者发现，在速度精度Pareto曲线的大多数阶段中，仅在输入分辨率和backbone深度上扩大模型是相当有效的，同时也非常简单。作者通过经验控制图像分辨率和backbone模型，然后进行如表1所示的网格搜索来确定Pareto曲线。</p><p>对于RetinaNet，将输入分辨率从512提高到768，ResNet backbone深度从50到152。把RetinaNet作为一阶段目标检测，作者发现大的缩放输入分辨率会带来大分辨率的特征图，因此更多的锚点处理。这将导致更高密度的预测Head和复杂的NMS计算。RetinaNet的输入分辨率为768×768。缩放方法如表2所示。作者将重新缩放的RetinaNet模型命名为<strong>RetinaNet-RS</strong>。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210712/4.png" alt=""></p><p>对于RCNN模型，提高输入分辨率比单阶段检测器更有效。RCNN采用两阶段目标检测机制。</p><p>第一阶段是区域建议阶段通常是轻量级和类无关的，因此输入解析不会在第一个阶段造成太多的开销。</p><p>第二阶段总是处理从第一阶段产生的固定数量的预选框。作者设计了新的缩放方法，将输入分辨率从512扩展到1280，将ResNet backbone深度从50扩展到200。RCNN模型的缩放方法如表3所示，重新缩放的模型族称为<strong>RCNN-RS</strong>。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210712/5.png" alt=""></p><h2 id="检测框架">检测框架</h2><h3 id="RetinaNet-RS">RetinaNet-RS</h3><h4 id="检测Head">检测Head</h4><p>遵循标准的RetinaNet head设计。简而言之，在最终的预测层之前使用4个3×3特征维数为256的卷积层和分类子网。每个卷积层之后是一个BN层和一个SiLU。</p><p>卷积层在检测头的所有特征层中共享，而BN层不共享。作者设置anchor的长宽比为[1.0;2.0;0.5]，并将基准anchor大小设置为3.0。focal loss参数α和γ分别设置为0.25和1.5。</p><h4 id="特征提取">特征提取</h4><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210712/6.png" alt=""></p><h3 id="Cascade-RCNN-RS">Cascade RCNN-RS</h3><h4 id="RPN-Head">RPN Head</h4><p>对于Cascade RCNN-RS，作者通常Cascade RCNN的实现。对于RPN head，作者在特征维数256处使用2个3×3卷积层，同时设计与RetinaNet相同的锚定框设置。作者用500个proposals进行训练，用1000个proposals进行推理。</p><h4 id="Box回归Head">Box回归Head</h4><p>作者对box regression head使用2种设置，一种用于常规尺寸模型，另一种用于大尺寸模型。</p><p>对于常规尺寸的模型，作者实现了2个级联head，增加IoU阈值0.6和0.7。在最终的预测层之前，每个head在特征维度256处有4个3×3卷积层，在特征维度1024处有一个全连接层。</p><p>需要注意的是，为了获得良好的性能改进，必须使用与类无关的边界框回归。对于box regression head这里只预测了4个bounding box<br>coordinates，而不是4个(类的数量)。</p><h4 id="Instance-segmentation-head">Instance segmentation head</h4><p>在Instance segmentation head的最终预测层之前，作者在特征维度256处使用了4个3×3卷积层和1个3×3 stride=2反卷积层。</p><h4 id="特征提取-2">特征提取</h4><p>作者首先使用常规大小的Cascade RCNN框架研究了ResNet-50/101/152/200模型族和EfficientNet B1到B7模型族的性能。</p><p>为了扩大基于ResNet的模型，作者使用表3中描述的缩放方法。扩大基于EfficientNet的模型。在ResNet和EfficientNet backbone上附加一个标准的FPN来提取P3到P7多尺度特征。</p><p>为了获得最好的性能，作者采用了SpineNet-143/143L backbone。SpineNet-143L backbone将SpineNet-143中所有卷积层的特征维度均匀地扩大了1.5倍。</p><h2 id="实验">实验</h2><h3 id="速度与精度实验">速度与精度实验</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210712/7.png" alt=""></p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210712/8.png" alt=""></p><h3 id="输入分辨率的影响">输入分辨率的影响</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210712/9.png" alt=""></p><h3 id="后处理速度对比">后处理速度对比</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210712/10.png" alt=""></p><h3 id="SOTA实验">SOTA实验</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210712/11.png" alt=""></p><h2 id="参考">参考</h2><p>[1].Simple Training Strategies and Model Scaling for Object Detection<br></p>]]></content>
      
      
      
        <tags>
            
            <tag> 训练目标检测模型 </tag>
            
            <tag> Waymo </tag>
            
            <tag> Google </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>详细解读SSPNet | 小目标检测该如何进行改进？</title>
      <link href="/2021/07/09/25/"/>
      <url>/2021/07/09/25/</url>
      
        <content type="html"><![CDATA[<p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210709/1.png" alt=""></p><blockquote><p>SSPNet：从无人机图像中检测微小目标的尺度选择金字塔网络,其由<strong>上下文注意模块</strong>（CAM）、<strong>尺度增强模块</strong>（SEM）和<strong>尺度选择模块</strong>（SSM）组成，在Tiny-Person上表现SOTA！性能优于Swin-T、NAS-FPN等网络。<br><strong>作者单位</strong>：四川大学</p></blockquote><h2 id="简介">简介</h2><p>随着搜救需求的不断增加，人们对在无人机（UAV）捕获的大尺度图像中检测感兴趣的物体的需求越来越高，由于物体的尺度极小，这非常具有挑战性。大多数现有方法采用特征金字塔网络（FPN）通过组合深层的上下文特征来丰富浅层的特征。然而，在跨层梯度计算不一致的限制下，FPN中的浅层没有被充分利用来检测微小物体。</p><p>在本文中提出了一个用于Tiny-Person检测的尺度选择金字塔网络（SSPNet），它由3个组件组成：</p><ul><li><p><strong>上下文注意模块（CAM）</strong>：CAM 考虑上下文信息以生成分层 Attention Heatmap。</p></li><li><p><strong>尺度增强模块（SEM）</strong>：SEM 在不同层突出特定尺度的特征，使检测器专注于特定尺度的物体。</p></li><li><p><strong>尺度选择模块（SSM）</strong>：SSM 利用相邻层的关系来实现深层和浅层之间适当的特征共享，从而避免不同层之间梯度计算的不一致。</p></li></ul><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210709/2.png" alt=""></p><p>此外，作者还提出了一种加权负采样（WNS）策略来指导检测器选择更具代表性的样本。Tiny-Person测试表明，本文所提方法优于其他SOTA检测器。</p><h2 id="本文方法">本文方法</h2><p>SSPNet主要是基于Faster R-CNN的框架，包括CAM、SEM、SSM，如图2(a)所示。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210709/3.png" alt=""></p><h3 id="Context-Attention-Module">Context Attention Module</h3><p>为了生成分层的attention heatmap，作者设计了CAM来生成不同层次的attention heatmap。因为上下文信息可以提高检测小目标的性能。</p><p>因此，作者首先将backbone在不同stage产生的特征进行上采样，使其与底部的特征具有相同的形状，并将它们cat起来。然后采用多尺度空间金字塔池算法(atrous spatial pyramid pooling, ASPP)，提取的多尺度特征来寻找目标线索；ASPP生成的上下文感知特征被传递到一个由多个$3\times 3$卷积和sigmoid激活函数组成的激活门，该激活门由多个不同stride的$3\times 3$卷积和sigmoid激活函数组成，生成层次attention heatmap  $A_k$:<br><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210709/4.png" alt=""></p><p>其中σ为sigmoid激活函数，$\phi _k$为第k层的$3\times 3$卷积，$w\in R^{C_F×1×3×3}$为卷积参数，$F_c$为ASPP生成的上下文感知特征,$s=2^{k−2}$为卷积步长。</p><p>为了指出在SSPNet的每一层中哪些尺度对象可以被指定为正样本，作者采用了supervised attention heatmap来突出SSPNet每一层中特定尺度的目标，避免被背景淹没。</p><p>具体地说，supervised attention heatmap与不同层次锚点匹配的对象相关联。如图2(b)所示，supervised attention heatmap在不同的层次上表现出不同的具体比例尺范围，其中红色和绿色虚线框表示对应层锚定不匹配的对象将被视为背景。与之对应的attention heatmap如图2(b)所示，CAM能够生成特定比例尺范围的attention heatmap。</p><h3 id="Scale-Enhancement-Module">Scale Enhancement Module</h3><p>采用SEM增强特定尺度物体的线索。由于不同层的attention heatmap具有不同的尺度偏好，使得SEM能够产生尺度感知特征:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210709/5.png" alt=""></p><p>其中$F^i_k$和$F_k^o$分别为输入特征图和输出尺度感知特征，$A_k$为第k层的attention heatmap。</p><p>请注意，残差连接用于避免降低目标周围的特征，因为上下文信息可能有助于检测。</p><h3 id="Scale-Selection-Module">Scale Selection Module</h3><p>为了从深层为浅层选择合适的特征，作者提出SSM来引导深层向浅层提供合适的特征，在浅层中，合适的特征被优化到同一类，因此不会导致梯度计算不一致。另一方面，如果相邻层的目标都能被检测到，那么深层将提供更多的语义特征，同时与下一层进行优化。SSM可以设计如下:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210709/6.png" alt=""></p><p>其中$A_{k−1}$与$A_k$的交点为$\bigodot$，$f_{nu}$为最近的上采样操作，$P_k’$为第k层的合并映射，$C_{k−1}$为第(k−1)个残块的输出。</p><p>具体来说，SSM扮演着比例选择器的角色。对于下一层尺度范围内的目标对应的特征将被视为合适的特征流入下一层，而其他特征将被弱化，以抑制梯度计算中的不一致性。</p><h3 id="Weighted-Negative-Sampling">Weighted Negative Sampling</h3><p>在无人机拍摄的大视场图像中，复杂的背景通常比自然场景图像引入更多的噪声。此外，这些图像中的部分遮挡导致一些物体只被可见部分标注，导致检测器将人的部分视为完整的个体，尤其是在数据集不大的情况下。基于这些考虑，作者提出了小波神经网络，通过更多地观察代表性样本来增强检测器的泛化能力。</p><p>首先，hard negative样本通常被检测器视为具有较高置信度的positive样本。因此，置信度是最需要考虑的直觉因素。然后，采用前景交叉准则来量化目标的不完整程度。接下来，构造一个考虑IoF和置信度2个因素的评分融合函数:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210709/7.png" alt=""></p><p>其中$C_i$和$I_i$分别表示第$i$个检测结果的置信度和对应的最大IoF， $\lambda$表示调整置信度与IoF的系数。然后，可以根据$s_i$调整每个样本的选择概率。</p><h3 id="损失函数">损失函数</h3><p>总损失如下：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210709/8.png" alt=""></p><p>RPN损失：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210709/9.png" alt=""></p><p>Head损失：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210709/10.png" alt=""></p><p>其中$L_{RPN}$和$L_{Head}$对边界框回归均采用smooth L1 loss，但在分类方面，$L_{RPN}$采用了二进制交叉熵(BCE)损失，$L_{Head}$采用了交叉熵损失。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210709/11.png" alt=""></p><p>对于$L_{RPN}$, i是minibatch中边界框的索引。$rc_i$和$rc^{*}_i$分别表示预测类和ground-truth的概率分布。</p><p>$rt_i$和$rt^{*}_i$  分别表示预测的边界框和ground-truth box。</p><p>分类和回归损失由$N_{cls}$(minibatch)和$N_{reg}$(box位置的数量)归一化，并由一个平衡参数$\mu_1$加权。默认情况下，将$\mu_1$和$\mu_2$设为1。$L_Head$以类似的方式定义。</p><p>$L_A$表示attention loss，引导CAM生成hierarchical attention heatmaps。attention loss可以表述为:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210709/12.png" alt=""></p><p>其中α和β分别表示dice loss $L^d_A$和BCE loss $L^b_A$的超参数。具体来说，为了避免被背景淹没，使用dice loss来优先考虑前景，因为它只与attention heatmaps和supervised attention heatmap之间的交集相关。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210709/13.png" alt=""></p><p>其次，为了弥补attention heatmaps和supervised attention heatmap没有交集时梯度消失的问题，利用BCE损失来处理这种极端情况，提供有效的梯度进行优化。此外，采用OHEM来保证检测器主要聚焦于容易被视为前景的非物体区域，并且将正负极的比例设置为1:3，而不是考虑所有的负样本。具体来说，使用BCE loss来学习分类差的底片，使用dice loss来学习分类分布，以缓解数据的不平衡。</p><h2 id="实验">实验</h2><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210709/14.png" alt=""></p><h3 id="可视化结果：">可视化结果：</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210709/15.png" alt=""></p><h2 id="参考">参考</h2><p>[1].SSPNet: Scale Selection Pyramid Network for Tiny Person Detection from UAV Images<br></p>]]></content>
      
      
      <categories>
          
          <category> SSPNet </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 小目标检测 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>解读UTNet | 用于医学图像分割的混合Transformer架构</title>
      <link href="/2021/07/06/24/"/>
      <url>/2021/07/06/24/</url>
      
        <content type="html"><![CDATA[<p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210706/1.png" alt=""></p><blockquote><p>UTNet：用于医学图像分割的混合Transformer架构,表现SOTA！性能优于ResUNet等网络。<br><strong>作者单位</strong>：罗格斯大学等</p></blockquote><h2 id="简介">简介</h2><p>Transformer架构已经在许多自然语言处理任务中取得成功。然而，它在医学视觉中的应用在很大程度上仍未得到探索。在这项研究中，本文提出了<strong>UTNet</strong>，这是一种简单而强大的混合Transformer架构，它将自注意力集成到卷积神经网络中，以增强医学图像分割。</p><p>UTNet在编码器和解码器中应用自注意力模块，以最小的开销捕获不同规模的远程依赖。为此，作者提出了一种有效的自注意力机制以及相对位置编码，将自注意力操作的复杂性从O(n2)显著降低到近似O(n)。还提出了一种新的自注意力解码器，以从编码器中跳过的连接中恢复细粒度的细节。</p><p>本文所提的方法解决了Transformer需要大量数据来学习视觉归纳偏差的困境。同时混合层设计允许在不需要预训练的情况下将Transformer初始化为卷积网络。</p><p>作者通过实验观察到UTNet相对于最先进方法具有卓越分割性能和鲁棒性，有望在其他医学图像分割上很好地泛化。</p><h2 id="本文方法">本文方法</h2><h3 id="Self-Attention机制的回顾">Self-Attention机制的回顾</h3><p>这里就不进行过多的描述了，前面关于Transformer的文章中已经说过很多次了，这里直接贴出Self-Attention的计算公式吧：</p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210706/2.png" width = "500" align=center /><p>具体细节大家可以参考下面文章的内容：<br>即插即用|卷积与Self-Attention完美融合X-volution插入CV模型将带来全任务涨点。</p><h3 id="Efficient-Self-attention-Mechanism">Efficient Self-attention Mechanism</h3><p>由于图像是高度结构化的数据，在局部足迹内的高分辨率特征图中，除边界区域外，大多数像素具有相似的特征。因此，对所有像素之间的注意力计算是非常低效和冗余的。</p><p>从理论角度来看，对于长序列，自注意力本质上是低秩的，这说明大部分信息集中在最大的奇异值上。受此启发，作者提出了一种有效的自注意机制，如图所示。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210706/3.png" alt=""></p><p>主要的想法是用2个投影来映射keys和values:$K,V\in R^{n\times d}$映射为低维度嵌入：$\overline K,\overline V\in R^{n\times d}$，其中k=hw&lt;&lt;n,h，并且w是经过sub-sampling后feature map缩小的尺寸。</p><p>efficient self-attention定义如下：</p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210706/4.png" width = "500" align=center /><p>这样，计算复杂度降低到O(nkd)。值得注意的是，低维嵌入的投影可以是任何降采样操作，如平均/最大池化，或strided convolutions。在实现中使用1×1卷积和双线性插值来对特征图进行降采样，reduced size为8。</p><h3 id="Relative-Positional-Encoding">Relative Positional Encoding</h3><p>标准的自注意力模块完全丢弃了位置信息，对于高度结构化的图像内容建模是无效的。以往的研究中的正弦嵌入在卷积层中不具有平移等方差的性质。</p><p>因此，作者通过采用了二维相对位置编码添加相对高度和宽度信息。在像素$i=(i_x;i_y)$和像素$j = (j_x;j_y)$：</p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210706/5.png" width = "500" align=center /><p>其中$q_i$为像素$i$的query向量，$k_i$为像素$j$的key向量，$r^W_{j_x−i_x}$和$r^H_{j_y−i_y}$分别为相对宽度$j_x−i_x$和相对高度$j_y−i_y$的可学习嵌入。与efficient self-attention相似，相对宽度和高度是在低维投影后计算的。包含相对位置嵌入的efficient self-attention为:</p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210706/6.png" width = "500" align=center /><p>式中，$S^{rel}_H;S^{rel}_W\in R^{HW×hw}$是满足</p><p>$S^{rel}H[i,j]=q_{i}^{Tr^{H{i_{y}-i_{y}}}};S^{rel}W[i,j]=q_{i}^Tr^{W{j_{x}-i_{x}}}$的沿高度和宽度尺寸的相对位置对数矩阵。</p><h3 id="Network-Architecture">Network Architecture</h3><p>如图，作者试图将卷积和自注意机制结合在一起。因此，混合架构可以利用卷积图像的归纳偏差来避免大规模的预训练，以及Transformer捕获远距离关系的能力。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210706/7.png" alt=""></p><p>由于错误分割区域通常位于感兴趣区域的边界，高分辨率上下文信息在分割过程中起着至关重要的作用。因此，作者将重点放在提出的自注意模块上，使其能够有效地处理大尺寸特征地图。</p><h2 id="实验">实验</h2><h3 id="SOTA结果">SOTA结果</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210706/8.png" alt=""></p><h3 id="消融实验">消融实验</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210706/9.png" alt=""></p><h3 id="可视化结果">可视化结果</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210706/10.png" alt=""></p><h2 id="参考">参考</h2><p>[1].UTNet: A Hybrid Transformer Architecture for Medical Image Segmentation<br></p>]]></content>
      
      
      <categories>
          
          <category> UTNet </category>
          
          <category> Transformer </category>
          
      </categories>
      
      
        <tags>
            
            <tag> UTNet </tag>
            
            <tag> 医学图像分割 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基础学习系列 | 深度学习优化器使用详解</title>
      <link href="/2021/07/02/23/"/>
      <url>/2021/07/02/23/</url>
      
        <content type="html"><![CDATA[<h2 id="深度学习算法本技-优化器">深度学习算法本技----优化器</h2><p>深度学习算法的本质是优化，实现的途径就是通过调整参数，使得损失尽可能的小。优化器就是实现优化的手段，它沿着损失函数导数的反方向调整参数，使得损失函数取值尽可能的小，从而达到优化的目的。</p><p>常见的优化器算法是随机梯度下降（Stochastic Gradient Descent，SGD）算法，以及在随机梯度下降算法基础上改进而来的自适应算法。</p><h2 id="随机梯度下降">随机梯度下降</h2><p>按照参数更新时计算梯度所使用样本数量不同来划分,优化器可以划分成批量梯度下降（Batch Gradient Descent，BGD）、随机梯度下降（Stochastic Gradient Descent，SGD）以及小批量梯度下降（Mini-Batch Gradient Descent，MBGD）。</p><p>批量梯度下降，是指每一次迭代时，使用全部样本来计算梯度。批量梯度下降的优点是优化过程更稳定，如果损失函数是凸函数，那么，批量梯度更新一定能够找到最优解；缺点是，当样本数据量很大时，由于每一次迭代都需要计算所有样本的梯度，所以，计算过程非常耗时。随着深度学习的发展，样本数据规模也越来越大，批量梯度下降耗时的缺点也越来越凸显。</p><p>随机梯度下降，是指每一次迭代时，随机选择一条样本数据来计算梯度。显然，在随机梯度下降算法的每一轮训练的参数更新次数更多。同时由于随机梯度下降的计算量很小，所以，随机梯度下降算法的耗时短、速度快。随机梯度下降的缺点主要有：模型的准确率不如批量梯度下降算法高；可能会收敛域局部最优解，即使损失函数是凸函数也有可能找不到全局最优解；不易实现并行计算，难以充分利用现代计算资源强大的并行计算能力。</p><p>小批量梯度下降，是对批量梯度下降和随机梯度下降两种算法折中。通过设置一个批次规模（Batch Size）参数，每一次迭代时，采用该批次数量的样本数据来计算梯度。可以看出，当批次规模等于样本数据的数量时，小批量下梯度下降就是批量梯度下降；当批次规模等于1时，小批量梯度下降时随机梯度下降。</p><p>小批量梯度下降是使用最多、最常见的优化算法。在实际应用中，经常会省略前面的“小”字，采用随机梯度下降来代指小批量梯度下降。</p><h2 id="自适应优化器">自适应优化器</h2><p>早期，优化器学习率的设置多采用指数衰减法。当初始学习率和衰减率都是已知的情况下，指数衰减法的每个训练步骤的学习率也是固定的，所以，指数衰减法有可能出现学习率衰减过快、或者学习率衰减过慢的问题。衰减过快是指，模型的参数距离最优解依然很远，但是，此时的学习率已经很小、导致模型需要很长的时间才能拟合；衰减过慢是指，模型已经接近最优解，但是，此时学习率依然很大、模型越过最优解来回震荡，同样会导致模型难以拟合，或者说模型训练困难。</p><p>针对以上不足，现代的优化器多采用自适应算法，能在优化过程中动态的调整学习率。比如，引入动量（Momentum）概念，判断本次更新的梯度方向与上一次方向是否一致，如果一致，说明距离最优解很远，就在本次优化的幅度上叠加上一次的优化幅度；如果方向不一致，说明已经越过最优解，就将本地的优化服务减去上一次的优化幅度、降低优化步幅。除了动量之外，有的算法还能各个参数梯度的大小，给不同参数设置不同的学习率，加速模型拟合。</p><p>自适应算法能够大幅度提高模型的拟合速度，广泛的应用在深度学习的各个场景中。</p><h2 id="常用优化器简介">常用优化器简介</h2><p>常用的优化器都会或多或少的采用自适应优化算法。</p><h3 id="SGD">SGD</h3><p>最常见的优化器是随机下降优化器。在默认的情况下，它的出示学习率会设置为0.01，动量的系数设置为0.0（不采用动量技术）。构造函数如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">__init__(</span><br><span class="line">    learning_rate=<span class="number">0.01</span>,</span><br><span class="line">    momentum=<span class="number">0.0</span>,</span><br><span class="line">    nesterov=<span class="literal">False</span>,</span><br><span class="line">    name=<span class="string">&#x27;SGD&#x27;</span>,</span><br><span class="line">    **kwargs</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="Adam">Adam</h3><p>Adam是最简单、易用的优化器，可以适用于各种常见场景。如果您不知道该选择哪一个优化器，那么，Adam优化器就是您最佳选择。从它的构造函数不难理解这一点，在模型情况下，它的初始学习率设置为很小的0.001、同时设置了动量系统0.9和0.999，再结合自适应优化算法Adam在多个常见的业务场景中都能有出色表现。<br>构造函数如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">__init__(</span><br><span class="line">    learning_rate=<span class="number">0.001</span>,</span><br><span class="line">    beta_1=<span class="number">0.9</span>,</span><br><span class="line">    beta_2=<span class="number">0.999</span>,</span><br><span class="line">    epsilon=<span class="number">1e-07</span>,</span><br><span class="line">    amsgrad=<span class="literal">False</span>,</span><br><span class="line">    name=<span class="string">&#x27;Adam&#x27;</span>,</span><br><span class="line">    **kwargs</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>本文选自----《GAN生成对抗神经网络原理与实践》一书中，经授权此公号。</p>]]></content>
      
      
      <categories>
          
          <category> 基础学习系列 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习优化器 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>简单有效 | 详细解读Interflow用注意力机制将特征更好的融合</title>
      <link href="/2021/06/30/22/"/>
      <url>/2021/06/30/22/</url>
      
        <content type="html"><![CDATA[<p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210630/1.png" alt=""></p><h2 id="简介">简介</h2><p>传统的CNN模型具有层次结构，利用最后一层的特征映射来获得预测输出。然而，很难确定最优网络深度，并使中间层学习显著的特征。</p><p>针对传统的CNN模型，本文提出了<strong>Interflow算法</strong>。Interflow根据深度将cnn划分为几个阶段，并利用每个阶段的特征映射进行预测。然后，将这些预测分支输入到一个精心设计的注意力模块中，该模块学习这些预测分支的权值，并将其聚合得到最终的输出。</p><p>Interflow对浅层和深层学习到的特征进行加权和融合，使各个阶段的特征信息得到合理有效的处理，使中间层能够学习到更多有区别的特征，增强了模型的表示能力。</p><p>另外，通过引入注意机制，Interflow可以缓解梯度消失问题，降低网络深度选择的难度，减轻可能出现的过拟合问题。此外，它可以避免网络退化。与原模型相比，带有Interflow的CNN模型在多个基准数据集上获得了更高的测试精度。</p><h2 id="相关工作">相关工作</h2><h3 id="辅助分类器">辅助分类器</h3><p>大多数backbone模型仅利用最后一层的特征映射进行Softmax分类并输出预测结果。然而，GoogLeNet在中间特征层添加了两个辅助分类器(Inception 4a和4d模块)来辅助训练。在训练过程中，将这2个辅助分类器的分类损失与最后一层特征映射(Inception 5b模块)得到的分类损失相乘，权重为0.3，即可得到总损失。然后进行反向传播更新权值。在测试过程中，GoogLeNet简单地去掉了这两个辅助分类器，只使用最后一个分类器进行分类和预测。</p><p>添加2个辅助分类器的基本目的是同时提供正则化和避免梯度消失问题。此外，它应该在正向引导梯度。另外，一些层次较浅的CNN模型取得了较好的性能，这说明CNN模型的中间层需要有较强的识别能力。因此，辅助分类器利用中间层的输出来辅助分类，迫使中间层卷积学习有区别的有效特征。</p><p>但实验数据表明，辅助分类器的作用不大。与不使用辅助分类器的GoogLeNet模型相比，使用辅助分类器的GoogLeNet模型在ImageNet数据集上仅提高了约0.5%的测试精度。此外，使用一个辅助分类器的GoogLeNet模型可以达到几乎相同的效果。</p><p>本文认为，<strong>虽然GoogleNet采用的这些辅助分类器可以缓解梯度消失的问题，但是它并没有很好地利用中间卷积层提取的有效特征</strong>。</p><p>此外，<strong>它融合辅助分类器的方法可能不恰当，从而不能使中间层学习更多的特征。最后，在预测过程中直接丢弃辅助分类器，导致辅助分类器在结果预测中没有起到积极有效的作用</strong>。</p><h3 id="Multi-layer-Features">Multi-layer Features</h3><p>考虑到目标检测任务，输入的图像通常包含大的和小的目标。<strong>深层CNN特征映射具有较强的语义特征，有利于分类识别。但感受野较大，特征分辨率较低。在这种情况下，如果仍然只使用上一层的特征映射进行预测，很容易导致对小目标的漏检和对候选框的定位精度不高</strong>。中、浅层的特征映射具有较大的特征分辨率，有利于目标的定位。</p><p>因此，SSD算法不仅为上一个feature map设置proposal box，还为浅、中、深6种不同比例尺的feature map建立proposal box。在较浅层的feature map中构建相对较小的proposal box来检测小目标，在较深层的feature map中构建较大的proposal box来检测大目标。</p><p>然而，SSD算法对小目标的检测效果较差。其原因是浅层特征的语义信息不足，无法完成预测。因此，研究人员提出了DSSD算法对深、浅层特征映射进行特征融合。</p><p>具体来说，DSSD直接利用最深层的feature map进行回归和分类。然后对该特征映射进行反卷积，再与浅层特征映射的元素相乘，得到深、浅层特征融合。融合特征的输出可以通过回归进行分类和计算。同样，如果继续对浅层特征的feature map进行反卷积和融合，融合后总共可以输出6张feature map。通过回归和分类，可以对这6个特征图进行预测。因此，DSSD输出6个预测。<strong>DSSD将深层特征集成到浅层特征映射中，提升了浅层特征的语义信息，提高了模型性能，特别是对小目标的检测</strong>。</p><p>本文认为，深层和浅层特征的信息融合在性能改善中起着至关重要的作用。然而，正是由于不同层次的特征映射采用了独立的分类和回归预测分支，才能够区分出大目标和小目标的检测任务。因此，这些较浅的卷积层能够学习更多的鉴别特征。受此启发，本文提出了<strong>Interflow</strong>，该方法在主干CNN模型的不同层增加了预测分支。最后，通过一个简单的线性加权或一个拟实现注意机制的学习权层，将分支的预测融合得到最终输出。本设计旨在使网络同时利用深层和浅层中不同层次的抽象信息。此外，它使中间的卷积层能够学习更多的特征，最终获得更优的输出。</p><h3 id="Attention-Mechanism">Attention Mechanism</h3><p>注意机制起源于人类视觉的研究。为了有效地利用有限的视觉资源来处理信息，人类有选择地专注于视觉区域的特定部分。注意力机制主要是确定需要注意的输入部件，并将有限的信息处理资源分配给这些重要部件。</p><p>在计算机视觉领域，注意力机制被引入到视觉信息的处理中。传统的局部图像特征提取、显著性检测和滑动窗口都可以视为一种注意力机制。此外，神经网络中的注意力机制通常是通过一个附加的注意力模块来实现的。这个注意力模块可以为输入的不同部分分配特定的权重。一般来说，注意力机制可以分为2种形式，即<strong>硬注意力机制</strong>和<strong>软注意力机制</strong>。</p><p><strong>硬注意力机制</strong>采用手动固定权重。它迫使计算机集中在需要注意的部分，并减少来自其他部分的影响。然而，它只对特定的采样图片有效。而且，由于权值掩模没有学习到的参数，它是一个不可微分的常数，不可能更新模型中的权值。</p><p>与此相反，<strong>软注意力机制</strong>在卷积层之间使用了一层学习的权重掩码。因此，它可以通过训练识别出神经网络应该密切关注的部分。注意力机制最初广泛应用于自然语言处理领域。引入自注意力机制的Transformer模型是一个经典案例。受NLP的启发，CV领域的模型也不断引入注意力机制。</p><p>在cnn中，注意力机制通常有2种实现模式，即<strong>空间注意力</strong>和<strong>通道注意力</strong>。它们的目的是对输入数据进行重采样，并加强特定对象。</p><p>在一种常见的<strong>空间注意力</strong>模型中，输出可以通过可微分的权重掩模和卷积特征映射之间的点态矩阵相乘得到。</p><p>另一方面，<strong>通道注意力</strong>对特征通道进行加权计算。</p><p>SENet是一种典型的通道注意力实现。SENet利用SE模块自动学习各个特征通道的重要度。<strong>CBAM</strong>兼顾了空间和通道2方面。</p><p>Interflow受到注意力机制的启发，通过注意力机制对不同层次特征映射的预测结果进行加权，得到最终的输出。因此，该模型可以学习不同分支获得的结果的重要程度，降低网络深度选择的难度，缓解极深网络造成的网络退化。</p><h2 id="Interflow">Interflow</h2><h3 id="Interflow算法">Interflow算法</h3><p>根据上面的分析可以知道中间的卷积层需要学习判别特征。此外，还应降低选择网络深度的难度，合理有效地利用不同层次的特征。因此,Interflow算法就是为了这些目的而设计的。<br><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210630/2.png" alt=""></p><p>Interflow首先根据层次的深度将CNN划分为不同的阶段。对于每个阶段的最后一个卷积层，通过自适应平均池化，特征映射的大小减少到$1\times 1$。然后，得到的输出经过一个全连接层，得到该阶段不同类别的置信系数。</p><p>随后，将这些具有不同置信系数的通道连接起来。同样，来自不同分支的特征信息交叉输入到注意力机制模块中，目的是使模型能够合理组织和利用不同阶段学习到的特征。因此，该模式专注于有效的特征，丢弃冗余的特征。通过注意力机制将多层特征聚合后，最终输入到softmax分类器中以实现最终输出。</p><h3 id="注意力模块">注意力模块</h3><p>如前所述，注意力机制的实现方法包括<strong>硬注意力机制</strong>和<strong>软注意力机制</strong>。同样，Interflow的注意力模块也有2种实现方式，即通过各个分支的信息流的交集来实现。</p><h4 id="硬注意力模块">硬注意力模块</h4><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210630/3.png" alt="图a 硬注意模块示意图"></p><p>作为一种手工设计的权重，难以注意的是将不同阶段的输出赋予手工设计的权重。可以直接把每个分支的权重看作一个超参数。但是，当分支流的数量太大时，就会有太多的超参数。因此，很难得到最优组合。此外，在对模型进行多次训练之前无法知道对于一个特定的任务，模型应该集中在哪个阶段。也就是说，手工设置无法合理整合多个分支的特征信息。因此，更注重软注意力。</p><h4 id="软注意力模块">软注意力模块</h4><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210630/4.png" alt="图b 软注意力模块示意图"></p><p>软注意力允许模型独立学习权重。具体来说，利用1×n卷积来让模型学习每个分支的权重。因此，它使模型能够识别出特定任务需要注意的阶段特征信息。并合理有效地整合了不同阶段的特征信息。</p><p>因为不同阶段的特征信息可能对不同的类别有偏好，也就是说，它可能对特定的类别学习到更有区别性的特征。因此，在后续的实验中也制作了模型来学习每个额外分支中不同类别置信度的权重。这是通过用逐点的n×n卷积代替1×n卷积来实现的。</p><h3 id="优点">优点</h3><h4 id="优点1">优点1</h4><p>Interflow利用注意力机制有效利用了不同阶段和抽象层次的特征映射。从而提高了对特征信息的利用。此外，它迫使中间的卷积层学习更多有用的特征。</p><h4 id="优点2">优点2</h4><p>Interflow可以解决梯度消失问题，类似于在系统中添加辅助分类器的GoogLeNet。某些中间卷积层直接连接到最终输出，因此梯度在衰减之前可以传播到较浅的层。</p><h4 id="优点3">优点3</h4><p>此外，CNN的浅层卷积层拟合速度要快于深层卷积层。通过对不同阶段分支权值的自主学习，在训练的最后阶段，高阶阶段分支权值可能为负，高阶阶段分支权值可能为正，这可能会降低浅层卷积层的拟合速度，在一定程度上降低了浅层卷积层过度学习导致过拟合的可能性。</p><h4 id="优点4">优点4</h4><p>当设计一个CNN模型的时候，通常无法提前知道模型的合理深度。而Interflow算法能够对冗余的深层卷积层学习微小甚至零权值，从而减少对这些卷积层特征信息的关注。进而降低了网络深度选择的难度。此外，它降低了模型因层数过多或复杂度过高而产生的过拟合风险。</p><h4 id="优点5">优点5</h4><p>通过为这些破坏模型表示能力的极深层阶段分配轻微或零权值，Interflow算法可以缓解应用于极深层CNN模型时的网络退化问题。</p><h4 id="优点6">优点6</h4><p>Interflow具有可移植性的优点。它可以用于任何backbone。</p><h3 id="复现细节">复现细节</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210630/5.png" alt=""></p><p>图3是本文实验中使用Interflow的具体CNN模型示意图。在该模型中，将VGGNet-16的卷积层作为提取特征base。如图所示，将13个卷积层分为4个阶段，并对每个阶段的输出特征映射进行自适应平均池化和全连接层。这样就得到了分类置信系数，即各阶段分支的特征信息。进一步将通道连接得到的特征映射输入到注意机制中进行加权。然后得到最终输出。从某种意义上说，Interflow可以看作是一个轻微的模型集成。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210630/6.png" alt=""></p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210630/7.png" alt=""></p><h2 id="参考">参考</h2><p>[1].Interflow: Aggregating Multi-layer Feature Mappings with Attention Mechanism<br></p>]]></content>
      
      
      <categories>
          
          <category> Interflow </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 注意力机制 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>详细解读PVT-v2 | 教你如何提升金字塔Transformer的性能？</title>
      <link href="/2021/06/29/21/"/>
      <url>/2021/06/29/21/</url>
      
        <content type="html"><![CDATA[<p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210629/1.png" alt=""></p><blockquote><p>本文提出PVTv2,PVTv2在分类、检测和分割方面显著改进了PVTv1，表现SOTA！性能优于Twins、DeiT和Swin等网络，代码刚刚开源！<br><strong>作者单位</strong>：南京大学, 香港大学, 南京理工大学, IIAI, 商汤科技</p></blockquote><h2 id="简介">简介</h2><p>计算机视觉中的Transformer最近取得了令人鼓舞的进展。在这项工作中，作者通过添加3个改进设计来改进原始金字塔视觉Transformer（PVTv1），其中包括：</p><ul><li>具有卷积的局部连续特征；</li><li>具有zero paddings的位置编码，</li><li>具有平均汇集。</li></ul><p>通过这些简单的修改，PVTv2在分类、检测和分割方面显著优于PVTv1。此外，PVTv2在ImageNet-1K预训练下取得了比近期作品（包括 Swin Transformer）更好的性能。</p><h2 id="Transformer-Backbones">Transformer Backbones</h2><p><strong>ViT</strong>将每个图像作为固定长度的token(patches)序列，然后将它们提供给多个Transformer层进行分类。这是第一次证明纯Transformer在训练数据充足时(例如，ImageNet-22k JFT-300M)也可以在图像分类方面表现出最先进的性能。</p><p><strong>DeiT</strong>进一步探讨了一种数据高效的ViT训练策略和一种蒸馏方法。</p><p>为了提高图像分类性能，一些最新的方法对ViT进行了定制化的改进。</p><p><strong>T2T-ViT</strong>将重叠滑动窗口中的token逐步连接到一个token中。</p><p><strong>TNT</strong>利用内部和外部的Transformer block分别生成pixel emebding和patch embedding。</p><p><strong>CPVT</strong>用条件位置编码取代了ViT中嵌入的固定尺寸位置，使其更容易处理任意分辨率的图像。</p><p><strong>Cross-ViT</strong>通过双支路Transformer处理不同尺寸的图像patch。</p><p><strong>Local-ViT</strong>将深度卷积融合到vision Transformers中，以提高特征的局部连续性。</p><p>为了适应密集的预测任务，如目标检测、实例和语义分割，也有一些方法将CNN中的金字塔结构引入到Transformer主干的设计中。</p><p><strong>PVT-v1</strong>是第一个金字塔结构的Transformer，它提出了一个有4个阶段的分层Transformer，表明纯Transformer主干可以像CNN对等体一样多用途，并在检测和分割任务中表现更好。</p><p>在此基础上，对特征的局部连续性进行了改进，消除了固定尺寸的位置嵌入。例如，Swin Transformer用相对位置偏差替换固定大小的位置嵌入，并限制移动窗口内的自注意。</p><p><strong>CvT、CoaT</strong>和<strong>LeViT</strong>将类似卷积的操作引入vision Transformers。<strong>Twins</strong>结合局部注意和全局注意机制获得更强的特征表示。</p><h2 id="金字塔ViT的改进点">金字塔ViT的改进点</h2><p>与ViT类似，PVT-v1将图像看作是一系列不重叠的patch，在一定程度上失去了图像的局部连续性。此外，PVT-v1中的位置编码是固定大小的，对于处理任意大小的图像是不灵活的。这些问题限制了PVT-v1在视觉任务中的表现。</p><p>为了解决这些问题，本文提出了PVT-v2，它通过以下设计改进了PVT-v1的性能:</p><h3 id="Overlapping-Patch-Embedding">Overlapping Patch Embedding</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210629/2.png" alt=""></p><p>作者利用重叠的patch嵌入来标记图像。如图1(a)所示，对patch窗口进行放大，使相邻窗口的面积重叠一半，并在feature map上填充0以保持分辨率。在这项工作中，作者使用0 padding卷积来实现重叠的patch嵌入。</p><p>具体来说，给定一个大小为h×w×c的输入，将其输入到一个步长为S、卷积核大小为2S−1、padding大小为S−1、卷积核数目为$c’$的卷积中。输出大小为$\frac{h}{S}×\frac{w}{S}×C’$。</p><h3 id="卷积的前向传播">卷积的前向传播</h3><p>这里删除固定大小位置编码,并引入0 padding位置编码进pvt如图1所示(b),在前馈网络中的FC层与GELU之间添加一个3×3的depth-wise卷积。</p><h3 id="Linear-Spatial-Reduction-Attention">Linear Spatial Reduction Attention</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210629/3.png" alt=""></p><p>为了进一步降低PVT的计算成本，作者提出<strong>Linear Spatial Reduction Attention</strong>(SRA)，如图所示。与SRA不同，线性SRA具有像卷积层一样的线性计算和存储成本。具体来说，给定输入规模为h×w×c, SRA的复杂度和线性SRA是:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210629/4.png" alt=""></p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210629/5.png" alt=""></p><p>式中R为SRA的空间压缩比。P为线性SRA的pool size，默认为7。</p><p>结合这3种改进，PVTv2可以：</p><ol><li>获得更多的图像和特征图的局部连续性;</li><li>变分辨率输入更加灵活;</li><li>具有和CNN一样的线性复杂度。</li></ol><h2 id="PVTv2系列详细介绍">PVTv2系列详细介绍</h2><p>作者通过改变超参数将PVTv2从B0扩展到B5。具体如下:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210629/6.png" alt=""></p><ul><li>$S_i$:第$i$阶段overlapping patch embedding的stride;</li><li>$C_i$:第$i$阶段输出的通道数;</li><li>$L_i$:第$i$阶段中编码器层数;</li><li>$R_i$:第$i$阶段SRA的reduction ratio;</li><li>$P_i$:第$i$阶段线性SRA的adaptive average pooling size;</li><li>$N_i$:第$i$阶段有效Self-Attention的head number;</li><li>$E_i$:第$i$阶段前馈层的expansion ratio;</li></ul><p>表1显示了PVT-v2系列的详细信息。设计遵循ResNet的原则。</p><ol><li>随着层数的增加，通道维数增大，空间分辨率减小。</li><li>阶段3为大部分计算开销。</li></ol><h2 id="实验">实验</h2><h3 id="Image-Classification">Image Classification</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210629/7.png" alt=""></p><p>在表中可以看到PVT-v2是ImageNet-1K分类中最先进的方法。与PVT相比，PVT-v2具有相似的FLOPs和参数，但图像分类精度有了很大的提高。例如，PVTv2-B1比PVTv1-Tiny高3.6%，并且PVTv2-B4比PVT-Large高1.9%。</p><p>与最近的同类模型相比，PVT-v2系列在精度和模型尺寸方面也有很大的优势。例如，PVTv2-B5的ImageNet top-1准确率达到83.8%，比Swin Transformer和Twins高0.5%，而参数和FLOPs更少。</p><h3 id="Object-Detection">Object Detection</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210629/8.png" alt=""></p><h2 id="参考">参考</h2><p>[1].PVTv2:Improved Baselines with Pyramid Vision Transformer<br></p>]]></content>
      
      
      <categories>
          
          <category> PVT-v2 </category>
          
          <category> Transformer </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PVT-v2 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>详细解读CVPR 2021轻量化目标检测模型MobileDets</title>
      <link href="/2021/06/25/20/"/>
      <url>/2021/06/25/20/</url>
      
        <content type="html"><![CDATA[<p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210625/1.png" alt=""></p><h2 id="简介">简介</h2><p>构建在深度卷积上的Inverted bottleneck layers已经成为移动设备上最先进目标检测模型的主要构建模块。在这项工作中，作者通过回顾常规卷积的实用性，研究了这种设计模式在广泛的移动加速器上的最优性。</p><p>作者研究发现，正则卷积是一个强有力的组件，以提高延迟-准确性权衡目标检测的加速器，只要他们被放置在网络通过神经结构搜索。通过在搜索空间中合并Regular CNN并直接优化目标检测的网络架构，作者获得了一系列目标检测模型，MobileDets，并在移动加速器中实现了最先进的结果。</p><p>在COCO检测任务上，在移动CPU上MobileDets比MobileNetV3+SSDLite提升了1.7 mAP。MobileDets比MobileNetV2+SSDLite提升了1.9mAP，</p><p>在不增加延迟的情况下，在谷歌EdgeTPU上提升了3.7 mAP，在Qualcomm Hexagon DSP上提升了3.4 mAP，在Nvidia Jetson GPU上提升了2.7 mAP。此外，MobileDets即使不使用金字塔也可以在移动cpu上媲美最先进的MnasFPN，并在EdgeTPUs和dsp上实现更好的mAP分数以及高达2倍的加速。</p><h4 id="本文主要贡献">本文主要贡献</h4><ul><li><p>不像许多现有的专门针对移动应用的IBN层的工作，本文提出了一种基于正则卷积构建块的增强搜索空间系列。证明了NAS方法可以从这种扩大的搜索空间中获得很大的收益，从而在各种移动设备上实现更好的延迟-准确性权衡。</p></li><li><p>提供了MobileDets，一组在多个硬件平台(包括手机)上具有最先进的Mobile目标检测模型。</p></li></ul><h2 id="前人工作">前人工作</h2><h3 id="Mobile-Object-Detection">Mobile Object Detection</h3><p>物体检测是一个经典的计算机视觉任务，其目标是学习识别图像中感兴趣的物体。现有的目标检测器可分为2类:</p><ul><li>Two-Stage检测器</li><li>One-Stage检测器</li></ul><p>对于Two-Stage检测器，包括Faster RCNN, R-FCN和ThunderNet，在检测器做出任何后续预测之前，必须首先生成区域建议。由于这种多阶段的特性，Two-Stage检测器在推理时间方面并不高效。</p><p>另一方面，One-Stage检测器，如SSD、SSDLite、YOLO、SqueezeDet和Pelee，只需要通过一次网络就可以预测所有的边界框，使其成为边缘设备高效推断的理想候选。因此，在这项工作中将重点放在One-Stage检测器上。</p><p>SSDLite是SSD的一个有效变体，它已经成为最流行的轻量级检测器之一。它非常适合移动设备上的应用。高效的backbone，如MobileNetV2、MobileNetV3，与SSDLite配对，以实现最先进的移动检测结果。这两个模型将被用作baseline，以证明所提出的搜索空间在不同移动加速器上的有效性。</p><h3 id="Mobile-Neural-Architecture-Search-NAS">Mobile Neural Architecture Search (NAS)</h3><p>NetAdapt和AMC是第一批尝试利用延迟感知搜索来微调预训练模型的通道数量的公司。MnasNet和MobileNetV3扩展了这一想法，以便在NAS框架中找到资源效率高的架构。通过技术的组合，MobileNetV3在移动CPU上提供了最先进的架构。作为一个互补的方向，最近有许多致力于提高NAS的搜索效率的工作。</p><h3 id="NAS-for-Mobile-Object-Detection">NAS for Mobile Object Detection</h3><p>大部分NAS文献主要集中于分类，只将学习到的特征提取器作为目标检测的backbone，而没有进一步的搜索。最近，多篇论文表明，通过直接搜索目标检测模型可以获得更好的延迟-精度权衡。</p><p>MnasFPN是移动检测模型的一个强大的检测NAS Baseline，它使用对移动友好的搜索空间搜索特征金字塔，极大地利用了深度可分离卷积。但是一九八存在几个因素限制了它在移动加速器上的推广:</p><ol><li><p>到目前为止，深度卷积和特征金字塔在这些平台上都没有得到很好的优化，</p></li><li><p>MnasFPN不搜索backbone，这是延迟的瓶颈。</p></li></ol><p>相比之下，本文的工作依赖于SSD Heads，并提出了基于全卷积Backbone的搜索空间，更易于接受移动加速。</p><h2 id="重新回顾全卷积移动搜索空间">重新回顾全卷积移动搜索空间</h2><h4 id="Are-IBNs-all-we-need">Are IBNs all we need ?</h4><p>Inverted Bottleneck(IBN)的布局如图2所示。IBN的设计目的是减少参数和FLOPS的数量，并利用depthwise和pointwise(1x1)卷积在移动cpu上实现高效率。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210625/2.png" alt=""></p><p>然而，并非所有的FLOPS都是一样的，特别是对于EdgeTPU和dsp这样的现代移动加速器来说。例如，一个常规的卷积在EdgeTPUs上的运行速度可能比它的深度变化快3倍，即使它有7倍的FLOPS。</p><p>观察结果表明，目前广泛使用的IBN-only搜索空间对于现代移动加速器来说可能是次优的。这促使本文通过重新访问规则(完全)卷积来提出新的构建块，以丰富移动加速器的IBN-only搜索空间。具体来说，提出了2个灵活的层分别进行通道扩展和压缩，具体如下。</p><h3 id="融合IBN层-扩展">融合IBN层(扩展)</h3><p>深度可分离卷积是IBN的关键(图2)。深度可分离卷积背后的想法是将深度卷积(用于空间维度)和$1\times 1$点卷积(用于通道维度)的组合代替复杂的全卷积。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210625/3.png" alt=""></p><p>然而，复杂的概念在很大程度上是基于FLOPS或参数的数量来定义的，这与现代移动加速器的推理效率不一定相关。为了整合卷积，作者提出对IBN层进行修改，将其前$1\times 1$卷积和随后的$K\times K$深度卷积融合为单个$K\times K$正则卷积(图3)。融合IBN的初始卷积使Kernel的数量增加了一个因子$s&gt;1$;这一层的扩展比例由NAS算法决定。</p><h3 id="Tucker卷积层-压缩">Tucker卷积层(压缩)</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210625/4.png" alt=""></p><p>在ResNet中引入瓶颈层，降低了在高维特征图上进行大卷积的消耗。压缩比s&lt;1的瓶颈层有：</p><ul><li>输入通道为$C_1$输出通道为$s·C_1$的1×1卷积；</li><li>输入通道为$s·C_1$输出通道为$e·C_2$的K×K卷积；</li><li>输入通道为$e·C_2$输出通道为$C_2$的1×1卷积；</li></ul><p>作者概括了这些瓶颈(图4)通过允许初始1×1卷积比K×K卷积有不同数量的输出卷积核，并让NAS算法决定最终的最佳配置。</p><p>作者将这些新的构建块称为<strong>Tucker卷积层</strong>，因为它们与Tucker分解有关。</p><h2 id="架构搜索方法">架构搜索方法</h2><p>本文提出的搜索空间是互补的任何神经结构搜索算法。</p><p>在实验中使用了<strong>TuNAS</strong>，因为它的可伸缩性和相对于随机baseline的可靠改进。TuNAS构建了一个one-shot模型，该模型包含给定搜索空间中的所有架构选择，以及一个控制器，其目标是选择优化平台感知的奖励功能的架构。</p><p>在搜索过程中，one-shot模型和控制器一起训练。在每一步中，控制器从跨越选择的多项分布中抽样一个随机体系结构，然后更新与抽样体系结构相关的one-shot模型权值的部分，最后计算抽样体系结构的奖励，用于更新控制器。更新内容是通过对以下奖励功能应用强化算法来实现的:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210625/5.png" alt=""></p><h3 id="Cost-Models">Cost Models</h3><p>作者训练了一个Cost Model，$c(·)$——一个线性回归模型，它的特征是，对于每一层，输入/输出通道规模和层类型之间的交叉乘积的指标。该模型跨平台高保真度$(r^2≥0.99)$。线性代价模型与之前提出的基于查找表的方法有关，但只要求在搜索空间内对随机选取的模型的延迟进行基准测试，而不要求度量卷积等单个网络操作的cost。</p><p>因为R(M)是在每次更新步骤时计算的，所以效率是关键。在搜索过程中，本文基于一个小型的小批处理估计了<em>mAP</em>(M)的效率，并使用回归模型作为设备上延迟c(M)的替代。为了收集成本模型的训练数据，本文从搜索空间随机抽取数千个网络架构，并在设备上对每个架构进行基准测试。这在每个硬件和搜索之前只执行一次，消除了服务器类ML硬件和移动设备之间直接通信的需要。对于最终的评估，所找到的体系结构将基于实际硬件测试而不是成本模型进行基准测试。</p><h2 id="实验">实验</h2><h3 id="不同硬件的实验">不同硬件的实验</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210625/6.png" alt=""></p><h4 id="CPU">CPU</h4><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210625/7.png" alt=""></p><p>图5显示了pixel-1 cpu的NAS结果。正如预期的那样，MobileNetV3+SSDLite是一个强大的baseline，因为它的backbone的效率已经在相同的硬件平台上对ImageNet上的分类任务进行了大量优化。作者还注意到，在这种特殊情况下，常规卷积并没有提供明显的优势，因为IBN-only在FLOPS/CPU延迟下已经很强大了。然而，w.r.t.进行特定领域的体系结构搜索，目标检测任务在COCO上提供了不小的收益(150-200ms范围内的+1mAP)。</p><h4 id="EdgeTPU">EdgeTPU</h4><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210625/8.png" alt=""></p><p>图6显示了以Pixel-4 EdgeTPUs为目标时的NAS结果。使用这3种搜索空间中的任何一种进行硬件感知的体系结构搜索，都能显著提高整体质量。这很大程度上是由于baseline架构(MobileNetV2)1对CPU延迟进行了大量优化，这与FLOPS/MAdds密切相关，但与EdgeTPU延迟没有很好地校准。值得注意的是，虽然IBN-only仍然提供了最好的准确性-madds权衡(中间图)，但在搜索空间中使用常规卷积(IBN+Fused或IBN+Fused+Tucker)在准确性-延迟权衡方面提供了明显的进一步优势。实验结果证明了完全卷积在EdgeTPUs上的有效性。</p><h4 id="DSP">DSP</h4><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210625/9.png" alt=""></p><p>图7显示了Pixel-4 DSP的搜索结果。与EdgeTPUs类似，很明显，在搜索空间中包含规则卷积可以在相当的推断延迟下显著改善mAP。</p><h3 id="SOTA对比结果">SOTA对比结果</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210625/10.png" alt=""></p><h2 id="参考">参考</h2><p>[1].MobileDets: Searching for Object Detection Architectures for Mobile Accelerators<br></p>]]></content>
      
      
      <categories>
          
          <category> 2021轻量化目标检测模型 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CVPR </tag>
            
            <tag> MobileDets </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CompConv卷积让模型不丢精度还可以提速</title>
      <link href="/2021/06/23/19/"/>
      <url>/2021/06/23/19/</url>
      
        <content type="html"><![CDATA[<p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210623/1.png" alt=""></p><blockquote><p>为了降低CNN的计算成本，本文提出了一种新的卷积设计：<strong>CompConv</strong>。它利用分治法策略来简化特征图的转换。即插即用！可直接替换普通卷积，几乎不牺牲性能，极致压缩CNN结构！<br><strong>作者单位</strong>：浙江大学, 香港中文大学</p></blockquote><h2 id="简介">简介</h2><p>卷积神经网络(CNN)在各种计算机视觉任务中取得了显著的成功，但其也依赖于巨大的计算成本。为了解决这个问题，现有的方法要么压缩训练大规模模型，要么学习具有精心设计的网络结构的轻量级模型。在这项工作中，作者仔细研究了卷积算子以减少其计算负载。特别是，本文提出了一个紧凑的卷积模块，称为<strong>CompConv</strong>，以促进高效的特征学习。通过分治法的策略，CompConv能够节省大量的计算和参数来生成特定维度的特征图。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210623/2.png" alt=""></p><p>此外，CompConv将输入特征集成到输出中以有效地继承输入信息。更重要的是<strong>CompConv是一个即插即用模块</strong>，可以直接应用于CNN结构，无需进一步设计即可替换普通卷积层。大量的实验结果表明，CompConv可以充分压缩baseline CNN结构，同时几乎不牺牲性能。</p><h4 id="本文主要贡献">本文主要贡献</h4><ul><li>提出了一种紧凑的卷积模块<strong>CompConv</strong>，它利用了分治法策略和精心设计的相同映射大大降低了CNN的计算代价。</li><li>通过研究递归计算对学习能力的影响，对所提出的CompConv进行了详尽的分析。进一步提出了一个切实可行的压缩率控制方案。</li><li>作为传统卷积层的方便替代作者将CompConv应用于各种benchmark。结果表明，CompConv可以大幅节省计算负载，但几乎不牺牲模型在分类和检测任务上的性能的情况下，CompConv方法优于现有的方法。</li></ul><h2 id="本文方法">本文方法</h2><h3 id="动机何在？">动机何在？</h3><p>卷积可以被视为一种将特征从一个空间映射到另一个空间的操作。在某种程度上，这个过程类似于离散傅里叶变换(DFT)，将信号序列从时域映射到频域。快速傅里叶变换(FFT)被广泛用于提高DFT的计算速度。所以本文通过分治策略来压缩普通的卷积模块：CompConv。</p><p>回顾一下FFT的公式。在时域对$N-points$个信号序列$x(t)$进行DFT时，FFT提出将其分割成2个$\frac{N}{2}-points$个子序列，分别记为$x^{(e)}(t)$和$x^{(o)}(t)$，并对每个子序列进行DFT。这里$e$和$o$分别代表“偶”和“奇”。据此，由中间变换结果$X^{(e)}(k)$和$X^{(o)}(k)$得到频域的最终结果$X(k)$:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210623/3.png" alt=""></p><p>其中$W^k_N=exp(−j\frac{2\pi}{N}k)$是一个乘数。在此基础上，可将分解后的结果$X^{(e)}(k)$和$X^{(o)}(k)$进一步划分为更小的分组，形成递归计算的方式。</p><h3 id="CompConv核心单元">CompConv核心单元</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210623/4.png" alt=""></p><p>在FFT的启发下，作者将分治策略引入到卷积模块中以提高其计算效率。通过类比，将由CNN生成的中间特征映射视为通道轴的序列。更具体地说，要开发带有C通道的特性映射$X$，可以选择开发2个特性映射$X_A$和$X_B$，每个特性映射都使用$\frac{C}{2}$个通道，然后将它们组合在一起:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210623/5.png" alt=""></p><p>其中+表示沿通道轴的拼接操作，W是用于变换特征映射的可学习参数。</p><p>上式体现了CompConv的核心思想。具体来说，CompConv的核心单元由2部分实现，如图2所示。其中一个部分(即$X_A$)从输入通道的子集完全映射过来，它能够轻松地从输入中继承信息。另一部分(即$X_B$)通过卷积模块从输入特征转化而来。</p><h3 id="递归计算">递归计算</h3><p>根据式(2)中的公式，将$X_B$进一步分解为2部分，可递归计算出CompConv：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210623/6.png" alt=""></p><p>其中d为递归深度。</p><h4 id="Tailing-Channels">Tailing Channels</h4><p>将第1个分离步骤${X_{A_0},X_{B_0}}$与其他步骤区别对待，如图2所示。具体来说，$X_{A_0}$不是直接从输入中来的，而是从$X_{B_0}$转化而来的。</p><p>这样做主要有2个原因:</p><ul><li>一方面，在所有相同的部件 ${X_{A_i}}^{d-1}_{i=0}$中，</li></ul><p>$X_{A_0}$的通道最多。如果直接将一些输入通道复制为$X_{A_0}$，那么输入特征映射和输出特征映射之间会有过多的冗余，严重限制了该模块的学习能力。</p><ul><li>另一方面，除了从$X_{B_0}$转换之外，还有一些其他方法可以获得$X_{A_0}$，例如从整个输入特征映射或构建另一个递归。其中，从$X_{B_0}$开发$X_{A_0}$是计算成本最低的一种方法。同时，$X_{B_0}$的推导已经从输入特征中收集了足够的信息，因此学习能力也可以保证。</li></ul><h4 id="整合递归结果">整合递归结果</h4><p>为了更好地利用递归过程中的计算，最终的输出不仅通过分组两个最大的子特征得到${X_{A_0},X_{B_0}}$，并综合了所有中间结果，如图2所示。这样就可以充分利用所有的计算操作来产生最终的输出。此外，在这些特征映射的连接之后会添加一个shuffle block。</p><h3 id="Adaptive-Separation策略">Adaptive Separation策略</h3><p>CompConv采用分治策略进行高效的特征学习。因此，如何对通道进行递归分割是影响通道计算效率和学习能力的关键。这里分别用$C_{in}$和$C_{out}$表示输入通道数和输出通道数。$C_{prim}$为图2中d=3时最小计算单元的通道数，如$X_{B_0}$。考虑到递归计算过程中通道数的指数增长，可以预期：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210623/7.png" alt=""></p><p>可以很容易得到以下结果：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210623/8.png" alt=""></p><p>其中[]表示使$C_{prim}$为整数的上限函数。如果所有单元的通道之和大于$C_{out}$，就简单地放入最后一些通道$X_{A_0}$以确保输出特征具有适当的尺寸。</p><h4 id="递归计算深度的选择">递归计算深度的选择</h4><p>由式(5)可知$C_{prim}$高度依赖于递归深度d，这是CompConv模块中的一个超参数。较大的d对应较高的压缩率，其中d=0表示没有压缩。针对现代神经网络不同的结构和不同的模型尺度，作者提出了一种自适应的深度选择策略：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210623/9.png" alt=""></p><p>在这里，$C_0$是一个特定于模型的设计选择，由目标压缩率和模型大小决定（[32;64;128;256;512;···]）。从直觉上看，$C_0$越大，d越小，压缩越轻。从这个角度来看，$C_0$可以用来控制计算效率和学习能力之间的权衡。</p><p>值得注意的是，<strong>递归深度d与Eq.(6)中输入通道的数量$C_{in}$有关，这意味着自适应策略会在不同层动态调整计算深度。同时，为了保证最小单元有足够的学习能力，要给它分配了足够的通道</strong>。换句话说，$C_{prim}$不能太小。从Eq.(5)可以看出，当d=3时，$C_{prim}$只占输出通道的约8%。因此，作者将深度d限定为最大值3。</p><h4 id="推荐配置">推荐配置</h4><p>对于最受欢迎的CNN网络，如VGG和ResNet，建议设置$C_0$=128。作者将此配置表示为<strong>CompConv128</strong>。</p><h3 id="复杂度分析">复杂度分析</h3><p>假设输入和输出特征图的分辨率都是H×W，那么普通卷积和CompConv的计算复杂度分别是：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210623/10.png" alt=""></p><p>其中k为卷积核的大小。</p><p>在$C_{in}=C_{out}$和d=3的配置下，与传统卷积相比，CompConv只需要约20%的计算资源就可以开发具有相同通道数的输出特征。</p><h2 id="实验">实验</h2><h3 id="ImageNet分类">ImageNet分类</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210623/11.png" alt=""></p><p>模型结构为使用CompConv替换普通CNN的ResNet50模型，实验结果如下：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210623/12.png" alt=""></p><p>可以看出，性价比很高的！！！</p><h3 id="COCO目标检测">COCO目标检测</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210623/13.png" alt=""></p><h2 id="参考">参考</h2><p>[1].CompConv: A Compact Convolution Module for Efficient Feature Learning<br></p>]]></content>
      
      
      <categories>
          
          <category> 即插即用模块 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CompConv卷积 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>教你How to train自己的Transformer模型</title>
      <link href="/2021/06/21/16/"/>
      <url>/2021/06/21/16/</url>
      
        <content type="html"><![CDATA[<p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210621/1.png" alt=""></p><h2 id="简介">简介</h2><p>Vision Transformers(Vision transformer, ViT)在图像分类、目标检测和语义分割等视觉应用中得到了具有竞争力得性能。</p><p>与卷积神经网络相比，当在较小的训练数据集上训练时，通常发现Vision Transformer较弱的归纳偏差导致对模型正则化或数据增强(简称AugReg)的依赖增加。为了更好地理解训练数据量、AugReg、模型大小和计算预算之间的相互作用，作者进行了系统的实验研究。</p><p>研究的一个结果是，作者发现增加的计算和AugReg相结合，可以产生与在更多训练数据上训练的模型具有相同性能的模型:在ImageNet-21k数据集上训练各种大小的ViT模型，这些模型与在更大的JFT-300M数据集上训练的模型比较甚至可以得到更好得结果。</p><h2 id="论文作者主要说了什么？">论文作者主要说了什么？</h2><ul><li><p>第一次系统的、大规模的研究在训练Vision Transformer之前，正则化、数据增强、模型大小和训练数据大小之间的相互作用，包括它们各自对达到一定性能水平所需的计算预算的影响。</p></li><li><p>通过迁移学习的视角来评估预训练模型。因此，作者描述了一个相当复杂的训练设置训练前的Vision Transformer跨越不同的模型尺寸。实验得出了许多关于各种技术的影响的令人惊讶的见解，以及什么时候增强和正则化是有益的，什么时候无益的情况。</p></li><li><p>作者还对Vision Transformer的迁移学习配置进行了深入分析。结论是<strong>在广泛的数据集中，即使下游数据似乎与用于前训练的数据只有微弱的关联，迁移学习仍然是最佳的可用选择</strong>。作者分析还表明，<strong>在执行类似的预训练模型中，对于迁移学习来说，具有更多训练数据的模型可能比具有更多数据增强的模型更受青睐</strong>。</p></li><li><p>本文研究将有助于指导未来的Vision Transformer的研究，并将成为一个有效的训练设置的有用来源，以寻求在给定的计算预算下优化他们的最终模型性能。</p></li></ul><h2 id="Findings">Findings</h2><h3 id="Scaling-datasets-with-AugReg-and-compute">Scaling datasets with AugReg and compute</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210621/2.png" alt=""></p><p>研究的一个主要发现(如图1(左)所示)是，通过使用图像增强和模型正则化预训练一个模型，使其达到与增加数据集大小约一个数量级相同的精度。更准确地说，在AugReg ImageNet-1k上训练的最佳模型的性能与在10倍大的普通ImageNet-21k数据集上训练的相同模型的性能差不多。</p><p>类似地，在AugReg ImageNet-21k上训练的最佳模型，当计算量也增加时，将匹配或优于在普通JFT-300M数据集上训练的模型。因此，可以将这些结果与公开可用的数据集进行匹配，可以想象，在JFT-300M上进行更长时间的训练和使用AugReg可能会进一步提高性能。</p><p>当然，这些结果不能适用于任意小的数据集。只对ImageNet-1k的10%进行大量数据增强的ResNet50训练可以改善结果，但不能恢复对完整数据集的训练。</p><h3 id="Transfer-is-the-better-option">Transfer is the better option</h3><p>在这里，作者调查了对于从业者可能遇到的合理规模的数据集，是否建议尝试使用AugReg从头开始进行训练，或者是否把时间和金钱花在迁移预训练模型上更好。其结果是，就大多数实际目的而言，迁移预先训练的模型不仅成本效益更高，而且会带来更好的结果。</p><p>作者在一个与ImageNet-1k数据集相似大小的数据集上对小的ViT-Ti/16模型进行了搜索，寻找一个好的训练策略。Resisc45包含大约3万幅训练图像，由一种非常不同的卫星图像组成，ImageNet-1k或ImageNet-21k都没有很好地覆盖这些图像。图1(右)和图2显示了这一广泛搜索的结果。</p><p>最惊人的发现是，无论花费多少训练时间，对于微小的Pet37数据集，似乎不可能从头开始训练ViT模型，使其达到接近迁移模型的精度。此外，由于预训练模型可以免费获取，所以从业者的预训练成本实际上为零，只有用于迁移学习的计算损失，因此迁移预训练的模型同时也大大便宜。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210621/3.png" alt=""></p><p>对于更大的Resisc45数据集，这个结果仍然成立，尽管多花费2个数量级的计算和执行大量搜索可能接近(但达不到)预先训练的模型的精度。</p><p>值得注意的是，这并没有考虑到很难量化的“exploration cost”。对于训练前的模型，我们强调那些在训练前验证集上表现最好的模型，可以称为推荐模型。可以看到，使用推荐的模型有很高的可能性在几次尝试中就能获得良好的结果。</p><h3 id="More-data-yields-more-generic-models">More data yields more generic models</h3><p>通过将预训练模型迁移到下游任务来研究预训练数据集大小的影响。作者在VTAB上评估了训练前的模型，包括19个不同的任务。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210621/4.png" alt=""></p><p>图3显示了3个VTAB类别的结果:natural、specialized和structured。模型按推理时间进行排序，模型越大推理速度越慢。</p><p>首先比较使用相同计算预算的2个模型，唯一的区别是ImageNet-1k(1.3M图像)和ImageNet-21k (13M图像)的数据集大小。作者实验对比ImageNet-1k训练300个epoch的模型和ImageNet-21k上训练30个epoch模型发现，在ImageNet-21k上进行预训练的模型3个VTAB类别上都明显优于ImageNet-1k。</p><p>随着计算预算的不断增长，作者观察到ImageNet-21k数据集在10倍长的调度上的一致改进。在一些几乎已经解决的任务中，例如花，获得的绝对数量很小。对于剩下的任务，与短期训练的模型相比，改进是显著的。</p><p>总的来说得出的结论是，<strong>数据越多，模型就越通用</strong>，这一趋势适用于不同的任务。作者<strong>建议设计选择使用更多的数据和一个固定的计算预算</strong>。</p><h3 id="Prefer-augmentation-to-regularization">Prefer augmentation to regularization</h3><p>目前尚不清楚在RandAugment和Mixup等数据增强和Dropout和randomdepth等模型正则化之间有哪些取舍。在本节的目标是发现这些通用模式，当将Vision transformer应用到一个新任务时，可以作为经验规则使用。</p><p>在图4中，作者展示了为每个单独设置获得的上游验证得分，即在更改数据集时，数字是不具有可比性的。</p><p>一个单元格的颜色编码其分数的改善或变差，与非正则化的，未增强的设置，即最左边的列。增强强度从左到右依次增大，模型容量从上到下依次增大。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210621/5.png" alt=""></p><p>第1个可见的观察结果是，对于中等规模的ImageNet-1k数据集，任何类型的AugReg都有帮助。然而，当使用10倍大的ImageNet-21k数据集并保持计算固定时，即运行30个epoch，任何一种AugReg都会影响除最大模型之外的所有模型的性能。只有当计算预算增加到300个时，AugReg才帮助更多的模型，尽管即使那样，它也继续影响较小的模型。</p><p>一般来说，<strong>增加增广效果比增加正规化效果好得多</strong>。更具体地说，图4中每个映射右侧的细列显示，对于任何给定的模型，其最佳正则化分数减去最佳非正则化分数。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210621/6.png" alt=""></p><p>在图7中，作者通过dropout和random depth的方式向模型添加正则化时，显示了精度上的增益(绿色，正数)或损失(红色，负数)。在早期的实验中证实，两者结合(峰值)下降概率0.1确实是最好的设置。</p><p>这表明，模型正规化主要帮助较大的模型，但是当训练时间较长的情况下，特别是ImageNet-21的预训练，除了最大的模型它对所有的模型都有害的。</p><h3 id="Choosing-which-pre-trained-model-to-transfer">Choosing which pre-trained model to transfer</h3><p>如上所述，在对ViT模型进行预训练时，各种正则化和数据增强设置会导致模型具有显著不同的性能。</p><p>然后，从实践者的观点来看，一个自然的问题出现了:<strong>如何选择一个模型进一步适应最终的应用程序</strong>?</p><ul><li><p>一种方法是：对所有可用的预训练模型进行下游适应，然后根据下游任务的验证分数选择表现最好的模型。但是这在实践中可能是相当麻烦的。</p></li><li><p>另一种方法是：可以根据上游验证精度选择一个单独的预训练模型，然后只使用该模型进行自适应，这要简单得多。</p></li></ul><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210621/7.png" alt=""></p><p>在这里作者将分析这2种策略之间的权衡。在5个不同的数据集上对它们进行了大量的预训练模型的比较。具体来说，在图5(左)中强调了只适应最好的预训练模型的简单策略和适应所有预训练模型(然后选择最好的)的复杂策略之间的性能差异。</p><p>结果好坏参半，但总体上反映出，在大多数情况下，成本较低的策略与成本较高的策略效果相同。然而，有一些显著的异常值，当它有利于适应所有的模型。</p><p>因此，作者得出结论，<strong>选择一个基于上游分数的单一预训练模型是一种具有成本效益的实用策略</strong>，并在整个论文中使用它。然而，作者也强调，<strong>如果有额外的计算资源可用，那么在某些情况下，可以通过微调额外的预训练模型来进一步提高自适应性能</strong>。</p><h4 id="关于ImageNet-1k数据集验证数据的说明">关于ImageNet-1k数据集验证数据的说明</h4><p>在执行上述分析时，作者发现在ImageNet-21k上预先训练并迁移到ImageNet-1k数据集的模型存在一个微小但严重的问题。这些模型(特别是大型模型)的验证分数与观察到的测试性能没有很好的关联，见图5(左)。这是因为ImageNet-21k数据包含ImageNet-1k训练数据，作者使用训练数据的一个小split来进行评估(见3.1节)。</p><p>因此，在较长训练计划上的大型模型记忆了来训练集的数据，这使得在小评估集中计算的评估指标存在偏差。为了解决这个问题并支持公平的超参数选择，作者使用独立收集的ImageNetV2数据作为传输到ImageNet-1k的验证split。如图5(右)所示。作者没有在其他数据集中观察到类似的问题。</p><p>作者建议<strong>将ImageNet-21k模型迁移到ImageNet-1k的研究人员遵循这一策略</strong>。</p><h3 id="Prefer-increasing-patch-size-to-shrinking-model-size">Prefer increasing patch-size to shrinking model-size</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210621/8.png" alt=""></p><p>作者研究的一个意想不到的结果是，训练了几个模型，它们在推理吞吐量方面大致相等，但在质量方面差异很大。</p><p>具体地说，图6(右)显示了包含Tiny变体的模型比具有32-patch-size的类似快速的更大模型的性能要差得多。对于给定的分辨率，patch-size会影响self-attention执行的token数量，因此会影响模型容量，而<strong>参数计数并不能反映模型容量。参数计数既不反映速度，也不反映容量</strong>。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210621/9.png" alt=""></p><h2 id="参考">参考</h2><p>[1].How to train your ViT? Data, Augmentation,and Regularization in Vision Transformers<br></p>]]></content>
      
      
      <categories>
          
          <category> Transformer </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 详细解读GooGle新作 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AdaConv自适应卷积让你的GAN比AdaIN更看重细节</title>
      <link href="/2021/06/20/15/"/>
      <url>/2021/06/20/15/</url>
      
        <content type="html"><![CDATA[<p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210620/1.png" alt=""></p><blockquote><p>本文提出了AdaIN的改进版本，称为自适应卷积 (AdaConv)，它可以同时适应统计和结构风格，表现SOTA！性能优于AdaIN等网络，已收录于CVPR 2021！<br><strong>作者单位</strong>：迪士尼研究院, ETH Zurich</p></blockquote><h2 id="简介">简介</h2><p>图像的风格迁移是CNN在艺术领域的一种应用，这里的风格迁移是指将其中一幅图像的“风格”迁移到另一幅图像上，同时保留后者的内容。</p><p>近期的SOTA风格迁移模型大多数都是基于最新的自适应实例归一化(AdaIN)，这是一种将风格特征的统计特性迁移到内容图像的技术，可以实时迁移大量风格。</p><p>然而，AdaIN是一个全局的操作；因此，在迁移过程中，风格图像中的局部几何结构常常被忽略。于是作者提出了自适应卷积(AdaConv)，这是AdaIN的通用扩展，允许同时传输统计和结构风格。除了风格迁移，本文的方法还可以很容易地扩展到基于风格的图像生成，以及其他已经采用AdaIN的任务。</p><h2 id="相关工作">相关工作</h2><h3 id="Neural-Style-Transfer-based-on-CNNs">Neural Style Transfer based on CNNs</h3><p>基于CNN的神经网络风格转移最初是由Gatys等人提出的。虽然该方法允许在图像之间转换任意样式，但它的优化过程是比较缓慢的。</p><p>Johnson等人通过引入感知损失(perceptual loss)来解决优化慢的问题，允许显著加速优化并实现实时结果。同时，Ulyanov等人提出了一种新的风格迁移方法，通过评估预先训练特定风格的前馈神经网络，进一步加快了推理速度。在后续工作中，他们还<strong>用实例标准化层(IN)取代了批处理标准化层(BN)</strong>，该方法在不影响速度的情况下产生更高质量的结果。</p><p>为了进一步改善对风格迁移结果的控制，Gatys等人随后在基于优化和前馈的方法中重新制定了损失函数，引入了显式的颜色、规模和空间控制。</p><p>在IN思想的基础上，Dumoulin等人提出了<strong>条件实例规范化(CIN)</strong>，并将CIN层设置在Style上，允许单个模型从32种预定义的Style或它们的插值中执行样式转换。Ghiasi等人则进一步扩展了CIN，允许转换为任意风格;这是通过使用大量的风格语料库来训练一个将风格图像转换为条件反射潜在向量的编码器来实现的。</p><p>Cheng等人提出了基于Patch的风格交换方法来实现任意的风格转移。同时，Huang等人提出了一种任意风格迁移的方法，通过有效地使IN适应风格特征的均值和标准差，从而产生了<strong>AdaIN</strong>。</p><p>Li等人对该方法<strong>AdaIN</strong>进行了扩展，对给定风格的潜在特征进行了增白和着色。Sheng等人进一步扩展了这一想法，并采用了风格装饰器模块和多尺度风格适配。</p><p>最近，Jing等人注意到，直接用样式特性的统计数据替换内容特性的统计数据可能是次优选择;相反，<strong>动态实例标准化</strong>(DIN)方法训练style编码器输出内容特性的新统计数据，同时还调整后续卷积层的大小和采样位置。</p><p>除了实例规范化，Kotovenko等人也探索了对抗学习，以更好地将风格与内容分离。</p><p>而本文工作的目的是进一步扩展AdaIN，根据风格图像预测整个卷积核和偏差，传递统计数据和风格的局部结构。</p><h3 id="Modulation-layers-in-generative-models">Modulation layers in generative models</h3><p>生成模型中的Modulation layers也促成了风格迁移提升的一个突破口。诸如StyleGAN使用了原始版本的AdaIN，但是输入风格统计数据是由MLP从高斯噪声向量中预测的。为了减轻AdaIN造成的一些可见的伪影，StyleGAN-v2用一个权重Modulation layer代替它，它只对标准差进行归一化和调制，而不改变平均值。</p><p>由于AdaIN及其变体只转换全局统计信息，它们对style输入中的局部空间语义不敏感。为了解决这一限制，有学者提出了新的方法，即从输入空间布局图像中预测空间变化的归一化参数。</p><p>SPADE用从输入语义掩码回归的逐像素变换替换AdaIN的全局仿射变换。SEAN进一步扩展了SPADE，考虑了一个附加的带有输入布局掩码的样式向量。SPADE和SEAN都保留了用于语义图像生成的条件空间布局;它们可以有效地控制每个kernel在特定的图像位置是如何被强调或抑制的。</p><p>相反，本文的AdaConv方法在测试时生成全新的kernel。另外，SPADE和SEAN也不直接适用于风格迁移，而是在样风格迁移中必须保留内容图像的空间布局。</p><h3 id="Kernel-prediction">Kernel prediction</h3><p>Kernel prediction也在以前的工作中进行了探讨。</p><p>请注意，上述特征归一化和调制的所有方法都遵循类似的过程:<strong>它们定义了单独应用于每个特征通道的标量仿射变换</strong>。</p><p><strong>主要区别在于</strong>:</p><ol><li>转换参数是手工制作的，还是在训练中学习的，还是在测试时预测的;</li><li>每个通道的转换是全局的还是空间变化的。</li></ol><p>那些回归全局转换的方法也可以理解为在测试时预测1×1 2D kernel。</p><p>对于风格迁移，Chen等人在内容图像特征上学习了卷积的风格特定的滤波器组。该方法局限于过滤训练时学到的组;它不能为在测试时给出的不可见style生成新的kernel。</p><p>Jing等声称使用通用DIN块能够从输入中回归动态卷积;然而，实验结果仅限于1×1转换。Kernel prediction的相关工作也不仅仅只是style transfer。</p><p>最新的蒙特卡罗渲染去噪方法使用神经网络预测动态kernel，用于重建最终去噪的帧。</p><p>神经网络也被提出用于预测手持相机以突发模式拍摄的自然图像的去噪核。Niklaus等人的预测视频帧插值核;他们后来将这项工作扩展到预测可分离卷积参数。</p><p>Xue等利用CNN从随机高斯变量中预测动态kernel用于合成可信的下一帧。</p><p>Esquivel等人的预测自适应kernel用于减少在有限的计算资源下准确分类图像所需的层数。</p><p>在本文中作者探讨了一个类似的想法，即<strong>利用测试时的Kernel prediction来改进生成模型中的风格迁移和基于风格的调制</strong>。</p><h2 id="Feature-Modulation-with-AdaConv">Feature Modulation with AdaConv</h2><p>这里先描述AdaConv和Kernel prediction，展示AdaConv如何泛化以及扩展特征调制中的1×1仿射变换。</p><p>首先在风格迁移的背景下画一个与AdaIN平行的例子，然后展示AdaConv如何更好地调节局部特征结构，更好地迁移空间风格，同时该方法也适用于风格迁移之外的高质量生成模型。</p><h3 id="Overview">Overview</h3><p>考虑通常的style表示法${a,b}\in R^2$，其中$a$和$b$分别表示风格为尺度和偏差项(例如，对于风格迁移，$a$和$b$是风格图像特征的平均值和标准差)。</p><p>给定一个值为$x\in R$的输入特征通道和所需的style，AdaIN将style定义的仿射变换应用于标准化的输入特征，</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210620/2.png" alt=""></p><p>其中，$\mu _x$和$\sigma_x$为特征通道上的均值和标准差。</p><p>因此，AdaIN只改变每个通道基于条件设置样式参数$(a,b)$的全局统计。注意，无论每个样本$x$周围的特征值的空间分布(结构)如何，整个通道都是相等调制的。</p><p>因此，作者扩展AdaIN的<strong>第1步</strong>是引入一个条件2D style filter $f \in R^{k_h×k_w}$，取代scale term和产生扩展的风格参数${f,b}$。该filter允许根据样本$x$周围邻域$N(x)$的局部结构以空间变化的方式调制特征通道:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210620/3.png" alt=""></p><p>注意，这个depthwise AdaConv变种包含AdaIN，这是1个特殊情况1×1 filter f和$N(x)={x}$</p><p>完整AdaConv调制的<strong>第2步</strong>是通过扩展输入style参数，也包括一个separable-pointwise卷积$p\in R^C$，该卷积用于C特征通道的输入，来扩展这个深度变体。这使得AdaConv可以基于一种风格进行调制，这种风格不仅可以捕获全局统计数据和空间结构，还可以捕获不同输入通道中特征$x_c$之间的关联。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210620/4.png" alt=""></p><p>AdaConv的输入风格${p,f,b}$有效地包含了一个深度可分离的3D卷积核，具有深度和逐点卷积分量，以及每个通道的偏差。</p><p>用于调制输入的深度和逐点卷积核的实际数量是一种设计选择，可以任意大，这可以通过使用深度可分离卷积层中的$n_g$组的数量来控制。</p><p>接下来，作者还提出了AdaConv的kernel prediction框架，并展示了它如何作为AdaIN的一般替代来实现更全面的基于风格的条件转换，也在其他高质量的生成模型。</p><h3 id="Style-Transfer-with-AdaConv">Style Transfer with AdaConv</h3><p>下图给出了风格迁移架构的概述。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210620/5.png" alt=""></p><p>输入风格和内容图像编码使用pre-trained VGG-19编码器获取潜在的风格特征S和内容C。</p><p>对于kernel prediction来说，风格特征编码进一步通过风格编码器ES获得全局风格描述符W；对于W kernel prediction网络$K={K_1、,K_2,…,K_N}$输出具有每通道偏差的深度可分卷积核。这些预测被输入到解码器D的所有层中来输出风格迁移的结果。</p><p>本文的风格迁移架构使用了4个kernel prediction，它们用于解码图像的4种不同分辨率，每个kernel具有不同的维度。</p><p>每个解码层都有一个自适应卷积块(下图)，其中预测的深度卷积和逐点卷积先于标准卷积。这些标准卷积层负责学习与风格无关的kernel，这些kernel对于重建自然图像很有用，并且在测试时保持固定。在VGG-19潜在特征空间内，联合训练编码器ES、kernel prediction K和解码器D。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210620/6.png" alt=""></p><h4 id="风格编码">风格编码</h4><p>现在转向从风格特征S预测卷积核的目标，用于图像解码器的每个尺度上的内容特征C。</p><p>在这里，一个中间步骤是计算一个综合描述不同尺度的风格图像的风格表示W，同时以风格传递损失为指导。这种设计选择也是通过与最先进的生成建模的类比而产生的，其中术语“style”表示图像的全局和局部属性。</p><p>预训练的VGG-19网络将尺寸为(3,256,256)的原始输入修改为尺寸为(512,32,32)的VGG-19 $relu4_1$层样式张量S。这里，感受野并没有覆盖整个风格图像。因此,需要通过训练一个额外的编码器组件ES，将S减少到全局嵌入W中，如图3所示。</p><p>这里的风格编码器ES包括3个初始块，每个块具有3×3卷积、一个平均池化操作和一个Leaky ReLU激活。</p><p>然后，ES的输出被Reshape并输入到最后一个完全连接的层，该层提供全局风格描述符，该层反过来又被Reshape为大小为W的输出张量$(s_d,s_h,s_W)$。这种嵌入的尺寸是超参数定义为要预测的kernel大小的一个因素。</p><p>由于使用了这个完全连接层，网络只能处理固定尺寸(3,256,256)的输入风格的图像。然而，内容图像的尺寸不受限制，因为它流经网络的一个全卷积的组件。</p><h4 id="预测深度可分离卷积">预测深度可分离卷积</h4><p>图2中的每个kernel predictor K都是一个简单的卷积网络，它的输入是风格描述符W，而输出是一个深度可分离的kernel。</p><p>选择预测深度可分离的kernel的动机是希望保持kernel predictor的简单和计算效率，同时也使随后的卷积更快。</p><p>标准卷积层取一个维数为1的输入特征张量$(1,c_{in},h,w)$，并将其与一个大小为$(c_{out}, c_{in}, k_h, k_w)$的kernel张量进行卷积，其中$c_{in}$和$c_{out}$是输入和输出通道的数量。每通道偏置也被添加到输出。因此，该层所需的权重数为:$c_{out}\times c_{in}\times k_h\times k_w+ c_{out}$。</p><p>深度可分离卷积通过将输入通道聚集到$n_g$个独立的组中，并通过应用独立的spatial和pointwise kernel(分别学习结构和交叉固定空间卷积适应通道相关)来减少这个数量。所需重量减少为$c_{out}\times \frac{c_{in}}{n_g} \times k_h\times k_w+ c_{out}$。对于带有$n_g=c_{in}$的卷积层，输入的每个通道都与自己的$c_{out}/c_{in}$卷积核进行卷积。</p><p>接下来是对1×1卷积核的逐点卷积，以扩展输出中的通道数，并在最终输出中添加每通道的偏置。</p><p>这里，需要注意的是，解码器中的4个AdaConv层的$c_{in}$随着空间分辨率的增加而减少,分别为512、256、128和64。</p><p>因此，最低空间分辨率的kernel predictor通常具有最高的参数数。为了将网络容量均匀分布在连续的分辨率层上，作者在较低的分辨率上设置了较大的$n_g$，并在连续的层上逐渐降低$n_g$，从而得到更好的结果。对于深度卷积核和depthwise卷积核，$n_g$的设置是相同的。</p><p>因此，每个kernel predictor K在该解码器内为深度卷积AdaConv层输出必要的权值。这些权重包括:</p><ol><li>spatial kernel的size $(c_{out},\frac{c_{in}}{n_g},k_h,k_w)$;</li><li>pointwise kernel的size $(c_{out},\frac{c_{out}}{n_g},1,1)$</li><li>bias项$b\in R^{out}$。</li></ol><p>每个kernel predictor K的输入是大小为$(s_d,s_h,s_w)$的全局风格描述符W，它通过卷积和池化层得到，这些层输出目标维度的spatial kernel，如图3所示。</p><p>这些层可能由标准卷积或转置卷积组成，其参数在设计时确定，并取决于要预测的kernel的大小。</p><p>为了预测pointwise 1×1 kernels，作者将W集合到一个大小$(s_d,1,1)$，然后执行一维卷积来预测pointwise的$c_{out}$核。</p><p>作者对每个通道的偏差使用一个单独的预测器，类似于pointwise kernels的预测器。一旦kernel和偏差被预测，它们被用来调制如图3右半部分所示的输入。</p><h2 id="实验">实验</h2><h3 id="风格迁移">风格迁移</h3><p>对比实验如下：</p><p>与AdaIN的对比如下，可以看出有明显的改善：</p><h3 id="生成模型的扩展">生成模型的扩展</h3><p>基于StarGAN-v2的改进如下：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210620/7.png" alt=""></p><p>实验结果如下：<br><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210620/8.png" alt=""></p><h2 id="参考">参考</h2><p>[1].Adaptive Convolutions for Structure-Aware Style Transfer<br></p>]]></content>
      
      
      <categories>
          
          <category> CVPR2021 GAN解读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AdaConv自适应卷积 </tag>
            
            <tag> GAN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>详细解读：HP-x激活函数</title>
      <link href="/2021/06/18/14/"/>
      <url>/2021/06/18/14/</url>
      
        <content type="html"><![CDATA[<p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210618/1.png" alt=""></p><h2 id="简介">简介</h2><p>本文提出了orthogonal-Padé激活函数，它是可以训练的激活函数，在标准深度学习数据集和模型中具有更快的学习能力，同时可以提高模型的准确率。根据实验，在六种orthogonal-Padé激活中找到了2种最佳的候选函数，作者称之为 safe Hermite-Pade(HP)激活函数，即HP-1和HP-2。</p><p>与ReLU相比,HP-1和HP-2帮助PreActResNet-34带来不同程度的提升(top-1精度提升分别为5.06%和4.63%),在CIFAR100数据集上MobileNet V2模型提升分别为3.02%和2.75%分别，在CIFAR10数据集上PreActResNet-34的top-1精度分别增加了2.02%和1.78%,LeNet的top-1精度分别提升为2.24%和2.06%,Efficientnet B0的top-1精度分别提升为2.15%和2.03%。</p><h2 id="前人工作简介">前人工作简介</h2><p>深度卷积神经网络由多个隐藏层和神经元构成。然后通过每个神经元的激活函数引入非线性。</p><p>ReLU由于其简单性，是深度学习中最受欢迎的激活函数。虽然ReLU有一个缺点叫做 dying ReLU，在这种情况下，多达50%的神经元可能会因为消失梯度问题，即有大量的神经元对网络性能没有影响。为了克服这一问题，后来又提出了Leaky Relu、Parametric Relu、ELU、Softplus，虽然找到最佳的激活函数仍是一个有待研究的问题，但这些方法都提高了网络的性能。最近，研究人员使用了自动搜索技术发现了Swish激活函数。与ReLU相比，Swish的精确度有了一些提高。GELU、Mish、TanhSoft、EIS是目前少数几个可以替代ReLU和Swish的候选激活函数。</p><p>近年来，人们对可训练激活函数的研究也越来越感兴趣。可训练激活函数具有可学习的超参数(s)，在训练过程中通过反向传播算法更新。本文提出了Orthogonal-Padé激活函数。Orthogonal-Padé函数可以近似大多数连续函数。</p><h2 id="Pade-activation-Unit-PAU-and-Orthogonal-PAU">Padé activation Unit (PAU) and Orthogonal-PAU</h2><p>考虑实线的一个闭合间隔为[a,b]。设$P_n(x)$是$x$中次数小于等于$n$的所有多项式的空间。对于一个非负连续函数$w(x)$,在[a, b]上定义Pn(x)上的内积为:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210618/2.png" alt=""></p><p>有多项式${P_1(x);P_2(x);···;P_k(x)}$是正交的，如果：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210618/3.png" alt=""></p><p>$P_n(x)$的一组基是由$P_n(x)$张成的n个多项式的集合。一组正交基也是一组正交集。</p><p>$P_n(x)$的标准基是${1;x, x^2;···;x^n}$。但是标准基与式1中定义的内积并不是正交的。</p><p>在许多应用中，使用正交基可以简化表达式并减少计算。多项式空间有几个众所周知的正交基。下表列出了其中一些多项式基。注意，它们有的由递归关系给出，有的由直接表达式给出。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210618/4.png" alt="表1 一些著名的正交多项式基"></p><h3 id="Pade-activation-Unit-PAU">Padé activation Unit (PAU)</h3><p>f(x)由有理函数F1(x)的Padé近似定义为：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210618/5.png" alt=""></p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210618/6.png" alt=""></p><p>其中P(x)和Q(x)分别是k次和l次的多项式，它们没有公因式。PAU是式(3)的可学习激活函数，其中多项式系数$a_i;b_j;0≤i≤k;1≤j≤l$为可学习参数，在反向传播过程中进行更新。为了将F1(x)的极点从Q(x)的0中移除，有学者提出了safe PAU。safe PAU定义为：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210618/7.png" alt=""></p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210618/8.png" alt=""></p><p>在分母中引入绝对值可以确保分母不会消失。实际上，也可以取和的绝对值来定义：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210618/9.png" alt=""></p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210618/10.png" alt=""></p><p>在许多任务中，F3定义的激活函数比F2定义的safe PAU能够提供更好的结果。</p><h3 id="Orthogonal-Pade-activation-Unit-OPAU">Orthogonal-Padé activation Unit (OPAU)</h3><p>g(x)由有理函数G(x)的orthogonal-Padé近似定义为：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210618/11.png" alt=""></p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210618/12.png" alt=""></p><p>其中$f_t(x)$属于正交多项式集合。与PAU一样，可学习激活函数OPAU由(6)定义，其中$c_i;d_j;0≤i≤k;1≤j≤l$为可学习参数。参数的初始化是通过近似的形式的如ReLU, Leaky ReLU等。为了去掉G(x)的极点，提出如下的safe OPAU。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210618/13.png" alt=""></p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210618/14.png" alt=""></p><p>作者考虑了6种正交多项式基- Chebyshev(两种)，Hermite(两种)，Laguerre和Legendre多项式的基。关于这些多项式基的详细信息见表1。</p><h3 id="通过反向传播学习激活参数">通过反向传播学习激活参数</h3><p>利用反向传播算法和梯度更新神经网络模型中的权值和偏差。这里也采用相同的方法更新激活函数的参数。作者已经在Pytorch和Tensorflow-Keras API实现了自动化更新参数。对输入x和参数$c_i’s、d_j’s$计算公式(6)的梯度如下:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210618/15.png" alt=""></p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210618/16.png" alt=""></p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210618/17.png" alt=""></p><h2 id="具有orthogonal-Pade激活以及函数近似的网络">具有orthogonal-Padé激活以及函数近似的网络</h2><p>Orthogonal-Padé网络类似于Padé网络，即将具有PAU或safe PAU的网络替换为OPAU或safe OPAU。在本文中，将safe OPAUs视为不同正交基的激活函数，如表1所示。用(7)中给出的函数形式近似Leaky ReLU对可学习参数(多项式系数)进行初始化，初始化参数值如下表所示。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210618/18.png" alt=""></p><p>利用反向传播方法对网络参数进行了优化。作者对所有的网络都保持了PAU的类似设计，例如每层的权重共享和可学习激活参数。由式(5)可知，每层总共有(k+l)个额外参数。因此，如果网络中有L层，网络中就会有额外的L(k+L)个可学习参数。为了训练网络，作者采用了Leaky ReLU初始化(α=0.01)，而不是随机初始化方法。</p><p>使用正交基的一个主要优点是，与标准基相比，可以在运行时间上更快地找到多项式系数。此外，目前广泛使用的激活函数在大多数情况下是零中心的。因此作者在Padé和Orthogonal-Padé近似上施加一些条件，以使已知函数近似为零中心，并检查是否有任何对模型性能的优势(一个明显的优势是每一层的参数量减少了)。</p><p>为了使Padé以零为中心，将式(4)中的$a_0=0$替换，并计算其他参数。为了保证OPAU的safe，会有几个bad case，作者研究了所有可能的bad case。</p><p>例如，如果选择HP-1作为基，如果分子中的常数项为零，则安全的OPAU函数近似可以以零为中心。由式(6)和表1可知，$c_0-c_2+3c_4=0$。可以推导出以下情况:</p><p>case 1:<br>$$c_0=c_2=c_4=0$$</p><p>case 2:</p><p>$c_0, c_2, c_4$其中一个等于0。例如，如果$c_0 = 0$，那么$c_2 = 3c_4$等等;</p><p>case 3:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210618/19.png" alt=""></p><p>在上述PAU和HP-1的所有情况下，作者已经在CIFAR10和CIFAR100数据集上对几个经典的模型进行了实验和测试（Leaky ReLU近似）。作者发现在大多数情况下，模型在top-1准确率下降了0.2%-0.6%。</p><p>此外，需要注意的是，具有safe OPAU激活函数的神经网络在C(K)中是dense的，其中K是$R_n$的一个紧凑子集，而C(K)是K上所有连续函数的空间。</p><h3 id="Proposition">Proposition</h3><ul><li><p>设$\rho : R\to R$是任意连续函数。设$N_n^\rho$表示一类具有激活函数$\rho$的神经网络，输入层有n个神经元，输出层有1个神经元，隐层有任意数量的神经元。设$K\subseteq R_n$是compact的。当且仅当$\rho$是非多项式时，$N_n^\rho$在C(K)中是dense的。</p></li><li><p>设$\rho : R\to R$是任意连续函数，它至少在一点上是连续可微的，且在这一点上导数为非零。设$K\subseteq R_n$是compact的。那么在$C(K;R^m)$中，$NN^\rho_{n;m;n+m+2}$是dense的。</p></li></ul><h2 id="实验">实验</h2><h3 id="CIFAR-100">CIFAR-100</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210618/20.png" alt=""></p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210618/21.png" alt=""></p><h3 id="Tiny-Imagenet">Tiny Imagenet</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210618/22.png" alt=""></p><h3 id="VOC-2007">VOC 2007</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210618/23.png" alt=""></p><h2 id="参考">参考</h2><p>[1].ORTHOGONAL-PADÉ ACTIVATION FUNCTIONS: TRAINABLE ACTIVATION FUNCTIONS FOR SMOOTH AND FASTER CONVERGENCE IN DEEP NETWORKS<br></p>]]></content>
      
      
      
        <tags>
            
            <tag> 全新激活函数 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>详细解读Attention-Based方法解决遮挡人脸识别问题</title>
      <link href="/2021/06/16/13/"/>
      <url>/2021/06/16/13/</url>
      
        <content type="html"><![CDATA[<p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210616/0.png" alt=""></p><h2 id="简介">简介</h2><p>在非约束性环境(如大量人群)中捕获的人脸照片，仍然对当前的人脸识别方法构成挑战，因为人脸经常被前景中的物体或人遮挡。然而，很少有研究涉及到识别部分面孔的任务。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210616/1.png" alt=""></p><p>本文提出了一种新的遮挡人脸识别方法，能够识别不同遮挡区域的人脸。通过将一个ResNet中间特征映射的attentional pooling与一个单独的聚合模块相结合来实现这一点。为了保证attention map的多样性，并处理被遮挡的部分，作者进一步对遮挡Face的常见损失函数进行了调整。实验表明，在多个benchmark下本文方法的性能优于所有baseline。</p><p>本文工作贡献可以概括为以下几点:</p><ul><li><p>以ResNet为例，利用attentional pooling和聚合网络提出了一种新的扩展，并使用2种适用于部分FR的常见损失函数进行训练；</p></li><li><p>在多个局部FR的详尽分析中表明，本文的改进大大提高了识别性能。</p></li></ul><h2 id="方法">方法</h2><h3 id="Network-Architecture">Network Architecture</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210616/2.png" alt=""></p><p>下图描述了partial FR方法，分为3个模块:Extract、Attend和Aggregate。</p><p>Extract模块从输入图像中提取特征图$F\in R^{20×20×1024}$和attention maps  $A\in R^{20×20×K}$，其中K表示attention maps的个数。</p><p>在Attend模块中，使用重新校准的attention maps将特征图合并为K个中间特征向量。</p><p>Aggregate模块将这些中间特征向量映射到联合特征空间中，得到最终特征向量$f\in R^{256}。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210616/3.png" alt=""></p><h4 id="Extract">Extract</h4><p>受Comparator networks启发，作者使用了一个删减的ResNet-50架构，它在第4个block之后结束。因此，只进行了3次空间降采样，得到了大小为20×20的特征图，其中区域仍然具有很好的可区分性。与Comparator networks不同的是，在第3个block之后分离ResNet，以允许2个分支专注于各自的任务。而在第4个block之后直接得到F，然后再加上一个1×1的卷积以及ReLU激活函数获取a。具体架构总结如表1所示。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210616/4.png" alt=""></p><p>生成的attention maps应满足以下2个关键属性:</p><ol><li>attention maps应是互斥的，即不同的attention maps聚焦于人脸图像的不同区域;</li><li>attention maps的激活与区域的可见性相关。</li></ol><p>值得注意的是，implicitly-defined attention maps激活并不一定遵循人类定义的面部标志(如眼睛或鼻子)的直觉。</p><h4 id="Attend">Attend</h4><p>和Comparator networks一样，attention maps A需要重新校准。Xie等人提出了基于集的FR归一化A的attentional pooling方法，对集合内的所有图像分别进行归一化，从而确保从A中激活程度最大的图像中提取出各自的信息。</p><p>本文作者只考虑一个单一的图像，并期望不同的attention maps是相关的，因为这些主要取决于脸部的区域，即，如果眼睛被遮挡，相应的attention maps应该包含低激活值。因此，建议使用无参数的重新标定：</p><p>首先，用sigmoid函数$f_{norm(·)}= sigmoid(·)$对A进行normalize。这样，每个attention maps的每个像素分别归一化为(0,1);此外，先使用Global Average Pooling (GAP)，然后使用$f_{ex(·)}= softmax(·)$，计算一个向量$s\in R^K}表示每个attention maps的重要性:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210616/5.png" alt=""></p><p>索引$i,j,k$表示第$k$个attention maps的第$i$行和第$j$列的像素。通过引入GAP获得了所有attention maps的全局信息，并利用softmax函数将其转化为指示各attention maps重要性的概率分布。接下来，将第$k$个自归一化的attention maps $A_k$与其相应的重要性$s_k$相乘，得到最终的重新校准的attention maps $A$。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210616/6.png" alt=""></p><p>因此，在重新校准中将每个attention maps中的局部信息与跨attention maps的全局信息结合在一起。</p><p>重新校准后，应用attentional pooling，得到K个特征描述子$v_k \in R^{1024}$:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210616/7.png" alt=""></p><p>这样，第$k$个特征描述符中就包含了对应attention maps $A_k$激活时$F$的信息。</p><h4 id="Aggregate">Aggregate</h4><p>用Aggregate模块来总结partial FR模型。由于所有的特征描述符$v_k$依赖于它们对应的attention maps $A_k$聚焦于$F$内的不同区域，所以不可能进行直接聚合。因此，将每个$v_k$分别映射到一个联合特征空间$f_k\in R^{256}$，每个$v_k$使用一个单独的全连接层。</p><p>注意，由于每个$v_k$都在不同的特征空间中，所以权重不是共享的。由于$f_k$同样对身份信息进行编码，所以通过计算平均值得到最终的特征向量$f\in R^{256}$:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210616/8.png" alt=""></p><h3 id="Loss-Functions">Loss Functions</h3><p>为了训练模型，作者使用3个损失的加权和，其描述如下:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210616/9.png" alt=""></p><p>用$\lambda_{wCE}$、$\lambda_{wDIV}$和$\lambda_{REG}$表示超参数来平衡损失，$L_{REG}$为所有可训练权重的$L_2$范数。</p><h4 id="Weighted-Cross-Entropy-L-wCE">Weighted Cross-Entropy $L_{wCE}$</h4><p>为了处理一些代表被遮挡区域的向量，从而降低相关性，作者提出了一种加权的softmax CrossEntropy(CE)。对于CE损失添加一个全连接层到每个特征向量$f_k$匹配训练数据集中类的数量。通过这种方法得到了K CE损失$L_{CE,K}$。为了得到最终加权CE损失，对每个$L_{CE,K}$及其重要性$s_k$进行了scale:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210616/10.png" alt=""></p><p>通过这种方式，该网络学习强调代表可见人脸区域的attention maps，同时减轻代表遮挡区域的attention maps的影响。需要注意的是，由于最后一个全连接层的权值是共享的，所以每个$f_k$的转换是相等的，因此，要保证它们同样编码身份信息，即位于相同的特征空间。此外，由于训练数据集中有大量的类，$f_k$作为瓶颈层提高了网络的泛化能力。</p><h4 id="Weighted-Diversity-Regularizer-L-wDIV">Weighted Diversity Regularizer $L_{wDIV}$</h4><p>多样性正则化的目的是确保attention maps的多样性，因为如果不进行正则化，网络容易倾向于只使用一个attention maps或生成K个相同的attention maps。因此作者使用多样性正则化算法来惩罚不同attention maps之间的相互重叠。首先，使用softmax函数将每个attention maps $A_k$自归一化为概率分布$P_k$:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210616/11.png" alt=""></p><p>接下来，计算所有$P_k$的像素级最大值，并得到所有像素的和。对于互不重叠的attention maps，这个和接近于1，可以计算加权多样性损失$L_{wDIV}$如下:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210616/12.png" alt=""></p><h2 id="实验">实验</h2><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210616/13.png" alt=""></p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210616/14.png" alt=""></p><p>表2描述了LFW数据集上不同benchmark protocols的聚合精度。当考虑一个ResNet-50(没有微调)，它在训练期间从未暴露于部分脸，可以观察到标准FR模型非常容易受到partial faces的影响。通过对partial faces进行微调，该模型在partial protocols上表现得更好。ResNet-50在非non-centered protocols上的性能优于ResNet-41，但在centered protocols上的性能较差。作者认为这是由于ResNet-50包含更多可训练参数。因此，由于中心不是数据扩充的一部分，它更容易对训练过程中呈现的空间信息进行过拟合。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210616/15.png" alt=""></p><p>在图中，中心部分面非遮挡区域a的影响:partial - cross protocol。虽然识别左眼-右眼的准确性只受到a的轻微影响，但验证左眼-嘴是否属于同一身份被认为是最具挑战性的。总的来说可以得出结论，本文模型比所有centered: partial-cross的baseline更稳健。</p><h2 id="参考">参考</h2><p>[1].ATTENTION-BASED PARTIAL FACE RECOGNITION<br>[2].<a href="https://github.com/stefhoer/PartialLFW">https://github.com/stefhoer/PartialLFW</a><br></p>]]></content>
      
      
      
        <tags>
            
            <tag> Attention-Based方法 </tag>
            
            <tag> 遮挡人脸识别问题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>详细解读：如何让EfficientNet更加高效、速度更快</title>
      <link href="/2021/06/12/12/"/>
      <url>/2021/06/12/12/</url>
      
        <content type="html"><![CDATA[<p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210612/1.png" alt=""></p><h2 id="简介">简介</h2><p>近年来，许多研究致力于提高图像分类训练和推理的效率。这种研究通常集中于提高理论效率，通常以每个FLOP的ImageNet验证精度来衡量。然而，事实证明，这些理论上的改进在实践中很难实现，特别是在高性能训练加速器上。</p><p>在这项工作中，作者关注的是在一个新的加速器类<strong>Graphcore IPU</strong>上提高最先进的EfficientNet模型的实际效率。本文主要通过以下方式扩展这类模型:</p><ul><li>将Depthwise CNN推广为Group CNN;</li><li>添加proxy-normalized激活，以使batch normalization性能与batch-independent statistics相匹配;</li><li>通过降低训练分辨率和低成本的微调来减少计算量。</li></ul><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210612/2.png" alt=""></p><p>作者发现这3种方法都提高了训练和推理的实际效率。</p><h2 id="研究背景">研究背景</h2><h3 id="Efficient-CNNs分析">Efficient CNNs分析</h3><p>在CNN的发展过程中，实际训练效率的提高是创新的重要力量。比如说，AlexNet的成功很大一部分因素便得益于GPU加速，ResNet的成功不仅可以归因于其良好的性能，而且其在GPU上的高吞吐量也相对比较高。</p><p>最近，在理论效率方面也取得了重大改进。最引人注目的创新是在空间操作中引入Group卷积和Depthwise卷积。ResNet-50单独引入Group卷积可以提高理论效率。类似地，通过将Group规模减少到1，即利用Depthwise卷积，相对于原始的CNN模型也实现了理论效率的提高。特别是，该方法为实现基于“mobile”级别的落地应用提供了可能。</p><p>通过使用NAS直接减少FLOPs进一步提高了这些理论上的效率增益。这带来了整个模型尺度范围内的效率提高，从mobile-sized的模型如MobileNetV3和MNasNet 到大型模型如NASNet和AmoebaNet。</p><p>值得注意的是，在ImageNet模型的最高精度前100名的所有NAS模型都使用了某种形式的Group卷积或Depthwise卷积，进一步突出了这些操作相对于CNN操作的优势，在高效的MNasNet基础上，EfficientNet进一步改进了训练方法并扩展到更大的模型，以在FLOP范围内实现SOTA性能。</p><p>虽然低功耗cpu的高效模型通常能实现实际改进，但这些模型通常难以将理论收益转化为高性能硬件上更高的训练吞吐量。例如，虽然EfficientNets在理论训练效率方面远远优于ResNets，但当考虑到GPU上的实际训练效率时经常被发现表现不佳。最近的一些工作也已经开始使用NAS来优化GPU的实际效率。</p><h3 id="硬件角度考虑与分析">硬件角度考虑与分析</h3><p>在研究模型的实际效率时，了解它所运行的硬件的特征是很重要的。关于这个问题的讨论通常主要集中在峰值计算速率上，以每秒浮点运算(FLOPS)衡量，这是计算操作的理论最大速率。虽然峰值率是需要考虑的一个重要因素，但了解实现峰值率所需的假设也同样重要，例如，计算的结构和数据的可用性。</p><p>计算结构很重要，因为现代硬件通常使用向量指令，允许用一条指令计算给定长度的点积。然而，如果计算的体系结构不能使这些向量指令被填满，那么FLOPs就可能被浪费掉。此外，如果数据不能立即在计算引擎上获得，那么将需要循环来移动它。这种操作将高度依赖于内存的带宽或者位宽。</p><p>对内存带宽的依赖依赖于模型，可以通过计算与数据传输的比率来表征，即算术<strong>arithmetic intensity</strong>——在这种情况下，低<strong>arithmetic intensity</strong>强度的操作更依赖于内存带宽。对于一个简单的Group卷积，<strong>arithmetic intensity</strong>强度随着Group大小、Kernel大小、field大小和Batch大小单调地增加。值得注意的是，这意味着Group卷积和Depthwise卷积在Group较小时的效率更可能受到可用内存带宽的限制。</p><p>在这项工作中，作者使用了一种新的硬件加速器<strong>Graphcore IPU</strong>。这种加速器与通常用于神经网络训练的GPU有很大的区别。IPU计算在芯片上分布在1472个核心中，尽管它的指令仍然是向量化的，但要充分利用计算引擎，只需要16项的点积即可。这有助于减少对计算结构的依赖。此外，IPU拥有超过900MB的高带宽片上内存，远远超过其他硬件。这大大降低了低<strong>arithmetic intensity</strong>强度操作的代价。</p><p>为了最大化IPU上的性能，保持尽可能多的工作内存(例如芯片上的激活状态)变得非常重要。这自然促进了更小批次的使用、内存节约优化和分布式处理的创新形式。同时，它确实需要重新考虑使用BN，因为在视觉模型中，最常见的归一化方法它很依赖于大的Batchsize。</p><h2 id="本文方法">本文方法</h2><h3 id="改用Group卷积">改用Group卷积</h3><p>NAS方法倾向于将它们的spatial卷积分组，通常分组大小为G=1(Depthwise卷积)。而Depthwise卷积具有很低的FLOP和参数，使用G&gt;1作为一个更大的Group将更有效地利用现代硬件加速器:</p><ul><li>(i) 增加<strong>arithmetic intensity</strong>强度;</li><li>(ii) 增加点积的长度(用于卷积)，允许使用更大的向量指令。</li></ul><p>作者的目的是研究在EfficientNet模型中增加spatial卷积的Group大小所涉及的权衡问题。单单增加G就会增加参数量和FLOPs。因此，为了保持相似的模型复杂度，作者相应地降低了扩展比（扩展比定义为输入到first pointwise CNN和spatial CNN之间的通道比）。这类似于ResNeXt的Flop等效扩展。</p><p>因此，<strong>对于相同的FLOP具有更大G的网络将更窄，更窄的网络模型将通过减少存储激活状态的大小和使用更大的BatchSize而获得计算优势</strong>。请注意，虽然这种补偿的目的是保持总FLOPs和参数量，但为简单起见，作者只在全局级别更改扩展比率。因此，并不需要保持与深度完全相同的参数和FLOPs分布。</p><p>与EfficientNet一样，其他NAS派生的架构通常只使用depthwise卷积，这表明depthwise卷积在验证准确性方面是最优的。在ResNeXts中，在保持总FLOPs的同时增加G会导致验证准确性下降。这也表明与类似G&gt;1的网络对比G=1的vanilla EfficientNet将实现更高的准确度。 然而，作者希望改进的网络提供更好的性能和训练时间之间的权衡。因此对EfficientNet B0和B2的Group规模在G=1和G=64之间进行了测试。</p><h3 id="Batch-Independent-Normalization">Batch-Independent Normalization</h3><h4 id="BN的问题在哪？">BN的问题在哪？</h4><p>我们都知道BN通常应用于没有归一化的pre-activations X进而产生归一化pre-activations Y，然后再进行仿射变换和非线性$\phi$，最终产生post-activations Z。形式上，每个通道c:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210612/3.png" alt=""></p><p>式中BN的归一化确保了Y被规范化，这意味着每个通道c的均值和单位方差都为零，因此BN对于将模型扩展到大型和深度模型是成为了可能:</p><ul><li><p>通过确保非线性$\phi$在每个通道中“sees”接近归一化的数据分布，$\phi$可以有效地形成非线性的分布。因此，额外的层可以增加表达能力，网络可以有效地利用其整个深度。这与$\phi$会“see”一个“collapsed”的数据分布的情况相反，这样它会在一阶上很好地近似于一个关于这个分布的线性函数;</p></li><li><p>通过保证不同通道的方差接近相等，网络可以有效地利用其整个带宽。这与一种情况相反，在这种情况下，一个单一的通道会任意支配其他渠通道，从而成为唯一的通道被后续层“seen”。</p></li></ul><p>尽管这一基本原则取得了实际的成功应用，但BN对小batchsize数据的依赖有时会产生问题。最值得注意的是，当batchsize较小或数据集较大时，来自小batchsize统计数据$(\mu_c \sigma_c)$中噪声的正则化可能会过大或不必要，从而导致性能下降。</p><h4 id="突破点在哪？">突破点在哪？</h4><p>为了解决这些问题，研究者们也提出了各种Batch-Independent相关的归一化技术:层归一化(LN)、组归一化(GN)、实例归一化(IN)、权重归一化(WN)、权重标准化(WS)、在线归一化(ON)、滤波器响应归一化(FRN)、EvoNorm等。虽然这些技术在其他环境中很有用，但在本工作中，没有一种技术能够缩小与大 Batch BN的性能差距，重点关注在ImageNet上使用RMSProp训练的EfficientNets。</p><p>这也促使作者重新思考如何执行独立于batch的Norm，并在工作中提出Proxy Normalized Activations。在本研究中，作者提出了一个假设，即除了提高对小batch的依赖外，与batch无关的归一化还应保持每个通道中归一化预激活Y的BN原则。</p><p>这一假设的第1个理由是BN的归纳偏差。第2个理由是，在更实际的层面上，BN被用于架构搜索，比如产生了EfficientNet模型系列的搜索。因此，坚持相同的标准化原则可以避免重新执行这些搜索。</p><p>为了保留BN原则，同时消除对Batchsize的依赖，作者扩展的工作如下:</p><ul><li><p>(i)将Eq.(1)的BN步骤替换为基于LN或GN的Batch无关的标准化步骤;</p></li><li><p>(ii)将式(2)的激活步骤替换为<em>proxy-normalized activation</em>步骤。</p></li></ul><p>这一步通过将$\phi(\gamma Y_{…c} + \beta_c)$与$\phi(\gamma \tilde Y_{c} + \beta_c)$同化，使$\phi(\gamma Y_{…c} + \beta_c)$归一化，其中$\tilde Y_{c} \sim N(\tilde \beta_c, (1+\tilde \gamma_c)^2)$是一个高斯proxy变量，具有均值$\tilde \beta_c$和方差$(1+\tilde \gamma_c)^2$，如果选择LN作为Batch无关的归一化，对于每个batch元素b和通道c，这表示为:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210612/4.png" alt=""></p><p>其中，$\tilde Y_{c} \sim N(\tilde \beta_c, (1+\tilde \gamma_c)^2)$，$\mu_b, \sigma_b$是X的batch元素b在空间和通道维度上的均值和标准差。</p><p>当与LN结合时，这种激活的proxy标准化(PN)迭代确保预激活Y保持接近于标准化（论文中有推导）。</p><h3 id="Image分辨率">Image分辨率</h3><p>引入全局平均池化允许CNN对任意分辨率的输入进行操作。虽然这已经在图像分割等任务中得到了探索，但在图像分类中，其影响仍有待更加深入的挖掘。EfficientNet模型将图像分辨率作为一个可调的超参数，使用更大的图像来训练更大的网络。Hoffer等人同时对多个图像尺寸的网络进行训练发现：</p><ul><li>i) 大分辨率可以加速训练以达到目标精度</li><li>ii) 大分辨率可以提高相同训练的最终性能。</li></ul><p>或许与目标最接近的是，<strong>Howard建议从低分辨率图像开始训练，在训练过程中逐步增加图像的大小，以减少总的训练时间</strong>。</p><p>Touvron等人研究表明，少量的微调可以使网络收敛的更好。微调步骤只需要对网络的最后部分起作用，而且只需要几个epoch就可以提高总体精度。因此，与其他训练相比，微调的计算成本几乎可以忽略不计。</p><p>从这一研究中获得了灵感，研究了在低分辨率图像上训练的网络的微调，并从效率的角度将其推广到更大的分辨率。在训练过程中使用较小的图像可以使用更少的内存更快地训练出一个给定的模型，或者在相同的时间内训练一个较大的模型。为了测试这一想法，作者在固有的EfficientNet图像大小以大约为原来像素数的一半进行训练，这里表示为半分辨率。结果与EfficientNet模型的FLOPs大致相当。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210612/5.png" alt=""></p><p>然后，微调和测试的图像尺寸范围高达700x700。在选择用于验证的精确分辨率时，注意到性能可能会受到混叠效应的影响。这种人工干扰是由于非对称下采样层的位置造成的，其中输入的维度是奇数，这取决于输入分辨率在不同的深度上决定的。作者还发现在训练和测试之间保持这些降采样层的位置一致是很重要的。这可以通过选择测试分辨率$r_{test}$来实现，使$r_{train}≡r_{test}(mod 2^n)$，其中n是模型中的下采样层数(对于EfficientNet, n=5)。</p><h2 id="实验">实验</h2><h3 id="Group卷积的影响">Group卷积的影响</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210612/6.png" alt=""></p><p>通过上表可以看出虽然组大小为G=4的情况下在这些测试中获得了最好的准确性，但发现组大小为G=16的增加的计算效益在实践中产生了比较好的权衡。</p><h3 id="Proxy-Normalized-Activations的影响">Proxy-Normalized Activations的影响</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210612/7.png" alt=""></p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210612/8.png" alt=""></p><p>从表中可以看出，对于B0和B2，在G=16上直接比较2种方法时，LN+PN得到的准确率与BN得到的准确率最匹配。</p><h3 id="分辨率的影响">分辨率的影响</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210612/9.png" alt=""></p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210612/10.png" alt=""></p><h3 id="推理效率">推理效率</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210612/11.png" alt=""></p><h2 id="参考">参考</h2><p>[1].Making EfficientNet More Efficient<br></p>]]></content>
      
      
      
        <tags>
            
            <tag> EfficientNet </tag>
            
            <tag> 效率新秀 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>详细解读：如何再CNN模型中插入Transformer后速度不变精度剧增？</title>
      <link href="/2021/06/11/11/"/>
      <url>/2021/06/11/11/</url>
      
        <content type="html"><![CDATA[<p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210611/1.png" alt=""></p><h2 id="简介">简介</h2><p>本文工作解决了Multi-Head Self-Attention(MHSA)中由于计算/空间复杂度高而导致的vision transformer效率低的缺陷。为此，作者提出了分层的MHSA(H-MHSA)，其表示以分层的方式计算。</p><p>具体来说，H-MHSA首先通过把图像patch作为tokens来学习小网格内的特征关系。然后将小网格合并到大网格中，通过将上一步中的每个小网格作为token来学习大网格中的特征关系。这个过程多次迭代以逐渐减少token的数量。</p><p>H-MHSA模块很容易插入到任何CNN架构中，并且可以通过反向传播进行训练。作者称这种新的Backbone为<strong>TransCNN</strong>，它本质上继承了transformer和CNN的优点。实验证明，<strong>TransCNN</strong>在图像识别中具有最先进的准确性。</p><h2 id="Vision-Transformer回顾">Vision Transformer回顾</h2><p>大家应该都很清楚Transformer严重依赖MHSA来建模长时间依赖关系。假设$X\in R^{N×C}$为输入，其中N和C分别为Token的数量和每个Token的特征维数。这里定义了Query $Q=XW^q$、key $K=XW^k$和 value $V=XW^v$，其中$Wq\in R^{C×C}$, $Wk\in R^{C×C}$, $Wv\in R^{C×C}$为线性变换的权重矩阵。在假设输入和输出具有相同维度的情况下，传统的MHSA可以表示为:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210611/2.png" alt=""></p><p>其中$\sqrt d$表示近似归一化，对矩阵行应用Softmax函数。注意，为了简单起见在这里省略了多个Head的概念。在上式中$QK^T$的矩阵乘积首先计算每对Token之间的相似度。然后，在所有Token的组合之上派生出每个新Token。MHSA计算后，进一步添加残差连接以方便优化，如:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210611/3.png" alt=""></p><p>其中，$W^p\in R^{C×C}$为特征映射的权重矩阵。最后，采用MLP层增强表示，表示形式为：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210611/4.png" alt=""></p><p>其中Y表示transformer block的输出。</p><p>有前面的等式可以得到MHSA的计算复杂度：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210611/5.png" alt=""></p><p>很容易推断出空间复杂度(内存消耗)。对于高分辨率的输入，$O(N^2)$可能变得非常大，这限制了Transformer在视觉任务中的适用性。基于此，本文的目标是在不降低性能的情况下降低这种复杂性，并保持全局关系建模的能力。</p><p>Transformer Block Pytorch实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Mlp</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_features, hidden_features=<span class="literal">None</span>, out_features=<span class="literal">None</span>, act_layer=nn.GELU, drop=<span class="number">0.</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        out_features = out_features <span class="keyword">or</span> in_features</span><br><span class="line">        hidden_features = hidden_features <span class="keyword">or</span> in_features</span><br><span class="line">        self.fc1 = nn.Linear(in_features, hidden_features)</span><br><span class="line">        self.act = act_layer()</span><br><span class="line">        self.fc2 = nn.Linear(hidden_features, out_features)</span><br><span class="line">        self.drop = nn.Dropout(drop)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.fc1(x)</span><br><span class="line">        x = self.act(x)</span><br><span class="line">        x = self.drop(x)</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        x = self.drop(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Attention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="comment"># Muliti-Head Self-Attention Block</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, dim, num_heads=<span class="number">8</span>, qkv_bias=<span class="literal">False</span>, qk_scale=<span class="literal">None</span>, attn_drop=<span class="number">0.</span>, proj_drop=<span class="number">0.</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.num_heads = num_heads</span><br><span class="line">        head_dim = dim // num_heads</span><br><span class="line">        self.scale = qk_scale <span class="keyword">or</span> head_dim ** -<span class="number">0.5</span></span><br><span class="line">        <span class="comment"># 输出 Q K V</span></span><br><span class="line">        self.qkv = nn.Linear(dim, dim * <span class="number">3</span>, bias=qkv_bias)</span><br><span class="line">        self.attn_drop = nn.Dropout(attn_drop)</span><br><span class="line">        self.proj = nn.Linear(dim, dim)</span><br><span class="line">        self.proj_drop = nn.Dropout(proj_drop)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        B, N, C = x.shape</span><br><span class="line">        qkv = self.qkv(x).reshape(B, N, <span class="number">3</span>, self.num_heads, C // self.num_heads).permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line">        q, k, v = qkv[<span class="number">0</span>], qkv[<span class="number">1</span>], qkv[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># q matmul k.T</span></span><br><span class="line">        attn = (q @ k.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) * self.scale</span><br><span class="line">        attn = attn.softmax(dim=-<span class="number">1</span>)</span><br><span class="line">        attn = self.attn_drop(attn)</span><br><span class="line">        <span class="comment"># attn&#x27; matmul v ==&gt; output</span></span><br><span class="line">        x = (attn @ v).transpose(<span class="number">1</span>, <span class="number">2</span>).reshape(B, N, C)</span><br><span class="line">        x = self.proj(x)</span><br><span class="line">        x = self.proj_drop(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Block</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="comment"># Transformer Encoder Block</span></span><br><span class="line">    <span class="comment"># Embedded Patches ==&gt; Layer Norm ==&gt; Muliti-Head Attention + ==&gt; Layer Norm ==&gt; MLP + ==&gt;</span></span><br><span class="line">    <span class="comment">#                 |_________________________________________|     |__________________|</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, dim, num_heads, mlp_ratio=<span class="number">4.</span>, qkv_bias=<span class="literal">False</span>, qk_scale=<span class="literal">None</span>, drop=<span class="number">0.</span>, attn_drop=<span class="number">0.</span>, drop_path=<span class="number">0.</span>, act_layer=nn.GELU, norm_layer=nn.LayerNorm</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.norm1 = norm_layer(dim)</span><br><span class="line">        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)</span><br><span class="line">        <span class="comment"># 进行稀疏化操作，可以得到更好的结果</span></span><br><span class="line">        self.drop_path = DropPath(drop_path) <span class="keyword">if</span> drop_path &gt; <span class="number">0.</span> <span class="keyword">else</span> nn.Identity()</span><br><span class="line">        self.norm2 = norm_layer(dim)</span><br><span class="line">        mlp_hidden_dim = <span class="built_in">int</span>(dim * mlp_ratio)</span><br><span class="line">        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = x + self.drop_path(self.attn(self.norm1(x)))</span><br><span class="line">        x = x + self.drop_path(self.mlp(self.norm2(x)))</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><h2 id="Hierarchical-Multi-Head-Self-Attention">Hierarchical Multi-Head Self-Attention</h2><p>在这里，作者介绍了如何使用H-MHSA降低MHSA的计算/空间复杂度。这里不是在整个输入中计算注意力，而是以分层的方式计算，这样每个步骤只处理有限数量的Token。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210611/6.png" alt=""></p><p>图b为H-MHSA的范式。假设输入特征映射$X\in R^{H_0×W_0×C}$的高度为$H_0$，宽度为$W_0$，有$N=H_0×W_0$。然后将特征图划分为大小为$G_0×G_0$的小网格，并将特征图Reshape为:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210611/7.png" alt=""></p><p>当$Q=X’W^q$, $K=X’W^k$和$V=X’W^v$时，式(1)生成局部注意$A_0$。为了简化网络优化，这里将$A_0$ Reshape为X的shape：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210611/8.png" alt=""></p><p>并添加一个残差连接：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210611/9.png" alt=""></p><p>由于$A_0$是在每个小$G_0×G_0$网格内计算的，因此计算/空间复杂度显著降低。</p><p>对于第i步(i&gt;0)，将第(i-1)步处的每个更小的网格$G_{i−1}×G_{i−1}$视为一个Token，这可以简单地通过对注意力特征$A_{i−1}$进行降采样来实现:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210611/10.png" alt=""></p><p>其中$MaxPool G_{i−1}(·)$和$AvgPoolG_{i−1}(·)$分别表示使用最大池化和平均池化(内核大小和步长为$G_{i−1}$)将样本$A_{i−1}$降为$G_{i−1}$次。因此，有$A’<em>{i-1}\in R^{H_i×W_i×C}$, 其中$H_i=H_0/(G_0G_1···G</em>{i−1})$，$W_i=W_0/(G_0G_1···G_{i−1})$。然后，将$A’_{i-1}$划分为$G_i×G_i$网格，并将其Reshape为:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210611/11.png" alt=""></p><p>当$Q=A’<em>{i−1}W^q$, $K=A’</em>{i−1}W^k$, $V=A’_{i−1}W^v$时，方程(1)获取注意特征$A_i$。$A_i$最终被Reshape为为输入的shape，比如：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210611/12.png" alt=""></p><p>并添加一个残差连接：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210611/13.png" alt=""></p><p>这个过程不断迭代，直到$H_i×W_i$足够小而不能在进行split。H-MHSA的最终输出为：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210611/14.png" alt=""></p><p>如果Upsample(·)表示将注意力特征上采样到原始大小，则$W^p$与Equ(2)含义相同， M为最大步数。通过这种方式，H-MHSA可以等价于传统的MHSA来模拟全局关系。</p><p>很容易证明，在所有$G_i$都相同的假设下，H-MHSA的计算复杂度近似:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210611/15.png" alt=""></p><p>与MHSA的计算复杂度相比较，本文所提方法显著降低了计算复杂度。</p><h2 id="将Transformer插入到CNN中">将Transformer插入到CNN中</h2><p>本文和之前将CNN与Transformer的方法一样遵循普遍做法，在网络Backbone中保留3D特征图，并使用全局平均池化层和全连接层来预测图像类别。这与现有的依赖另一个1D类标记进行预测的Transformer不同。</p><p>作者还观察到以往的Transformer网络通常采用GELU函数进行非线性激活。然而，在网络训练中，<strong>GELU函数非常耗费内存</strong>。作者通过经验发现，SiLU的功能与GELUs不相上下，而且更节省内存。因此，TransCNN选择使用SiLU函数进行非线性激活。</p><blockquote><p>作者做了一组实验。在ImageNet验证集上，当训练为100个epoch时，提出的具有SiLU的跨网络网络(TransCNN)在ImageNet验证集上获得80.1%的top-1精度。GELU的TransCNN得到79.7%的top-1精度，略低于SiLU。当每个GPU的batchsize=128时，SiLU在训练阶段占用20.2GB的GPU内存，而GELU占用23.8GB的GPU内存。</p></blockquote><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210611/16.png" alt=""></p><p>TransCNN的总体架构如图所示。</p><p>在TransCNN的开始阶段使用了2个连续的$3\times 3$个卷积，每个卷积的步长为2，将输入图像降采样到1/4的尺度。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210611/17.png" alt=""></p><p>然后，将H-MHSA和卷积块交替叠加，将其分为4个阶段，分别以1/4,1/8,1/16,1/32的金字塔特征尺度进行划分。这里采用的卷积模块是广泛使用的<strong>Inverted Residual Bottleneck</strong>(IRB，图c)，卷积是深度可分离卷积。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210611/18.png" alt=""></p><p>在每个阶段的末尾，作者设计了一个简单的二分支降采样块(TDB，图d)。它由2个分支组成:一个分支是一个典型的$3\times 3$卷积，步长为2;另一个分支是池化层和$1\times 1$卷积。在特征降采样中，这2个分支通过元素求和的方式融合，以保留更多的上下文信息。实验表明，<strong>TDB的性能优于直接降采样</strong>。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210611/19.png" alt=""></p><p>TransCNN的详细配置如表所示。提供了2个版本的TransCNN: TransCNN-Small和TransCNN-Base。TransCNN-Base的参数个数与ResNet50相似。需要注意的是，这里只采用了最简单的参数设置，没有进行仔细的调优，以证明所提概念H-MHSA和trannn的有效性和通用性。例如，作者使用典型的通道数，即64、128、256和512。MHSA中每个Head的尺寸被设置为64。作者提到对这些参数设置进行细致的工程调整可以进一步提高性能。</p><h2 id="实验">实验</h2><h3 id="ImageNet图像分类">ImageNet图像分类</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210611/20.png" alt=""></p><p>通过上表可以看出，将H-MHSA插入到相应的卷积模型中，可以以很少的参数量和FLOPs换取很大的精度提升。</p><h3 id="MS-COCO-2017目标检测">MS-COCO 2017目标检测</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210611/21.png" alt=""></p><p>通过上表可以看出，在比ResNet50更少的参数量的同时，RetinaNet的AP得到了很大的提升。</p><h3 id="MS-COCO-2017语义分割">MS-COCO 2017语义分割</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210611/22.png" alt=""></p><p>通过上表可以看出，在比ResNet50更少的参数量的同时，Mask R-CNN的AP得到了很大的提升。可见本文所提方法的实用性还是很强的。</p><h2 id="参考">参考</h2><p>[1].Transformer in Convolutional Neural Networks<br></p>]]></content>
      
      
      <categories>
          
          <category> Transformer </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
            <tag> 卷积CNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>太顶流！Scaling ViT将ImageNet Top-1 Acc刷到90.45%！</title>
      <link href="/2021/06/09/10/"/>
      <url>/2021/06/09/10/</url>
      
        <content type="html"><![CDATA[<p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210609/1.png" alt=""></p><blockquote><p>本文改进了ViT的架构和训练，减少了内存消耗并提高了模型的准确性！最终成功训练了一个具有20亿参数的ViT模型：ViT-G，在ImageNet上达到了90.45%的Top-1准确率.<br><strong>作者单位</strong>：谷歌大脑（苏黎世），有原ViT一作和二作</p></blockquote><h2 id="简介">简介</h2><p>视觉Transformer(ViT)等基于注意力的神经网络最近在许多计算机视觉基准测试中取得了最先进的结果。比例是获得出色结果的主要因素，因此，了解模型的scaling属性是有效设计的关键。虽然已经研究了扩展Transformer语言模型的规律，但尚不清楚Vision Transformers如何扩展。</p><p>为了解决这个问题，作者向上和向下扩展ViT模型和数据，并描述错误率、数据和计算之间的关系。在此过程中，作者改进了ViT的架构和训练，减少了内存消耗并提高了结果模型的准确性。结果，作者成功地训练了一个具有20亿个参数的ViT模型，该模型在ImageNet上达到了90.45%的Top-1准确率。该模型在小样本学习上也表现良好，例如，在ImageNet上每类只有10个examples的情况下可以达到84.86%的Top-1准确率。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210609/2.png" alt=""></p><h2 id="主要结论">主要结论</h2><p>作者首先介绍了关于扩展趋势的主要结果。在接下来的实验中在多达30亿个weakly-labelled图像上训练了几个ViT模型。作者期间改变了体系结构的大小，训练图像的数量和训练时间。为了评估模型学习的质量，作者测量了一下方面：</p><ul><li><p>通过在冻结的权值上训练线性分类器来实现的few-shot transfer；</p></li><li><p>通过对所有数据进行fine-tuning来实现对整个模型的transfer。</p></li></ul><h3 id="将计算、模型和数据一起放大">将计算、模型和数据一起放大</h3><p>下图显示了ImageNet上的10-shot线性评估和微调评估。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210609/3.png" alt=""></p><p>对于模型大小和数据大小的每个组合，作者预训练了不同数量的Steps。在图中，连接点表示经过不同步骤训练的同一模型。</p><p>结论：</p><ul><li><p>首先，将计算、模型和数据一起放大可以提高模型表征能力。</p></li><li><p>其次，模型的大小会影响表征能力。</p></li><li><p>第三，大模型受益于额外的数据，甚至需要超过1 Billion图像。</p></li></ul><h3 id="Double-saturating-power-law">Double-saturating power law</h3><p>对于超过2个数量级的计算，计算和性能之间的关系遵循power-law($E=aC^b$)，在log-log图上形成一条直线。然而，在compute spectrum的2端都观察到了饱和。在计算的最后最大模型误差率并不趋向于零错误率。如果按照作者的观察进行推断，无限容量模型将得到一个非零误差。</p><p>其实之前生成模型也观察到了这种类似的效应,作者将这种残差称为任务的<strong>不可约熵</strong>。通过作者绘制错误率图像，信息论的解释并不适用，但作者的观察支持ImageNet的基本性能上限的概念。根据该定律，这个饱和对应于错误率的一个附加常数c:$E = aC^b+c$。</p><p>在compute spectrum的前面看到小模型的饱和；最小模型的性能优于幂律模型的预测。出现这种饱和是因为即使是普通的解决方案也可以实现非零误差。例如，预测大多数类(几乎为零计算)将获得与其在测试集中出现频率相关的精度。在生成模型中没有观察到这个下界，要么是因为它们的最小模型大到足以避免这个区域，要么是因为对数损失在性能比精度更差的情况下达到饱和(最终会达到饱和)。这个饱和对应于x轴上的位移$E=a(C+d)^{-b}+c$中的d。这个常数表明零计算模型仍将获得非零精度。</p><h3 id="大模型的样本效率更高">大模型的样本效率更高</h3><p>下图显示了预处理过程中“seen”的图像总数(批量大小乘以step数)的表征质量。除了ImageNet微调和公共验证集上的线性10-shot结果，作者还给出了ImageNet微调模型的结果，其中ImageNet-v2测试集作为泛化性的指标。在这幅图中展示了对30亿张图像进行预训练的3个ViT模型的结果。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210609/4.png" alt=""></p><p>作者观察到<strong>更大的模型样本效率更高</strong>，可以用更少的图像达到相同的错误率水平。对于10-shot Ti/16模型需要看到近100倍以上的图像来匹配L/16模型的表征质量。当进行微调时，这个因数从100降低到大约20。实验结果表明，在有足够的数据的情况下，<strong>训练一个更大的模型以较少的step训练是更好的</strong>。</p><h3 id="ViT-G-14结果">ViT-G/14结果</h3><p>ViT-G/14比以前最好的ViT-H/14模型有很大的优势(超过5%)，每类10个样本的准确率达到84.86%。每类10张图片不到1%ImageNet数据(每个类13个例子)，通常用于自监督和半监督学习。作为参考，下图显示了2个最先进的自监督学习模型，SimCLR v2和BYOL，使用了1%的ImageNet数据。但是请注意，这些方法有很大的不同:ViT-G/14使用大量缺乏监督的数据，并且只进行一次预训练，然后转移到不同的任务中。同时，自监督学习模型使用无标记但领域内的数据进行训练，并以单个任务为目标。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210609/5.png" alt=""></p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210609/6.png" alt=""></p><p>表1显示了其余基准测试的结果。ViT-G/14在ImageNet上达到90.45%的top-1精度，设置新的艺术状态。在ImageNet-v2上，ViT-G/14比基于EfficientNet-L2的Noisy Student模型改进了3%。对于ReaL, ViT-G/14仅比ViT-H和BiT-L略胜一筹，这再次表明<strong>ImageNet分类任务很可能达到饱和点</strong>。对于ObjectNet来说，ViT-G/14比BiT-L表现要好很多，比Noisy Student好2%，但比CLIP落后2%。请注意，与其他方法不同，CLIP不会在ImageNet上进行微调，而是直接在ObjectNet上进行评估，这可能提高了它的健壮性。最后，当将ViT-G/14模型转移到VTAB时，它在所有任务中只使用一个超参数就能得到一致更好的结果。</p><h2 id="Method-details">Method details</h2><p>作者对ViT模型和训练过程提出了一些改进。这些改进大都很容易实现，并且可以显著提高内存利用率和模型质量。它们允许单独使用数据并行性来训练ViT-G/14，整个模型拟合在一个单独的模型上TPUv3核心。</p><h3 id="“Head”解耦权重衰减">“Head”解耦权重衰减</h3><p>在低数据条件下，权值衰减对model adaptation有显著影响。作者对这一现象进行了mid-size规模的研究。</p><p>作者发现，对于模型中的最后一个线性层(“Head”)和其余的权重(“Body”)可以从解耦权值衰减强度中获益。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210609/7.png" alt=""></p><p>上图展示了这种效应:在JFT-300M上训练一个collection ViT-B/32模型，每个cell对应不同的head/body权重衰减值。对角线对应于对2个衰变使用相同的值。可以观察到最佳的性能表现是非对角线的(例如，head和body的权值衰减是解耦的)。有趣的是，作者观察到head的权值衰减尽管提高了迁移性能，但是也降低了pre-training(upstream)任务的表现。</p><p>对于这种现象，作者还没有一个完整的解释。然而，如果假设Head的权值衰减越大，代表就会在类别之间有更大的差距，从而更好地few-shot adaptation。这与支持向量机背后的主要思想相似。这种大的衰减使得上游预训练中更难获得高精确度，但作者的主要目标是高质量的迁移。</p><h3 id="通过删除-class-token来节省内存">通过删除[class] token来节省内存</h3><p>我们都知道最大的VIT模型使用14×14个patch, 224×224的图像产生了256个visual “tokens”，每个token对应一个图像patch。在此之上，ViT模型有一个额外的[class] token，它用于产生最终的全局表征，最终使得toekn的总数达到256+1=257个。</p><p>对于ViT模型，当前的TPU硬件将token维度设置为128的倍数，这可能导致高达50%的内存开销。为了克服这个问题，作者研究了使用额外的[class] token的替代方法。评估了全局平均池(GAP)和多头注意力池(MAP)来聚合来自所有patch token的表示。将MAP中的heads数设置为与模型其他部分中的注意heads数相等。为了进一步简化head设计，在最终预测层之前去掉了最终的非线性投影。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210609/8.png" alt=""></p><p>为了选择最好的head，作者对[class] token和GAP/MAP head进行并行比较。结果上图所示。作者发现所有heads的表现都是相似的，而由于前面提到的padding考虑，GAP和MAP的内存效率更高。作者还观察到非线性映射可以移除。因此，作者选择了MAP  head，因为它是最具表现力和泛化性。</p><h3 id="Scaling-up-data">Scaling up data</h3><p>在本研究中作者使用了专有的JFT-3B数据集，这是JFT-300M数据集的一个更大的版本，JFT-300M数据集在以前的大规模计算机视觉模型研究中使用过。该数据集由近30亿张图像组成，通过半自动管道标注了约30k个标签的类层次结构。因此，数据和相关的标签是有噪声的。这里忽略标签的层次方面，只使用分配的标签作为目标，通过sigmoid交叉熵损失进行多标签分类。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210609/9.png" alt=""></p><p>如图，左边显示了整个线性10-shot的ImageNet性能评估。作者观察到，JFT-3B甚至在JFT-300M未完全训练一个epoch之前就产生了一个更好的模型。因此，JFT-300M在small B/32模型以及larger L/16上过拟合并不是改善的唯一原因。</p><p>作者将模型微调到完整的ImageNet数据集(右)，并确认这些改进转移到完整的微调设置。总体而言，无论是小型还是大型模型，数据集的改变提高了ImageNet的1%左右。JFT-300M和JFT-3B的训练行为除了表现上的改善外，还有相似之处。最重要的是，JFT-3B可以让我们在不担心过拟合和正则化的情况下进一步扩大规模。</p><h3 id="Memory-efficient-optimizers">Memory-efficient optimizers</h3><p>当训练大型模型时，模型参数所需的存储成为瓶颈。本文设计最大的模型ViT-G，大约有20亿个参数，占用8GiB的显存。更糟糕的是，通常用于训练transformer的Adam优化器为每个参数存储2个额外的浮点标量，这导致了额外的2倍开销(额外的16GiB)。为了解决Adam优化器带来的开销，作者做了2个修改：</p><h4 id="Adam-with-half-precision-momentum">Adam with half-precision momentum</h4><p>通过经验观察到，半精度(bfloat16类型)的动量存储不会影响训练，对结果也没有影响。这可以将优化器的开销从原来的2倍减少到1.5倍。值得注意的是，<strong>使用半精度存储第2个动量会导致性能显著下降</strong>。</p><h4 id="Adafactor优化器">Adafactor优化器</h4><p>上述Adam优化器仍然会导致较大的内存开销。因此，作者将注意力转向adfactor优化器，它使用秩1因式分解存储第2动量。从实用的角度来看，这将导致微不足道的内存开销。然而，作者并没有直接使用Adafactor优化器，而是做了以下修改:</p><ul><li><p>1 重新将半精确引入第1个动量，而推荐的设置根本不使用第1个动量。</p></li><li><p>2 禁用了相对于权重Norm的学习率的缩放，在Adafactor默认开启的。</p></li></ul><p>由此产生的优化器只引入了50%的内存开销。作者观察到，这2个建议的优化器的性能与最初的Adam优化器相当，甚至稍还要好一些。</p><h3 id="Learning-rate-schedule">Learning-rate schedule</h3><p>在研究中，作者希望使用几个不同的训练时间训练一个模型，以衡量模型大小和训练时间之间的权衡。当使用线性衰减时，每个训练时间都需要从头开始运行自己的训练，这是一种低效的方法。</p><p>通过探索学习率计划来解决这个问题，类似于开始的warmup阶段，包括训练结束的cooldown阶段，在这个阶段学习率线性趋于零。在warmup和cooldown阶段之间，学习率不应该很快下降到零。这可以通过使用一个常量，或一个倒数平方根schedule的主要训练部分来实现。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210609/10.png" alt=""></p><p>上图述了这些选项中的几个，在大约200k、400k和500k步之后有一个cooldown时间。图的上半部分显示了这些选项及其间隔期的验证分数(越高越好)，以及2个线性时间表。</p><p>当事先知道训练时间而不打算继续训练时，线性计划仍然是可取的，所有3种选择都相当接近，其优点是允许不确定的训练，并从一次跑步中评估多个训练时间。对于每一个schedule作者优化了学习率和exact shape。作者也简单地尝试了循环学习率计划，但是它们似乎表现得更差，因此，选择了平方根倒数schedule。</p><h3 id="选择模型的维度">选择模型的维度</h3><p>ViT模型有许多参数可以用来控制模型的shape，作者参考原始版本的完整细节。简单地说，这些包括patch-size，编码器block的数量(深度)，patch embeddings和self-attention(宽度)，attention heads的数量，MLP块的维度(MLP-宽度)。</p><p>在此基础上，作者依赖XLA编译器来优化模型以提高运行时速度和内存占用。在底层，XLA使用复杂的启发式方法将模型编译为特定硬件的代码，以最佳的方式权衡内存和速度。因此，很难预测哪个模型配置适合一个设备的内存。</p><p>因此，作者运行了一个广泛的模拟，其中实例化了大量不同shape的ViT，并试图训练它们几个step。作者通过改变深度、宽度、头部和mlp宽度，但保持patch大小在14像素。通过这种方式，可以测量它们的速度，以及给定的模型是否适合设备的内存。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210609/11.png" alt=""></p><p>上图总结了这个模拟的结果。每个块对应一个模型配置，块的阴影对应其训练速度(越亮越快)。橙色块显示哪个没有任何修改原始ViT模型适合。绿色块还包括第3.2节中描述的内存节约，以及第3.4节中描述的半精度Adam。最后，蓝色块是修改的AdaFactor优化器。通过修改和实验能够适应深度最多为100个编码器块的thin ViT模型。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210609/12.png" alt=""></p><p>最初的Vision Transformer 结论是伸缩所有方面是最有效的(深度、宽度、mlp宽度和patch大小)同时以相似的数量。遵循这一建议，并在相应的内存容量限制下为ViT-g和ViT-G选择形状，并在表中进行总结。</p><h2 id="参考">参考</h2><p>[1].Scaling Vision Transformers<br></p>]]></content>
      
      
      <categories>
          
          <category> Transformer </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Scaling ViT </tag>
            
            <tag> ImageNet Top-1 Acc </tag>
            
            <tag> Transformer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>详细解读NMS-Loss是如何解决目标检测中的遮挡问题</title>
      <link href="/2021/06/08/9/"/>
      <url>/2021/06/08/9/</url>
      
        <content type="html"><![CDATA[<p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210608/1.png" alt=""></p><h2 id="简介">简介</h2><p>非极大值抑制(Non-Maximum Suppression, NMS)在目标检测中至关重要，它通过合并假阳性(FP)和假阴性(FN)影响目标检测结果，尤其是在人群遮挡场景中。在本文中提出了NMS造成的训练目标和评估指标之间的弱连接问题，并提出了一种新的损失函数<strong>NMS-loss</strong>，使NMS过程可以端到端地被训练而不需要任何附加的网络参数。</p><p>NMS-loss惩罚2种情况，即FP没有被抑制，而FN被NMS错误地删除。具体来说，NMS-Loss提出了pull loss将具有相同目标的预测拉得很近，以及push loss将具有不同目标的预测推得很远。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210608/2.png" alt=""></p><p>实验结果表明，在NMS-Loss的帮助下NMS-Ped检测器在Caltech数据集上的Miss Rate为5.92%，在CityPersons数据集上的Miss Rate为10.08%，均优于现有的同类检测器。</p><h4 id="本文主要贡献">本文主要贡献</h4><ul><li><p>首先提出了行人检测中训练目标与评估指标之间的弱连接问题，并提出了一种新的NMS-loss，使NMS过程在不引入任何参数和运行时间开销的情况下可以端到端进行训练。</p></li><li><p>作者提出了精心设计的pull loss和push loss，分别考虑预测坐标和置信度，帮助网络提高精度和召回性能。</p></li><li><p>在行人检测中，作者借助NMS-Loss提出的NMS-Ped在Caltech和CityPersons数据集上优于现有的SOTA方法。</p></li></ul><h2 id="NMS-LOSS">NMS-LOSS</h2><p>传统的NMS流程如Alg.1中所示，没有考虑红色字体。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210608/3.png" alt=""></p><p>NMS从一组得分为S的检测框$B$开始，</p><p><strong>首先</strong>，将得分最大的proposal $𝑏_𝑚$从$B$集合移动到最终保留检测的集合$K$;</p><p><strong>然后</strong>，删除$B$中得分为$S$的且与$𝑏_𝑚$的重叠高于阈值$𝑁_𝑡$的框。</p><p>对剩下的$B$集重复此过程。</p><p>但是，现有的方法没有将NMS纳入训练过程中来调整检测框，使得学习目标与评价指标不一致，这意味着NMS未抑制FP和NMS消除FN分别会损害精度和召回率。为了避免不一致，作者提出NMS-loss将NMS程序引入到训练过程中，自适应地选择由NMS引起的错误预测，并使用精心设计的pull和push两种损失来最小化FP和FN。具体来说NMS-Loss定义为:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210608/4.png" alt=""></p><p>其中$L_{pull}$为pull损失用来惩罚FP同时不抑制由NMS，$L_{push}$为push损失用来惩罚NMS的FN错误删除。系数$\lambda_{pull}$和$\lambda_{push}$是平衡损失的权重。</p><p>NMS-Loss的细节在Alg.1中用红色文本强调。与传统pipeline不同，这里使用一组$G$，包含相应的检测框ground truth index，用于识别FP和FN。在NMS-Loss计算过程中，M是一个辅助字典，以ground truth指数为key，对应最大检测得分为value，用来记录每个ground truth的max score预测。</p><p>NMS-loss自然地合并到NMS过程中，而不包含任何额外的训练参数。对于测试来说，NMS-Loss的运行时成本为零。</p><h3 id="定义Pull-Loss">定义Pull Loss</h3><p>以降低FP为目标需要找出错误的预测。为此，在每次迭代中检查当前的max score预测$𝑏_𝑚$是否为其对应的$g_𝑚$ ground truth的max score预测。如果不是，则说明$𝑏_𝑚$是一个未被NMS抑制的FP，pull loss应在$𝑏_𝑚$和$g_𝑚$ ground truth的max score prediction $𝑏_{𝑚𝑎𝑥}$之间执行(见图1)。形式上pull loss计算如下:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210608/5.png" alt=""></p><p>其中$N_t$为预定义的NMS阈值，$s_m$为对应于$b_m$的预测score。</p><p>作者注意到pull loss的2个特性:</p><ul><li><p>当$b_{max}$和$b_m$之间的IoU较小时，pull loss有增加的趋势，迫使网络学会将$b_m$拉向$b_{max}$。NMS的阈值$N_t$用于防止异常值的梯度对模型学习的影响过大。另外，对于NMS只需要使FP和TP之间的IoU高于$N_t$即可。在pull loss中使用$N_t$来减小异常值的梯度，可以使网络易于学习和收敛。</p></li><li><p>FP预测得分对pull loss也有较大影响。FP得分越高，对评价结果的影响越大，直观上需要更多的关注。此外，它使网络学习修正FP不仅要制约box坐标，而且要考虑降低预测分数。</p></li></ul><h3 id="定义Push-Loss">定义Push Loss</h3><p>在NMS中，当前的最大score预测$𝑏_𝑚$用$𝑏_𝑚$消除了获得高于$𝑁_𝑡$的IoU的box。如果剔除的框$b_i$对应的ground truth index 与$b_m$不同，则$b_i$为FN，降低召回率(见图1)。为了避免错误地删除$𝑏_𝑖$提出push loss来惩罚FN:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210608/6.png" alt=""></p><p>其中$s_i$为$b_i$对应的预测得分。与pull loss不同当$IoU(b_i,b_m)\to1$时，push loss增大，模型学会将$b_i$推离$b_m$。为了避免模型倾向于通过降低FN的分数来减少push loss，作者只使用$s_i$来重新加权损失，而不使用反向传播梯度。</p><p>对于拥挤的场景，特别是在CityPersons数据集中，边界框的ground truths是相互重叠的。在IoU=0的情况下，将他们的预测相互排斥是不合理的。为了处理这个问题，作者只在预测IoU高于其对应ground truth box的IoU时才计算$𝐿_{𝑝𝑢𝑠ℎ}$。</p><p>本文所提的Pull Loss和Push Loss是根据预测来执行的。当pull/push loss被激活时，网络会尝试pull/push两个预测，分别pull/push彼此。因为高分预测通常会得到一个更准确的位置，所以在一个不准确的预测基础上移动一个准确的预测是不合理的。为了解决这个问题，作者停止了高分预测的梯度向后传播，导致网络专注于错误的预测。</p><h3 id="与RepLoss的不同之处在哪里？">与RepLoss的不同之处在哪里？</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210608/7.png" alt=""></p><p>RepLoss通过设置损失函数的方式，使预测框和所负责的真实目标框的距离缩小，而使得其与周围非负责目标框（包含真实目标框和预测框）的距离加大 。如下式，如果与周围目标的距离越大，损失值会越小。<br><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210608/8.png" alt=""></p><p>作者对NMS-Loss和RepLoss进行了详细的比较，因为这2种方法都是基于它们的目标进行pull/push预测的。</p><p>主要有3个区别:</p><ul><li><p>RepLoss在所有实例上执行，而NMS-Loss只在被NMS错误处理的实例上执行，从而实现了端到端训练。</p></li><li><p>RepLoss只考虑回归，而score在NMS-Loss中也用于实例重加权。</p></li><li><p>在密集人群场景下RepLoss将实例推开，即使它们的目标本来很接近，使RepLoss与回归损失相矛盾。相反，NMS-Loss会推送与其他IoU高于其对应ground truth box IoU的实例，这样可以消除RepLoss的矛盾。</p></li></ul><p>如表所示，NMS-Loss不仅比RepLoss表现更好，而且在CityPersons上有更高的相对改善。这表明，NMS-Loss可以在广泛使用的数据集上实现稳定的相对改进(高于10%)。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210608/9.png" alt=""></p><h2 id="实验">实验</h2><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210608/10.png" alt=""></p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210608/11.png" alt=""></p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210608/12.png" alt=""></p><h2 id="参考阅读">参考阅读</h2><p>[1].NMS-Loss: Learning with Non-Maximum Suppression for Crowded Pedestrian Detection<br></p><p>[2].Repulsion Loss: Detecting Pedestrians in a Crowd<br></p>]]></content>
      
      
      
        <tags>
            
            <tag> NMS-Loss </tag>
            
            <tag> 让检测告别遮挡 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>卷积与Self-Attention完美融合X-volution插入CV模型将带来全任务的涨点</title>
      <link href="/2021/06/07/8/"/>
      <url>/2021/06/07/8/</url>
      
        <content type="html"><![CDATA[<br><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210607/1.png" alt=""></p><blockquote><p>本文建立了一个由卷积和self-attention组成的多分支基本模块，能够统一局部和非局部特征交互，然后可以结构重新参数化为一个纯卷积风格的算子：X-volution，即插即用！可助力分类、检测和分割任务的涨点！<br><strong>作者单位</strong>：上海交通大学(倪冰冰团队), 华为海思</p></blockquote><h2 id="简介">简介</h2><p>卷积和self-attention是深度神经网络中的2个基本构建块，前者以线性方式提取图像的局部特征，而后者通过非局部关系编码高阶上下文关系。尽管本质上是相互补充的，即一阶/高阶、最先进的架构，但是，CNN或Transformer均缺乏一种原则性的方法来在单个计算模块中同时应用这2种操作，因为它们的异构计算视觉任务的全局点积的模式和过度负担。</p><p>在这项工作中，作者从理论上推导出一种全局self-attention近似方案，该方案通过对变换特征的卷积运算来近似self-attention。基于近似方案建立了一个由卷积和self-attention操作组成的多分支基本模块，能够统一局部和非局部特征交互。重要的是，一旦经过训练，这个多分支模块可以通过结构重新参数化有条件地转换为单个标准卷积操作，呈现一个名为X-volution的纯卷积风格的算子，准备作为atomic操作插入任何现代网络。大量实验表明，所提出的X-volution实现了极具竞争力的视觉理解改进（ImageNet分类的top-1准确率+1.2%，COCO 检测和分割的+1.7box AP和+1.5mask AP）。</p><h2 id="方法">方法</h2><p>本文提出了一种新颖的原子算子<strong>X-volution</strong>，将基本卷积算子和self-attention算子集成到一个统一的计算块中，期望从<strong>局部vs非局部</strong>/<strong>线性vs非线性</strong>两方面获得令人印象深刻的性能改进。</p><p><strong>首先</strong>，回顾卷积和self-attention的基本数学公式；</p><p><strong>然后</strong>，解读全局self-attention近似方案，它可以直接转换为一个兼容的卷积模式。</p><p><strong>最后</strong>，解释在推断阶段如何有条件地合并卷积分支和所提出的self-attention近似到单个卷积风格原子操作符。</p><h3 id="2-1-回顾卷积和self-attention">2.1 回顾卷积和self-attention</h3><p>这2个算子想必大家已经非常熟悉了，这里就简单的说一下哈！！！</p><h4 id="卷积Module">卷积Module</h4><p>卷积算子是用于构建卷积神经网络(CNN)的基本算子，它通过有限局部区域内的线性加权来估计输出。给定一个特征张量$X\in R^{C_i×H×W}$, $C_i$表示输入通道的数量，H是高度，W是宽度。卷积算子的估计结果$Y\in R^{C_o×H×W}$由以下公式定义:</p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210607/2.png" width = "500" align=center /><p>其中$C_o$为输出通道数。$w\in R^{C_o×C_i×K×K}$为卷积核，$W_{c_o,c_i,δ_i+[K/2],δ_j+[K/2]}$为特定位置核标量值。$K$为卷积kernel大小，$B\in R^{C_o}$为偏差向量，$∆k\in Z^2$为$K × K$卷积kernel中所有可能偏移的集合。</p><h4 id="Self-Attention-Module">Self-Attention Module</h4><p>与卷积不同，self-attention不能直接处理图像张量，首先将输入特征张量reshape为向量$X\in R^{C×L}$。$L$表示向量的长度，$L=H×W$。$W^Q、W^K、W^V$分别表示Query、Key、Value的嵌入变换，是空间共享的线性变换。Self-Attention的定义如下:</p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210607/3.png" width = "500" align=center /><p>其中$\overline W(X)$表示最终的Self-Attention等价系数矩阵，可以认为是一个动态和空间变化的卷积kernel。</p><h3 id="全局self-attention近似方案">全局self-attention近似方案</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210607/4.png" alt=""></p><p>全局自注意是最原始的attention方案，它得益于全局范围的优势而具有优异的性能。然而，它的复杂度太大了$O(n^2)(n表示总像素数)$使得其在CV任务中的应用受到严重限制。关键问题是<strong>能否在公式2中推导出$\overline W(X)$的适当近似结果，即能否找到$\overline W(X)$的兼容计算模式，即能否找到卷积、single element-wise product等现成的算子替代?</strong></p><p>在本部分中，作者展示了在简单的element-wise shift和dot-product之后，可以用卷积的形式近似全局self-attention算子。给定特征张量$X$中的一个位置，将其特征向量表示为$x_0$，其attention logit $s_0$可以写成如下公式:</p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210607/5.png" width = "500" align=center /><p>其中$\alpha _t = w^pw^qw^vx_t$， Ω为全局区域，A为以x0为中心的局部区域。在图1的左边说明了局部区域和非局部区域。图中灰框表示输入特征X的全局区域，绿框表示以$x_0$为中心的局部区域。</p><p>另外，non-local区域是指局部区域以外的区域。因为图像具有很强的说服力（根据马尔可夫性质），$x_0$可以用像素在其局部区域近似线性表示:$x_0≈\sum_{x_k\in A˚}\beta _kx_k$，其中$\beta_k$为线性权值。代入式3中第2项，可得:</p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210607/6.png" width = "500" align=center /><p>在不失一般性的情况下，可以在区域A中加入系数为零的项。通过设计，non-local区域也在局部区域的边界像素的接受域内。因此可以将上式转化为:</p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210607/7.png" width = "400" align=center /><p>根据图像的马尔可夫性质，可以假设对于$x_k\in A$，远离$x_k$的$x_i$与$x_k$之间的相互作用是弱的。因此，可以进一步简化式上式:</p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210607/8.png" width = "400" align=center /><p>其中$U(x_k)$为$x_k$的局部区域。将上式代入Eq.3中的第2项，可以改写为:</p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210607/10.png" width = "500" align=center /><p>注意,$x_k,x_i$是$x_k$和$x_i$之间的内积，它衡量了$x_k$和$x_i$之间的相似性。$\sum_{x_i\in U(x_k)}\alpha_i\beta_k (x_k,x_i)$是$x_k$在其邻近区域的attention结果。因此，在$x_0$处的全局注意力logit可以通过加权求和其邻域内像素的attention结果来近似。</p><p>根据以上理解，可以设计一个近似算子，通过逐点上下文关系传播来估计全局attention。因此，作者提出了一个全局注意力近似方案，Pixel Shift<br>Self-Attention (PSSA)，基于像素偏移和卷积来近似全局attention。</p><p>具体来说，首先将特征映射沿给定的方向(即左、右、上等)移动L个像素，然后将原始特征与移动的特征进行元素积，得到变换后的特征。</p><p>实际上，shift-product操作建立了邻域内点之间的上下文关系，通过分层叠加可以将上下文关系传播到全局区域。最后，对这些变换后的特征进行加权求和(可以通过卷积算子实现)，得到一个近似的自注意力映射。平移、元素积和加权求和的复杂度为O(n)，因此提出的PSSA是一个时间复杂度为O(n)的算子。值得注意的是，PSSA实际上是将self-attention转换为对转换特征的标准卷积运算。该结构通过层次叠加进而通过上下文关系传播实现全局self-attention logit的估计。</p><h3 id="卷积和Self-Attention的统一-X-volution">卷积和Self-Attention的统一: X-volution</h3><h4 id="卷积和Self-Attention是相辅相成的">卷积和Self-Attention是相辅相成的</h4><p>卷积采用局域性和各向同性的归纳偏差，使其具有平移等方差的能力。然而，局部固有的特性使卷积无法建立形成图所必需的长期关系。</p><p>与卷积相反，<strong>Self-Attention摒弃了提到的归纳偏差，即所谓的低偏差，并从数据集中发现自然模式，而没有明确的模型假设。低偏差原则给予Self-Attention以探索复杂关系的自由(例如，长期依赖、各向异性语义、CNN中的强局部相关性等)，因此该方案通常需要对超大数据集进行预训练(如JFT-300M、ImageNet21K)</strong>。</p><p>此外，Self-Attention很难优化，需要更长的训练周期和复杂的Tricks。有文献提出将卷积引入Self-Attention以提高Self-Attention的鲁棒性和性能。简而言之，采用不同的模型假设，使卷积和Self-Attention在优化特征、注意范围(即局部/长期)和内容依赖(内容依赖/独立)等方面得到相互补充。</p><h4 id="统一的多分支拓扑">统一的多分支拓扑</h4><p>有一些工作试图将卷积和self-attention结合起来，然而，粗糙的拓扑组合(例如，分层堆叠，级联)阻止他们获得单个原子操作(在同一个模块中应用卷积和注意)，使结构不规则。例如，AANet将经过卷积层和Self-Attention层处理的结果直接连接起来，得到组合结果。说明单一的卷积或单一的Self-Attention都会导致性能下降，当它们同时存在时，性能会有显著的提高。</p><p>在这个工作中，作者研究卷积和self-attention的数学原理后找到了近似形式。作者还观察到全局元素相互作用(点积)可以用局部元素相互作用的传播来近似表示。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210607/11.png" alt=""></p><p>因此，这2种算子可以用统一的计算模式来处理，即卷积。从另一个角度来看，卷积运算可以看作是Self-Attention的空间不变偏差。考虑到这一点，可以将算子组合成一个多分支拓扑，如图所示，这可以同时受益于卷积和Self-Attention。多分支模块由2个主要分支组成。左边的分支由级联的Shift Pixel Self-Attention和batch-normalization组成起到近似全局Self-Attention操作的作用，右分支被设计成由级联卷积和批归一化组成的卷积分支。</p><h4 id="有条件地将多分支方案转换为Atomic-X-volution">有条件地将多分支方案转换为Atomic X-volution</h4><p>多分支模块实现了卷积与Self-Attention的功能组合。然而，它只是一种粗粒度的算子组合，这将使网络高度复杂和不规则。从硬件实现的角度来看，多分支结构需要更多的缓存来服务于多路径的处理。相反，单个算子操作效率更高，内存开销更低，这是硬件友好的。</p><p>为了简单起见，在这里省略批标准化的公式。实际上，批归一化可以看作是一个$1×1$组卷积(其组等于channel数)，可以合并到卷积/Self-Attention层中。实际上，一般采用分层叠加的PSSA，堆叠结构中的加权运算可以省略，因为分层叠加隐含了加权邻接像素的运算。本文提出的多分支模块的训练阶段如下：</p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210607/12.png" width = "500" align=center /><p>其中$w^c$为卷积权值，$b^c$为其对应的偏置。</p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210607/13.png" width = "500" align=center /><p>其中$w^A(x_0,x_i)=w^qw^kw^v(x_0,x_i)$表示来自pixel shift self-attention 分支的content-dependent/dynamic coefficients。$W_c$表示从卷积分支继承的content-independent/static coefficients，训练完成后会修复。</p><p>观察上式可以发现，经过一个简单的变换，多分支结构可以转换成卷积形式。值得指出的是，这个过程在CNN中被广泛使用，被称为structural re-parameterization。在这里首先把它扩展到卷积和self-attention的合并。根据上式将由卷积和self-attention组成的多分支模块等价地转换为一个动态卷积算子X-voultion。</p><p>请注意，这里建议X-volution可以作为一个原子操作插入主流网络(例如，ResNet)。</p><h2 id="实验">实验</h2><h3 id="图像分类">图像分类</h3><h4 id="架构设计">架构设计</h4><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210607/14.png" width = "500" align=center /><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210607/15.png" width = "400" align=center /><p>结果表明，第3阶段的替换效果最好，ResNet-34的top-1准确率为+1.2%，ResNet-50的top-1准确率为+0.9%。作者怀疑第4阶段替换的性能较差ResNet-50可以归因于可学习参数的增加，这减慢了网络的收敛。</p><h3 id="目标检测">目标检测</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210607/16.png" alt=""></p><p>特别是，本文所提X-volution(SA)实现了最好的性能，与ResNet-50相比增加了+1.7boxes AP。通过结合低阶局部特征和高阶长依赖，所提出的X-volution算子比单独的卷积或自注意力算子具有更高的精度。</p><p>结果表明，图完备原子算符有助于视觉理解，而现有的计算算符忽略了这一性质。此外，基于PSSA的X-volution也取得了与X-volution(SA)相当的性能，表明在X-volution模块中，近似效果良好，对硬件实现和计算更加友好。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210607/17.png" alt=""></p><h3 id="语义分割">语义分割</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210607/19.png" alt=""></p><p>可以观察到，作者提出的X-volution比其他算子的性能要好很多。其中，X-volution(SA)实现了41.1 box AP和37.2 mask AP。</p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210607/20.png" width = "400" align=center /><h2 id="参考">参考</h2><p>[1].X-volution: On the Unification of Convolution and Self-attention.<br></p>]]></content>
      
      
      <categories>
          
          <category> 卷积CNN </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 卷积 </tag>
            
            <tag> Self-Attention </tag>
            
            <tag> CV模型 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>详细解读Transformer怎样从零训练并超越ResNet</title>
      <link href="/2021/06/06/7/"/>
      <url>/2021/06/06/7/</url>
      
        <content type="html"><![CDATA[<br><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210606/1.png" alt=""></p><blockquote><p>本文证明了在没有大规模预训练或强数据增广的情况下，在ImageNet上从头开始训练时，所得ViT的性能优于类似大小和吞吐量的ResNet！而且还拥有更敏锐的注意力图。<br><strong>作者单位</strong>：谷歌,UCLA</p></blockquote><h2 id="简介">简介</h2><p>Vision Transformers(ViTs)和MLPs标志着在用通用神经架构替换手动特征或归纳偏置方面的进一步努力。现有工作通过大量数据为模型赋能，例如大规模预训练和/或重复的强数据增广，并且还报告了与优化相关的问题（例如，对初始化和学习率的敏感性）。</p><p>因此，本文从损失几何的角度研究了ViTs和MLP-Mixer，旨在提高模型在训练和推理时的泛化效率。可视化和Hessian揭示了收敛模型极其敏感的局部最小值。</p><p>同时通过使用最近提出的<strong>锐度感知优化器</strong>提高平滑度，进而大大提高了ViT和MLP-Mixer在跨越监督、对抗、对比和迁移学习（例如，+5.3% 和 +11.0%）的各种任务上的准确性和鲁棒性使用简单的Inception进行预处理，ViT-B/16和Mixer-B/16在ImageNet上的准确率分别为Top-1）。</p><p>作者研究表明，改进的平滑度归因于前几层中较稀疏的活动神经元。在没有大规模预训练或强数据增强的情况下，在ImageNet上从头开始训练时，所得 ViT的性能优于类似大小和吞吐量的ResNet。同时还拥有更敏锐的注意力图。</p><h2 id="Background和Related-Work">Background和Related Work</h2><p>最近的研究发现，ViT中的self-attention对性能并不是至关重要的，因此出现了一些专门基于mlp的架构。这里作者以MLP-Mixer为例。MLP-Mixer与ViT共享相同的输入层;也就是说，它将一个图像分割成一系列不重叠的Patches/Toekns。然后，它在torkn mlp和channel mlp之间交替使用，其中前者允许来自不同空间位置的特征融合。</p><h2 id="ViTs和MLP-Mixers收敛到锐局部极小值">ViTs和MLP-Mixers收敛到锐局部极小值</h2><p>目前的ViTs、mlp-mixer和相关的无卷积架构的训练方法很大程度上依赖于大量的预训练或强数据增强。它对数据和计算有很高的要求，并导致许多超参数需要调整。</p><p>现有的研究表明，当在ImageNet上从头开始训练时，如果不结合那些先进的数据增强，尽管使用了各种正则化技术(例如，权重衰减，Dropout等)ViTs的精度依然低于类似大小和吞吐量的卷积网络。同时在鲁棒性测试方面，vit和resnet之间也存在较大的差距。</p><p>此外，Chen等人发现，在训练vit时，梯度会出现峰值，导致精确度突然下降，Touvron等人也发现初始化和超参数对训练很敏感。这些问题其实都可以归咎于优化问题。</p><p>在本文中，作者研究了ViTs和mlp-mixer的损失情况，从优化的角度理解它们，旨在减少它们对大规模预训练或强数据增强的依赖。</p><h3 id="ViTs和MLP-Mixers收敛到极sharp局部极小值">ViTs和MLP-Mixers收敛到极sharp局部极小值</h3><p>众所周知，当模型收敛到曲率小的平坦区域时模型会具有更好的泛化性能。在[36]之后，当resnet、vit和MLP-Mixers在ImageNet上使用基本的初始风格预处理从头开始训练时，作者绘制损失图：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210606/2.png" alt=""></p><p>如图1(a)到1©所示，ViTs和mlp-mixer比ResNets收敛到更清晰的区域。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210606/3.png" alt=""></p><p>在表1中，通过计算主要的Hessian特征值$\lambda_{max}$进一步验证了结果。ViT和MLP-Mixer的$\lambda_{max}$值比ResNet大一个数量级，并且MLP-Mixer的曲率在3种中是最大的(具体分析见4.4节)。</p><h3 id="Small-training-errors">Small training errors</h3><p>这种向sharp区域的收敛与图2(左)所示的训练动态一致。尽管Mixer-B/16参数少于ViT-B/16(59M vs 87M)，同时它有一个小的训练误差，但测试性能还是比较差的，这意味着使用cross-token MLP学习的相互作用比ViTs’ self-attention机制更容易过度拟合。这种差异可能解释了mlp-mixer更容易陷入尖锐的局部最小值。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210606/4.png" alt=""></p><h3 id="ViTs和MLP-Mixers的可训练性较差">ViTs和MLP-Mixers的可训练性较差</h3><p>此外，作者还发现ViTs和MLP-Mixers的可训练性较差，可训练性定义为通过梯度下降优化的网络的有效性。Xiao等人的研究表明，神经网络的可训练性可以用相关的神经切线核(NTK)的条件数来表征:</p><p>$$Θ(x,x’)= J(x)J(x’)^T$$</p><p>其中$J$是雅可比矩阵。</p><p>用$\lambda_1≥··≥\lambda_m$表示NTK $Θ_{train}$的特征值，最小的特征值$\lambda_m$以条件数κ$=\lambda_1=\lambda_m$的速率指数收敛。如果κ是发散的，那么网络将变得不可训练。如表1所示，ResNets的κ是相当稳定的，这与之前的研究结果一致，即ResNets无论深度如何都具有优越的可训练性。然而，当涉及到ViT和时，条件数是不同的MLP-Mixer，证实了对ViTs的训练需要额外的辅助。</p><h2 id="CNN-Free视觉架构优化器原理">CNN-Free视觉架构优化器原理</h2><p>常用的一阶优化器(如SGD,Adam)只寻求最小化训练损失。它们通常会忽略与泛化相关的高阶信息，如曲率。然而，深度神经网络的损失具有高度非凸性，在评估时容易达到接近0的训练误差，但泛化误差较高，更谈不上在测试集具有不同分布时的鲁棒性。</p><p>由于对视觉数据缺乏归纳偏差ViTs和MLPs放大了一阶优化器的这种缺陷，导致过度急剧的损失scene和较差的泛化性能，如前一节所示。假设平滑收敛时的损失scene可以显著提高那些无卷积架构的泛化能力，那么最近提出的锐度感知最小化(SAM)可以很好的避免锐度最小值。</p><h3 id="SAM-Overview">SAM:Overview</h3><p>从直觉上看，SAM寻找的是可以使整个邻近训练损失最低的参数w，训练损失$L_{train}$通过构造极小极大目标:</p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210606/5.png" width = "350" align=center /><p>其中$\rho$是neighbourhood ball的大小。在不失一般性的情况下，这里使用$l_2$范数作为其强经验结果，这里为了简单起见省略了正则化项。</p><p>由于内部最大化下式的确切解很难获得：</p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210606/6.png" width = "350" align=center /><p>因此，这里采用了一个有效的一阶近似:</p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210606/7.png" width = "400" align=center /><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210606/8.png" width = "350" align=center /><p>在$l_2$范数下，$\hat\epsilon(w)$是当前权值$w$的缩放梯度。计算$\hat\epsilon$后，SAM基于锐度感知梯度更新w。</p><h3 id="SAM优化器实质上改进了ViTs和MLP-Mixers">SAM优化器实质上改进了ViTs和MLP-Mixers</h3><p>作者在没有大规模的预训练或强大的数据增强的情况下训练了vit和MLP-Mixers。直接将SAM应用于vit的原始ImageNet训练pipeline，而不改变任何超参数。<br>pipeline使用了基本的Inception-style的预处理。最初的mlp-mixer的训练设置包括强数据增强的组合;也用同样的Inception-style的预处理来替换它，以便进行公平的比较。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210606/9.png" alt=""></p><p>注意，在应用SAM之前，我们对学习速率、权重衰减、Dropout和随机深度进行网格搜索。</p><h4 id="局部极小值周围的平滑区域">局部极小值周围的平滑区域</h4><p>由于SAM, ViTs和mlp-mixer都汇聚在更平滑的区域，如图1(d)和1(e)所示。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210606/10.png" alt=""></p><p>曲率测量，即Hessian矩阵的最大特征值$\lambda_{max}$，也减小到一个小值(见表1)。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210606/11.png" alt=""></p><h4 id="Higher-accuracy">Higher accuracy</h4><p>随之而来的是对泛化性能的极大改进。在ImageNet验证集上，SAM将ViT-B/16的top-1精度从74.6%提高到79.9%，将Mixer-B/16的top-1精度从66.4%提高到77.4%。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210606/12.png" alt=""></p><p>相比之下，类似规模的ResNet-152的性能提高了0.8%。根据经验，<strong>改进的程度与架构中内置的归纳偏差水平呈负相关</strong>。与基于注意力的ViTs相比，具有inherent translation equivalence和locality的ResNets从landscape smoothing中获益较少。MLP-Mixers从平滑的loss geometry中获得最多。</p><p>此外，SAM对更大容量(例如:+4.1%的Mixer-S/16 vs. +11.0%的Mixer-B/16)和更长的patch序列(例如:+2.1%的vits/32 vs. +5.3%的vits /8)的模型带来了更大的改进。</p><h4 id="Better-robustness">Better robustness</h4><p>作者还使用ImageNet-R和ImageNetC评估了模型的鲁棒性，并发现了smoothed loss landscapes的更大影响。在ImageNet-C上，它通过噪音、恶劣天气、模糊等来破坏图像，实验了5种严重程度上19种破坏的平均精度。如表1和表2所示， ViT-B/16和Mixer-B/16的精度分别增加了9.9%和15.0%。</p><h3 id="无需预训练或强大的数据增强ViTs优于ResNets">无需预训练或强大的数据增强ViTs优于ResNets</h3><p>模型体系结构的性能通常与训练策略合并，其中数据增强起着关键作用。然而，数据增广的设计需要大量的领域专业知识，而且可能无法在图像和视频之间进行转换。由于有了锐度感知优化器SAM，可以删除高级的数据增强，并专注于体系结构本身(使用基本的Inception-style的预处理)。</p><p>当使用SAM在ImageNet上从0开始训练时，ViT的准确性(在ImageNet、ImageNet-Real和ImageNet V2上)和健壮性(在ImageNet-R和ImageNet-R上)方面都优于类似和更大的ResNet(在推理时也具有相当的吞吐量)。</p><p>ViT-B/16在ImageNet、ImageNet-r和ImageNet-C上分别达到79.9%、26.4%和56.6%的top精度，而对应的ResNet-152则分别达到79.3%、25.7%和52.2%(见表2)。对于小型架构，vit和resnet之间的差距甚至更大。<br>在ImageNet上，ViT-S/16的表现比同样大小的ResNet-50好1.4%，在ImageNet-C上好6.5%。SAM还显著改善了MLP-Mixers的结果。</p><h3 id="SAM后的内在变化">SAM后的内在变化</h3><p>作者对模型进行了更深入的研究，以理解它们如何从本质上改变以减少Hessian的特征值$\lambda_{max}$以及除了增强泛化之外的变化意味着什么。</p><h4 id="结论1：每个网络组件具有Smoother-loss-landscapes">结论1：每个网络组件具有Smoother loss landscapes</h4><p>在表3中，将整个体系结构的Hessian分解成与每一组参数相关的小的斜对角Hessian块，试图分析在没有SAM训练的模型中，是什么特定的成分导致$\lambda_{max}$爆炸。</p><p>作者观察到较浅的层具有较大的Hessian特征值$\lambda_{max}$，并且第1个linear embedding layer产生sharpest的几何形状。</p><p>此外，ViTs中的多头自注意(MSA)和MLP-Mixers中的token mlp(Token mlp)跨空间位置混合信息，其$\lambda_{max}$相对较低。SAM一致地降低了所有网络块的$\lambda_{max}$。</p><p>可以通过递归mlp的Hessian矩阵得到上述发现。设$h_k$和$a_k$分别为第k层激活前的值和激活后的值。它们满足$h_k=W_ka_k−1,a_k=f_k(h_k)$，其中$W_k$为权值矩阵，$f_k$为激活函数(mlp-mixer中的GELU)。为了简单起见，在这里省略偏置项。Hessian矩阵$H_k$相对于$W_k$的对角块可递归计算为:</p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210606/19.png" width = "500" align=center /><p>其中⊗为Kronecker product，$H_k$为第$k$层的预激活Hessian，L为目标函数。因此，当递归公式反向传播到浅层时，Hessian范数累积，这也解释了为什么表3中第一个块的$\lambda_{max}$比最后一个块大得多。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210606/13.png" alt=""></p><h4 id="结论2：Greater-weight-norms">结论2：Greater weight norms</h4><p>应用SAM后，作者发现激活后的值$a_{k−1}$的范数和权重$W_{k+1}$的范数变得更大(见表3)，说明常用的权重衰减可能不能有效地正则化ViTs和MLP-Mixers。</p><h4 id="结论3：MLP-Mixers中较稀疏的active-neurons">结论3：MLP-Mixers中较稀疏的active neurons</h4><p>根据递归公式(3)到(4)，作者确定了另一个影响Hessian的MLP-Mixers的内在度量:激活神经元的数量。</p><p>事实上，$B_k$是由大于零的被激活神经元决定的，因为当输入为负时，GELU的一阶导数变得非常小。因此，活跃的GELU神经元的数量直接与Hessian规范相连。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210606/14.png" alt=""></p><p>图2(右)显示了每个块中被激活的神经元的比例，使用ImageNet训练集的10%进行计算。可以看到，SAM极大地减少了前几层被激活神经元的比例，使它们处于更稀疏的状态。这一结果也说明了图像patch的潜在冗余性。</p><h4 id="结论4：ViTs的active-neurons高度稀疏">结论4：ViTs的active neurons高度稀疏</h4><p>虽然公式(3)和(4)只涉及mlp，但仍然可以观察到vit的第1层激活神经元的减少(但不如MLP-Mixers显著)。更有趣的是，作者发现ViT中被激活神经元的比例比ResNets或MLP-Mixers中要小得多——在大多数ViT层中，只有不到5%的神经元的值大于零。换句话说，ViT为网络修剪提供了巨大的潜力。</p><p>这种稀疏性也可以解释<strong>为什么一个Transformer可以处理多模态信号(视觉、文本和音频)?</strong></p><h4 id="结论5：ViTs中有更多的感知注意力Maps">结论5：ViTs中有更多的感知注意力Maps</h4><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210606/15.png" alt=""></p><p>在图3中可视化了classification token的attention map。有趣的是，经过SAM优化的ViT模型能够编码合理的分割信息，比传统SGD优化训练的模型具有更好的可解释性。</p><h4 id="结论6：Higher-training-errors">结论6：Higher training errors</h4><p>如图2(左)所示，使用SAM的ViT-B/16比使用vanilla SGD的训练误差更高。当在训练中使用强数据增强时，这种正则化效应也会发生，它迫使网络显式地学习RandAugment中的旋转平移等方差和mixup中的线性插值等先验。然而，增益对不同的训练设置很敏感(第5.2节)，并导致高噪声损失曲线(图2(中间))。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210606/16.png" alt=""></p><h2 id="实验">实验</h2><p>具有smoother loss geometry的ViTs和MLP-Mixers可以更好地迁移到下游任务。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210606/17.png" alt=""></p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210606/18.png" alt=""></p><h2 id="参考">参考</h2><p>[1].When Vision Transformers Outperform ResNets without Pretraining or Strong Data Augmentations.<br></p>]]></content>
      
      
      <categories>
          
          <category> Transformer </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ResNet </tag>
            
            <tag> Transformer </tag>
            
            <tag> Tricks </tag>
            
            <tag> 图像分类 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>多域自适应MSDA-YOLO解读，恶劣天气也看得见</title>
      <link href="/2021/06/04/6/"/>
      <url>/2021/06/04/6/</url>
      
        <content type="html"><![CDATA[<br><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210604/1.png" alt=""></p><blockquote><p>本文介绍了一种新的多尺度域自适应YOLO(MS-DAYOLO)框架，该框架在YOLOv4检测器的不同尺度上使用多个域自适应路径和相应的域分类器来生成域不变特征。</p></blockquote><h2 id="简介">简介</h2><p>Domain Adaptation在解决许多应用中遇到的Domain Shift问题方面发挥了重要作用。这个问题的出现是由于用于训练的源数据的分布与实际测试场景中使用的目标数据之间存在差异。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210604/2.png" alt=""></p><p>本文介绍了一种新的多尺度域自适应YOLO(MS-DAYOLO)框架，该框架在YOLOv4检测器的不同尺度上使用多个域自适应路径和相应的域分类器来生成域不变特征。实验表明，当使用本文提出的MS-DAYOLO训练YOLOv4时，以及在自动驾驶应用中具有挑战性的天气条件的目标数据上进行测试时，目标检测性能得到了显著改善。</p><h2 id="方法">方法</h2><h3 id="YOLO-V4简述">YOLO V4简述</h3><p>相对于YOLO V3，YOLOv4包含了许多新的改进和新技术，以提高整体检测精度。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210604/3.png" alt=""></p><p>如图所示YOLOv4有3个主要部分:backbone、neck和head。</p><p>backbone负责提取不同尺度下的多层特征。</p><p>neck使用上采样层将backbone的3种不同尺度的特征聚集在一起，并将它们输入head。</p><p>最后，head预测目标周围的边界框以及与每个边界框相关联的类别概率。</p><p>本文作者的目标是将域适应应用于这3个特征（图中的F1、F2、F3），使它们对不同尺度的域变化具有鲁棒性，从而使它们在基于域适应的训练中向域不变性收敛。</p><h3 id="Domain-Adaptive-Network-for-YOLO">Domain Adaptive Network for YOLO</h3><p>提出的域自适应网络(DAN)仅在训练时附加到YOLOv4中以学习域不变特征。对于推理，在推理阶段，将使用原始的YOLOv4体系结构中使用领域自适应训练的权重(没有DAN网络)。因此，本文所提出的框架不会增加推理过程中底层检测器的复杂性。</p><p>DAN使用backbone的3个不同的尺度特征作为输入。它有几个卷积层来预测域类。然后，利用二元交叉熵计算域分类损失(Ldc):</p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210604/4.png" width = "400" align=center /><p>这里$t_i$为第$i$个训练图像的ground truth域标签，其中$t_i = 1$为源域，$t_i = 0$为目标域。$P^{(x,y)}$是第$i$个训练图像在位置$(x,Y)$的特征图。</p><p>DAN通过最小化这种上述损失来区分源域和目标域。另一方面，为了最大限度地学习域不变特征，对主干也进行了优化。因此，对于这2个域，backbone的特征应该是难以区分的。因此，这将提高目标域的目标检测性能。</p><p>为了解决联合最小化和最大化问题，作者采用了对抗学习策略。通过在backbone网络和DAN网络之间使用梯度反转层(GRL)来实现这个矛盾的目标。</p><p>GRL是一个双向算子，用于实现2个不同的优化目标。在前向传播方向上，GRL作为恒等算子。这导致了在DAN内执行局部反向传播时最小化分类错误的标准目标。另一方面，向主干网络反向传播时，GRL变成一个负标量$(\lambda)$。因此，在这种情况下，它会导致最大的二分类错误，这种最大化促进了由backbone生成领域不变特征。</p><p>为了计算检测损失(ldt)，只使用源图像。因此，通过最小化ldt, YOLOv4的所有3个部分(即backbone, neck和head)都得到了优化。另一方面，利用源标记图像和目标未标记图像计算域分类损失(Ldc)，Ldc通过最小化来优化DAN, Ldc通过最大化来优化backbone。因此，Ldet和Ldc都被用来优化backbone。换句话说，通过最小化以下总损失，backbone被优化了：</p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210604/5.png" width = "300" align=center /><p>其中$(\lambda)$是GRL的一个负标量，用来平衡检测损失和域分类损失。事实上，$(\lambda)$是用来优化DAN对backbone的影响。</p><h3 id="DAN-Architecture">DAN Architecture</h3><p>与在Domain Adaptive Faster R-CNN架构中只对特征提取器的最终尺度应用域自适应不同，本文分别开发了3个尺度的域自适应来解决梯度消失问题。也就是说，只对最终的尺度(F3)进行域自适应，由于之前的尺度(F1和F2)之间有很多层，存在梯度消失的问题，因此对之前的尺度(F1和F2)没有显著影响。</p><p>因此，作者采用了一个多尺度策略，将主干的三个特征F1、F2和F3通过三个相应的grl连接到DAN，如图2所示。对于每个尺度，GRL之后有2个卷积层，第1个卷积层将特征通道减少一半，第2个卷积层预测域类概率。最后，利用域分类器层计算域分类损失。</p><h2 id="实验">实验</h2><h3 id="Clear-Foggy">Clear=&gt;Foggy</h3><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210604/6.png" width = "500" align=center /><p>从这些结果可以看出，将域自适应应用于所有3个特征尺度提高了目标域的检测性能，取得了最好的结果。此外，作者提出的MS-DAYOLO在性能上大大优于原来的YOLOv4方法，几乎达到了理想(oracle)场景的性能。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210604/7.png" alt=""></p><h3 id="Sunny-Rainy">Sunny=&gt;Rainy</h3><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210604/8.png" width = "500" align=center /><p>结果如表2所示。在2个数据集中，本文的方法都比原始的YOLO得到了明显的性能提升。</p><h2 id="参考">参考</h2><p>[1].Multiscale Domain Adaptive YOLO for Cross-Domain Object Detection<br></p>]]></content>
      
      
      <categories>
          
          <category> YOLO </category>
          
      </categories>
      
      
        <tags>
            
            <tag> YOLO </tag>
            
            <tag> 目标检测 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>没有Attention的Transformer依然是顶流！！！</title>
      <link href="/2021/06/02/5/"/>
      <url>/2021/06/02/5/</url>
      
        <content type="html"><![CDATA[<br><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210602/1.png" alt=""></p><blockquote><p>本文主要介绍了Attention Free Transformer(AFT)，同时作者还引入了AFT-local和AFT-Conv，这两个模型在保持全局连通性的同时，利用了局域性和空间权重共享的思想。通过实验验证了AFT在所有benchmarks上具有竞争性能的同时具有出色的效率。</p></blockquote><h2 id="简介">简介</h2><p>本文主要介绍了Attention Free Transformer(AFT)，在AFT层中，首先将key和value与一组学习到的位置偏差结合起来，然后以元素方式将其结果与query相乘。这个新的操作在context size和特征维度上都具有线性的内存复杂度，使得它能够兼容大的输入和模型大小。</p><p>作者还引入了AFT-local和AFT-Conv，这两个模型变种在保持全局连通性的同时还利用了局域性和空间权重共享的思想。作者对2个自回归建模任务(CIFAR10和Enwik8)以及一个图像识别任务(ImageNet-1K分类)进行了广泛的实验。验证了AFT在所有benchmarks上不仅具有不错的性能，同时还具有出色的效率。</p><h2 id="本文方法">本文方法</h2><h3 id="Attention-Free-Transformer">Attention Free Transformer</h3><p>首先，定义了Attention Free Transformer(AFT)，它是MHA的plugin replacement，不需要改变Transformer的其他架构。给定输入X, AFT先将它们线性变换为$Q=XW^Q$,$K=XW^K$,$V=XW^V$，然后执行以下操作:</p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210602/2.png" width = "500" align=center /><p>其中，$\bigodot$是元素的乘积;$\sigma_q$是应用于query的非线性映射，默认为sigmoid;$w\in R^{T\times T}$是学习到成对的位置偏差。</p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210602/3.png" width = "500" align=center /><p>换句话说，对于每个目标位置$t$, AFT把加权平均的结果与具有元素级乘法的query相结合。而加权操作则是由key和一组学习成对的位置偏差组成。这提供了一个直接的优势，即不需要计算和存储消耗大的注意力矩阵，同时能够像MHA那样维护query和value之间的全局交互。</p><p>为了进一步了解AFT与MHA的关系可以将方程改写为：</p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210602/4.png" width = "400" align=center /><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210602/5.png" width = "200" align=center /><p>这里使用上标$i$来索引矩阵的特征维数。在这种重新排列的形式中，能够再次用注意力来表达AFT。具体来说，对于每个位置有一个关注向量$a_t^i\in R^T$，每个维度由$Q、K、w$组成。换句话说，AFT可以解释为与特征尺寸一样多的Head中进行implicit attention，其中注意力矩阵采用因数分解的形式进行求解。</p><h3 id="AFT-variants-locality-weight-sharing-and-parameterization">AFT variants: locality, weight sharing and parameterization</h3><h4 id="AFT-full">AFT-full</h4><p>将下面方程中定义的AFT的基本版本表示为AFT-full：</p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210602/6.png" width = "500" align=center /><h4 id="AFT-local">AFT-local</h4><p>作者发现了训练的标准Transformers倾向于表现出广泛的局部注意力模式。具体地说，把ImagenetNet预训练Vision Transformer(ViT)，由12层组成，每层6个Head。为了实现可视化忽略分类标记，将每一层的注意力张量reshape为6×196×196(因为ViT特征图的空间大小为14×14)。然后从ImageNet验证集中采样256张图像。对于每一层和每一个Head，计算平均的average relative 2d attentions、averaged across position和images。这就产生了一组尺寸为12×6×27×27的注意力map（如下图）。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210602/7.png" alt=""></p><p>通过上图可以看到，相对注意力Map显示出强烈的局部模式，特别是在lower layers。这激发了AFT的一种变体，称为<strong>AFT-local</strong>，即只在局部应用一组学习到的相对位置偏差:</p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210602/8.png" width = "400" align=center /><p>这里s≤T是一个局部window size。AFT-local提供了进一步的计算量的节省，包括参数的数量和时间/空间复杂度。</p><h4 id="AFT-simple">AFT-simple</h4><p>AFT-local的一个极端形式是当s=0时，即没有学习到位置偏差。这就产生了一个极其简单的AFT版本，<strong>AFT-simple</strong>，有:</p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210602/9.png" width = "300" align=center /><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210602/10.png" width = "300" align=center /><p>在这个版本中，context reduction进一步简化为元素操作和全局池化。其实AFT-simple类似于线性化注意，公式为：</p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210602/11.png" width = "400" align=center /><p>然而，AFT-simple完全摆脱了点积操作，这促使复杂度从$O(Td^2)$降低为$O(Td)$。</p><h4 id="AFT-conv">AFT-conv</h4><p>作者还可以进一步扩展局部化locality的思想，<strong>加入空间权值共享</strong>，即<strong>卷积</strong>。这种变体与视觉任务特别相关，因为它通常希望将一个预训练模型扩展到可变大小的输入。具体来说，让$w_{t,t’}$的值只依赖于$t$和$t’$, 而$w.r.t.$为在给定的空间网格(1d或2d)中的相对位置。与CNN类似也可以学习多组位置偏差(重用head的概念作为参考)。为了考虑到#parameters随着 #heads的增加而增长，作者还采用了一个设计，将K的维度与#heads联系起来。这使得AFT-conv可修改为依赖于深度可分离卷积、全局池化和元素操作来实现。</p><p>类似的尺寸的AFT-conv学习到的相对位置偏差。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210602/12.png" alt=""></p><p>举一个例子，这里将模型构型表示为AFT-conv-h-s，其中h为head的个数，s×s为2d local window size。$w\in R^{h\times s\times s}, Q,V\in R^{T\times h\times d/h}, K\in R^{T\times h}$，于是对于每一个head $i=1,2,…,h$来说，有：</p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210602/13.png" width = "500" align=center /><p>注意，上式可以很容易地解释为一个特殊的卷积层，具有：</p><ol><li><p><strong>全局连通性</strong></p></li><li><p><strong>非负卷积权值</strong></p></li><li><p><strong>复杂的除法/乘法门机制</strong></p></li></ol><p>实验表明，这3个方面对AFT-conv的性能都有显著的影响。</p><h4 id="Parameterization">Parameterization</h4><p>根据经验，作者发现适当地参数化位置偏差是很重要的。</p><p>对于AFT-full和AFT-local，采用w的因数分解形式:</p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210602/14.png" width = "400" align=center /><p>其中$d’$是一个小的嵌入维数(例如128)。这种简单的因式分解不仅大大减少了参数量，而且在训练和测试中都有效地提高了模型的性能。</p><p>对于AFT-conv，因式分解的技巧并不适用。相反，作者采用一个简单的重新参数化，对于每个head i，让：</p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210602/15.png" width = "300" align=center /><p>其中$\gamma\in R^h, \beta \in R^h$是可学习增益和偏置参数，均初始化为0。</p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210602/16.png" width = "500" align=center /><h2 id="实验">实验</h2><h3 id="Image-Autoregressive-Modeling">Image Autoregressive Modeling</h3><h4 id="SOTA模型对比">SOTA模型对比</h4><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210602/17.png" alt=""></p><h4 id="Factorization的影响">Factorization的影响</h4><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210602/18.png" width = "500" align=center /><h3 id="Language-Modeling">Language Modeling</h3><h4 id="SOTA模型对比-2">SOTA模型对比</h4><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210602/19.png" alt=""></p><h4 id="local-window-size的影响">local window size的影响</h4><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210602/20.png" alt=""></p><h4 id="Longer-sequence-size">Longer sequence size</h4><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210602/21.png" alt=""></p><h3 id="Image-Classification">Image Classification</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210602/22.png" alt=""></p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210602/23.png" alt=""></p><h2 id="参考">参考</h2><p>[1].An Attention Free Transformer<br></p>]]></content>
      
      
      <categories>
          
          <category> Transformer </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Tansformer </tag>
            
            <tag> Attention </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>从零开始边缘部署轻量化人脸检测模型————EAIDK310部署篇</title>
      <link href="/2021/05/30/4/"/>
      <url>/2021/05/30/4/</url>
      
        <content type="html"><![CDATA[<br><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210527/0.png" alt=""></p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210527/1.png" alt=""></p><blockquote><p>继续上一章的话题，前面我们主要聊到关于人脸检测模型UltraFace的训练任务，本文将和大家讨论在开发板上如何部署UltraFace模型，并进行实时视频人脸检测，或者图片流人脸检测。</p></blockquote><h2 id="Tengine简介">Tengine简介</h2><p>Tengine 由 OPEN AI LAB 主导开发，该项目实现了深度学习神经网络模型在嵌入式设备上的快速、高效部署需求。为实现在众多 AIoT 应用中的跨平台部署，本项目基于原有 Tengine 项目使用 C 语言进行重构，针对嵌入式设备资源有限的特点进行了深度框架裁剪。同时采用了完全分离的前后端设计，有利于 CPU、GPU、NPU 等异构计算单元的快速移植和部署，同时降低评估和迁移成本。</p><h3 id="Tengine推理流程">Tengine推理流程</h3><p>依照顺序调用Tengine核心API如下：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210530/1.png" alt=""></p><h2 id="模块实现">模块实现</h2><h3 id="模型转换">模型转换</h3><h4 id="第1步：转换到onnx模型">第1步：转换到onnx模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">model_path = <span class="string">&quot;models/pretrained/version-RFB-320.pth&quot;</span></span><br><span class="line">net = create_Mb_Tiny_RFB_fd(<span class="built_in">len</span>(class_names), is_test=<span class="literal">True</span>)</span><br><span class="line">net.load(model_path)</span><br><span class="line">net.<span class="built_in">eval</span>()</span><br><span class="line">net.to(<span class="string">&quot;cuda&quot;</span>)</span><br><span class="line"></span><br><span class="line">model_name = model_path.split(<span class="string">&quot;/&quot;</span>)[-<span class="number">1</span>].split(<span class="string">&quot;.&quot;</span>)[<span class="number">0</span>]</span><br><span class="line">model_path = <span class="string">f&quot;models/onnx/<span class="subst">&#123;model_name&#125;</span>.onnx&quot;</span></span><br><span class="line"></span><br><span class="line">dummy_input = torch.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">240</span>, <span class="number">320</span>).to(<span class="string">&quot;cuda&quot;</span>)</span><br><span class="line">torch.onnx.export(net, dummy_input, model_path, verbose=<span class="literal">False</span>, input_names=[<span class="string">&#x27;input&#x27;</span>], output_names=[<span class="string">&#x27;scores&#x27;</span>, <span class="string">&#x27;boxes&#x27;</span>])</span><br></pre></td></tr></table></figure><h4 id="第2步：编译Tengine模型转换工具">第2步：编译Tengine模型转换工具</h4><p>依赖库安装</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install libprotobuf-dev protobuf-compiler</span><br></pre></td></tr></table></figure><p>源码编译</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir build &amp;&amp; cd build</span><br><span class="line">cmake ..</span><br><span class="line">make -j`nproc` &amp;&amp; make install</span><br></pre></td></tr></table></figure><p>编译完成后，生成的可行性文件tm_convert_tool存放在 ./build/install/bin/ 目录下。</p><h4 id="第3步：转换onnx模型为tmfile模型">第3步：转换onnx模型为tmfile模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./tm_convert_tool -m xxx.onnx -o xxx.tmfile</span><br></pre></td></tr></table></figure><ul><li>-m 为*.caffemodel, *.params, *.weight, *.pb, *.onnx, *.tflite等模型；</li><li>-o 为output fp32 tmfile</li></ul><h3 id="NMS计算">NMS计算</h3><h3 id="伪代码：">伪代码：</h3><ul><li><p>1 将各组box按照score降序排列;</p></li><li><p>2 从score最大值开始，置为当前box，保存idex，然后依次遍历后面的box，计算与当前box的IOU值，若大于阈值，则抑制，不会输出;</p></li><li><p>3 完成一轮遍历后，继续选择下一个非抑制的box作为当前box，重复步骤2;</p></li><li><p>4 返回没有被抑制的index即符合条件的box;</p></li></ul><h3 id="python版本">python版本</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">NMS</span>(<span class="params">dects,threshhold</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    detcs:二维数组(n_samples,5)</span></span><br><span class="line"><span class="string">    5列：x1,y1,x2,y2,score</span></span><br><span class="line"><span class="string">    threshhold: IOU阈值</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    x1=dects[:,<span class="number">0</span>]</span><br><span class="line">    y1=dects[:,<span class="number">1</span>]</span><br><span class="line">    x2=dects[:,<span class="number">2</span>]</span><br><span class="line">    y2=dects[:,<span class="number">3</span>]</span><br><span class="line">    score=dects[:,<span class="number">4</span>]</span><br><span class="line">    ndects=dects.shape[<span class="number">0</span>]<span class="comment">#box的数量</span></span><br><span class="line">    area=(x2-x1+<span class="number">1</span>)*(y2-y1+<span class="number">1</span>)</span><br><span class="line">    order=score.argsort()[::-<span class="number">1</span>] <span class="comment">#score从大到小排列的indexs,一维数组</span></span><br><span class="line">    keep=[] <span class="comment">#保存符合条件的index</span></span><br><span class="line">    suppressed=np.array([<span class="number">0</span>]*ndects) <span class="comment">#初始化为0，若大于threshhold,变为1，表示被抑制</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> _i <span class="keyword">in</span> <span class="built_in">range</span>(ndects):</span><br><span class="line">        i=order[_i]  <span class="comment">#从得分最高的开始遍历</span></span><br><span class="line">        <span class="keyword">if</span> suppressed[i]==<span class="number">1</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        keep.append(i) </span><br><span class="line">        <span class="keyword">for</span> _j <span class="keyword">in</span> <span class="built_in">range</span>(i+<span class="number">1</span>,ndects):</span><br><span class="line">            j=order[_j]</span><br><span class="line">            <span class="keyword">if</span> suppressed[j]==<span class="number">1</span>: <span class="comment">#若已经被抑制，跳过</span></span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            xx1=np.<span class="built_in">max</span>(x1[i],x1[j])<span class="comment">#求两个box的交集面积interface</span></span><br><span class="line">            yy1=np.<span class="built_in">max</span>(y1[i],y1j])</span><br><span class="line">            xx2=np.<span class="built_in">min</span>(x2[i],x2[j])</span><br><span class="line">            yy2=np.<span class="built_in">min</span>(y2[i],y2[j])</span><br><span class="line">            w=np.<span class="built_in">max</span>(<span class="number">0</span>,xx2-xx1+<span class="number">1</span>)</span><br><span class="line">            h=np.<span class="built_in">max</span>(<span class="number">0</span>,yy2-yy1+<span class="number">1</span>)</span><br><span class="line">            interface=w*h</span><br><span class="line">            overlap=interface/(area[i]+area[j]-interface) <span class="comment">#计算IOU（交/并）</span></span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> overlap&gt;=threshhold:<span class="comment">#IOU若大于阈值，则抑制</span></span><br><span class="line">                suppressed[j]=<span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> keep</span><br></pre></td></tr></table></figure><h3 id="C-版本">C++版本</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">UltraFace::nms</span><span class="params">(std::vector&lt;FaceInfo&gt; &amp;input, std::vector&lt;FaceInfo&gt; &amp;output, <span class="keyword">int</span> type)</span> </span>&#123;</span><br><span class="line">    <span class="comment">//根据score对候选框进行 sort 排序操作</span></span><br><span class="line">    std::<span class="built_in">sort</span>(input.<span class="built_in">begin</span>(), input.<span class="built_in">end</span>(), [](<span class="keyword">const</span> FaceInfo &amp;a, <span class="keyword">const</span> FaceInfo &amp;b) &#123; <span class="keyword">return</span> a.score &gt; b.score; &#125;);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> box_num = input.<span class="built_in">size</span>();</span><br><span class="line"></span><br><span class="line">    <span class="function">std::vector&lt;<span class="keyword">int</span>&gt; <span class="title">merged</span><span class="params">(box_num, <span class="number">0</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; box_num; i++) &#123;</span><br><span class="line">        <span class="keyword">if</span> (merged[i])</span><br><span class="line">            <span class="keyword">continue</span>;</span><br><span class="line">        std::vector&lt;FaceInfo&gt; buf;</span><br><span class="line"></span><br><span class="line">        buf.<span class="built_in">push_back</span>(input[i]);</span><br><span class="line">        merged[i] = <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">float</span> h0 = input[i].y2 - input[i].y1 + <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">float</span> w0 = input[i].x2 - input[i].x1 + <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">float</span> area0 = h0 * w0;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j = i + <span class="number">1</span>; j &lt; box_num; j++) &#123;</span><br><span class="line">            <span class="keyword">if</span> (merged[j])</span><br><span class="line">                <span class="keyword">continue</span>;</span><br><span class="line">            <span class="comment">//确立每个候选框的坐标以及宽高</span></span><br><span class="line">            <span class="keyword">float</span> inner_x0 = input[i].x1 &gt; input[j].x1 ? input[i].x1 : input[j].x1;</span><br><span class="line">            <span class="keyword">float</span> inner_y0 = input[i].y1 &gt; input[j].y1 ? input[i].y1 : input[j].y1;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">float</span> inner_x1 = input[i].x2 &lt; input[j].x2 ? input[i].x2 : input[j].x2;</span><br><span class="line">            <span class="keyword">float</span> inner_y1 = input[i].y2 &lt; input[j].y2 ? input[i].y2 : input[j].y2;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">float</span> inner_h = inner_y1 - inner_y0 + <span class="number">1</span>;</span><br><span class="line">            <span class="keyword">float</span> inner_w = inner_x1 - inner_x0 + <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (inner_h &lt;= <span class="number">0</span> || inner_w &lt;= <span class="number">0</span>)</span><br><span class="line">                <span class="keyword">continue</span>;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">float</span> inner_area = inner_h * inner_w;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">float</span> h1 = input[j].y2 - input[j].y1 + <span class="number">1</span>;</span><br><span class="line">            <span class="keyword">float</span> w1 = input[j].x2 - input[j].x1 + <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">float</span> area1 = h1 * w1;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">float</span> score;</span><br><span class="line">            <span class="comment">//计算IOU</span></span><br><span class="line">            score = inner_area / (area0 + area1 - inner_area);</span><br><span class="line">            <span class="comment">//根据阈值进行极大值抑制的筛选</span></span><br><span class="line">            <span class="keyword">if</span> (score &gt; iou_threshold) &#123;</span><br><span class="line">                merged[j] = <span class="number">1</span>;</span><br><span class="line">                buf.<span class="built_in">push_back</span>(input[j]);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="获取候选框">获取候选框</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//获取候选框</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">UltraFace::generateBBox</span><span class="params">(std::vector&lt;FaceInfo&gt; &amp;bbox_collection, <span class="keyword">tensor_t</span> scores, <span class="keyword">tensor_t</span> boxes)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">float</span>* scores_blob = ( <span class="keyword">float</span>* )<span class="built_in">get_tensor_buffer</span>(scores);</span><br><span class="line">    <span class="keyword">float</span>* boxes_blob = ( <span class="keyword">float</span>* )<span class="built_in">get_tensor_buffer</span>(boxes);</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; num_anchors; i++) &#123;</span><br><span class="line">        <span class="keyword">if</span> (scores_blob[i * <span class="number">2</span> + <span class="number">1</span>] &gt; score_threshold) &#123;</span><br><span class="line">            FaceInfo rects;</span><br><span class="line">            <span class="comment">//确定坐标中心以及box的宽高</span></span><br><span class="line">            <span class="keyword">float</span> x_center = boxes_blob[i * <span class="number">4</span>] * center_variance * priors[i][<span class="number">2</span>] + priors[i][<span class="number">0</span>];</span><br><span class="line">            <span class="keyword">float</span> y_center = boxes_blob[i * <span class="number">4</span> + <span class="number">1</span>] * center_variance * priors[i][<span class="number">3</span>] + priors[i][<span class="number">1</span>];</span><br><span class="line">            <span class="keyword">float</span> w = <span class="built_in">exp</span>(boxes_blob[i * <span class="number">4</span> + <span class="number">2</span>] * size_variance) * priors[i][<span class="number">2</span>];</span><br><span class="line">            <span class="keyword">float</span> h = <span class="built_in">exp</span>(boxes_blob[i * <span class="number">4</span> + <span class="number">3</span>] * size_variance) * priors[i][<span class="number">3</span>];</span><br><span class="line">            <span class="comment">//截取坐标结果</span></span><br><span class="line">            rects.x1 = <span class="built_in">clip</span>(x_center - w / <span class="number">2.0</span>, <span class="number">1</span>) * image_w;</span><br><span class="line">            rects.y1 = <span class="built_in">clip</span>(y_center - h / <span class="number">2.0</span>, <span class="number">1</span>) * image_h;</span><br><span class="line">            rects.x2 = <span class="built_in">clip</span>(x_center + w / <span class="number">2.0</span>, <span class="number">1</span>) * image_w;</span><br><span class="line">            rects.y2 = <span class="built_in">clip</span>(y_center + h / <span class="number">2.0</span>, <span class="number">1</span>) * image_h;</span><br><span class="line">            rects.score = <span class="built_in">clip</span>(scores_blob[i * <span class="number">2</span> + <span class="number">1</span>], <span class="number">1</span>);</span><br><span class="line">            bbox_collection.<span class="built_in">push_back</span>(rects);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="模型检测函数">模型检测函数</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//模型检测函数</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">UltraFace::detect</span><span class="params">(cv::Mat &amp;raw_image, std::vector&lt;FaceInfo&gt; &amp;face_list)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (raw_image.<span class="built_in">empty</span>()) &#123;</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;image is empty ,please check!&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    image_h = raw_image.rows;</span><br><span class="line">    image_w = raw_image.cols;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> img_size      = in_w * in_h * <span class="number">3</span>;</span><br><span class="line">    <span class="keyword">float</span>* input_data = ( <span class="keyword">float</span>* )<span class="built_in">malloc</span>(img_size * <span class="built_in"><span class="keyword">sizeof</span></span>(<span class="keyword">float</span>));</span><br><span class="line">    <span class="comment">// 获取来自opencv读取的图片或者视频数据，并返回一个适应模型输入的结果</span></span><br><span class="line">    <span class="built_in">get_input_data_cv</span>(raw_image, input_data, in_w, in_h, mean_vals, norm_vals, <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">set_tensor_buffer</span>(input_tensor, input_data, (in_w * in_h * <span class="number">3</span>) * <span class="number">4</span>) &lt; <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Set input tensor buffer failed\n&quot;</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//开始计时⏲</span></span><br><span class="line">    <span class="keyword">auto</span> start = chrono::steady_clock::<span class="built_in">now</span>();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 6、Run网络</span></span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">run_graph</span>(graph, <span class="number">1</span>) &lt; <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Run graph failed\n&quot;</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取输出结果</span></span><br><span class="line">    string scores = <span class="string">&quot;scores&quot;</span>;</span><br><span class="line">    string boxes = <span class="string">&quot;boxes&quot;</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//7.1、获取分类得分结果</span></span><br><span class="line">    <span class="keyword">tensor_t</span> tensor_scores = <span class="built_in">get_graph_tensor</span>(graph, scores.<span class="built_in">c_str</span>());</span><br><span class="line">    <span class="comment">//7.2、获取检测框坐标结果</span></span><br><span class="line">    <span class="keyword">tensor_t</span> tensor_boxes = <span class="built_in">get_graph_tensor</span>(graph, boxes.<span class="built_in">c_str</span>());</span><br><span class="line"></span><br><span class="line">    std::vector&lt;FaceInfo&gt; bbox_collection;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//结束计时，然后计算推理时间</span></span><br><span class="line">    <span class="keyword">auto</span> end = chrono::steady_clock::<span class="built_in">now</span>();</span><br><span class="line">    chrono::duration&lt;<span class="keyword">double</span>&gt; elapsed = end - start;</span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;inference time:&quot;</span> &lt;&lt; elapsed.<span class="built_in">count</span>() &lt;&lt; <span class="string">&quot; s&quot;</span> &lt;&lt; endl;</span><br><span class="line">    <span class="comment">//后处理操作，主要是获取BBox以及NMS操作</span></span><br><span class="line">    <span class="built_in">generateBBox</span>(bbox_collection, tensor_scores, tensor_boxes);</span><br><span class="line">    <span class="built_in">nms</span>(bbox_collection, face_list);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">free</span>(input_data);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="主函数">主函数</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&quot;UltraFace.hpp&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;opencv2/opencv.hpp&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;opencv2/highgui.hpp&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;opencv2/imgproc.hpp&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    string tengine_path = <span class="string">&quot;/home/chaucer/Tengine_Tutorial/2_FaceDetector/models/version-RFB-320_simplified.tmfile&quot;</span>;</span><br><span class="line">    <span class="function">UltraFace <span class="title">ultraface</span><span class="params">(tengine_path, <span class="number">320</span>, <span class="number">240</span>, <span class="number">4</span>, <span class="number">0.65</span>)</span></span>; <span class="comment">// config model input</span></span><br><span class="line"></span><br><span class="line">    cv::Mat frame;</span><br><span class="line">    <span class="comment">//cv::VideoCapture capture(0);</span></span><br><span class="line">    <span class="function">cv::VideoCapture <span class="title">capture</span><span class="params">(<span class="string">&quot;/home/chaucer/face_detect/test_1.mp4&quot;</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//cv::Mat frame = cv::imread(image_file);</span></span><br><span class="line">    <span class="keyword">while</span>(<span class="number">1</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        capture &gt;&gt; frame;</span><br><span class="line">        <span class="keyword">auto</span> start = chrono::steady_clock::<span class="built_in">now</span>();</span><br><span class="line">        vector&lt;FaceInfo&gt; face_info;</span><br><span class="line">        ultraface.<span class="built_in">detect</span>(frame, face_info);</span><br><span class="line"></span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;face_info &quot;</span> &lt;&lt; face_info.<span class="built_in">size</span>() &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">auto</span> face : face_info) &#123;</span><br><span class="line">            <span class="function">cv::Point <span class="title">pt1</span><span class="params">(face.x1, face.y1)</span></span>;</span><br><span class="line">            <span class="function">cv::Point <span class="title">pt2</span><span class="params">(face.x2, face.y2)</span></span>;</span><br><span class="line">            cv::<span class="built_in">rectangle</span>(frame, pt1, pt2, cv::<span class="built_in">Scalar</span>(<span class="number">0</span>, <span class="number">255</span>, <span class="number">0</span>), <span class="number">2</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">auto</span> end = chrono::steady_clock::<span class="built_in">now</span>();</span><br><span class="line">        chrono::duration&lt;<span class="keyword">double</span>&gt; elapsed = end - start;</span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;all time: &quot;</span> &lt;&lt; elapsed.<span class="built_in">count</span>() &lt;&lt; <span class="string">&quot; s&quot;</span> &lt;&lt; endl;</span><br><span class="line">        cv::<span class="built_in">imshow</span>(<span class="string">&quot;UltraFace&quot;</span>, frame);</span><br><span class="line">        cv::<span class="built_in">waitKey</span>(<span class="number">1</span>);</span><br><span class="line">        string result_name = <span class="string">&quot;result&quot;</span> + <span class="built_in">to_string</span>(<span class="number">2</span>) + <span class="string">&quot;.jpg&quot;</span>;</span><br><span class="line">        cv::<span class="built_in">imwrite</span>(result_name, frame);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="输出结果">输出结果</h2><h3 id="图片检测结果">图片检测结果</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210530/2.png" alt=""></p><h2 id="参考">参考</h2><p>[1].<a href="https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB">https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB</a><br><br>[2].<a href="https://github.com/OAID/Tengine">https://github.com/OAID/Tengine</a><br><br>[3].<a href="https://github.com/jiangzhongbo/Tengine_Tutorial">https://github.com/jiangzhongbo/Tengine_Tutorial</a><br></p>]]></content>
      
      
      <categories>
          
          <category> 项目部署 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人脸识别 </tag>
            
            <tag> 人脸检测 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>从零开始边缘部署轻量化人脸检测模型————训练篇</title>
      <link href="/2021/05/27/3/"/>
      <url>/2021/05/27/3/</url>
      
        <content type="html"><![CDATA[<p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210527/0.png" alt=""></p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210527/1.png" alt=""></p><h2 id="简介">简介</h2><p>该模型是针对边缘计算设备设计的轻量人脸检测模型。</p><ul><li>在模型大小上，默认FP32精度下（.pth）文件大小为 1.04~1.1MB，推理框架int8量化后大小为 300KB 左右。</li><li>在模型计算量上，320x240的输入分辨率下 90~109 MFlops左右。</li><li>模型有两个版本，version-slim(主干精简速度略快)，version-RFB(加入了修改后的RFB模块，精度更高)。</li><li>提供320x240、640x480不同输入分辨率下使用widerface训练的预训练模型，更好的工作于不同的应用场景。</li></ul><h2 id="数据处理">数据处理</h2><h3 id="输入尺寸的选择">输入尺寸的选择</h3><p>由于涉及实际部署时的推理速度，因此模型输入尺寸的选择也是一个很重要的话题。</p><p>在作者的原github中，也提到了一点，如果在实际部署的场景中大多数情况为中近距离、人脸大同时人脸的数量也比较少的时候，则可以采用$320\times 240$的输入尺寸；</p><p>如果在实际部署的场景中大多数情况为中远距离、人脸小同时人脸的数量也比较多的时候，则可以采用$640\times 480$或者$480\times 360$的输入尺寸；</p><blockquote><p>这里由于使用的是EAIDK310进行部署测试，边缘性能不是很好，因此选择原作者推荐的最小尺寸$320\times 240$进行训练和部署测试。<br><br><strong>注意：过小的输入分辨率虽然会明显加快推理速度，但是会大幅降低小人脸的召回率。</strong></p></blockquote><h3 id="数据筛选">数据筛选</h3><p>由于widerface官网数据集中有比较多的低于10像素的人脸照片，因此在这里选择剔除这些像素长宽低于10个pixel的照片；</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210527/2.png" alt=""></p><blockquote><p>这样做的原因是：<strong>不清楚的人脸，不太利于高效模型的收敛，所以需要进行过滤训练。</strong></p></blockquote><h2 id="SSD网络结构">SSD网络结构</h2><p>SSD是一个端到端的模型，所有的检测过程和识别过程都是在同一个网络中进行的；同时SSD借鉴了Faster R-CNN的Anchor机制的想法，这样就像相当于在基于回归的的检测过程中结合了区域的思想，可以使得检测效果较定制化边界框的YOLO v1有比较好的提升。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210527/3.png" alt=""></p><p>SSD较传统的检测方法使用顶层特征图的方法选择了使用多尺度特征图，因为在比较浅的特征图中可以对于小目标有比较好的表达，随着特征图的深入，网络对于比较大特征也有了比较好表达能力，故SSD选择使用多尺度特征图可以很好的兼顾大目标和小目标。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210527/4.png" alt=""></p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210527/5.png" alt=""></p><p>SSD模型结构如下：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210527/6.png" alt=""></p><p>这里关于SSD不进行更多的阐述，想了解的小伙伴可以扫描下方的二维码查看（是小编在CSDN的记录，非常详细！！！）：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210527/7.png" alt=""></p><p>整个项目模型搭建如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 网络的主题结构为SSD模型</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SSD</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, num_classes: <span class="built_in">int</span>, base_net: nn.ModuleList, source_layer_indexes: <span class="type">List</span>[<span class="built_in">int</span>],</span></span></span><br><span class="line"><span class="params"><span class="function">                 extras: nn.ModuleList, classification_headers: nn.ModuleList,</span></span></span><br><span class="line"><span class="params"><span class="function">                 regression_headers: nn.ModuleList, is_test=<span class="literal">False</span>, config=<span class="literal">None</span>, device=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Compose a SSD model using the given components.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(SSD, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.num_classes = num_classes</span><br><span class="line">        self.base_net = base_net</span><br><span class="line">        self.source_layer_indexes = source_layer_indexes</span><br><span class="line">        self.extras = extras</span><br><span class="line">        self.classification_headers = classification_headers</span><br><span class="line">        self.regression_headers = regression_headers</span><br><span class="line">        self.is_test = is_test</span><br><span class="line">        self.config = config</span><br><span class="line"></span><br><span class="line">        <span class="comment"># register layers in source_layer_indexes by adding them to a module list</span></span><br><span class="line">        self.source_layer_add_ons = nn.ModuleList([t[<span class="number">1</span>] <span class="keyword">for</span> t <span class="keyword">in</span> source_layer_indexes</span><br><span class="line">                                                   <span class="keyword">if</span> <span class="built_in">isinstance</span>(t, <span class="built_in">tuple</span>) <span class="keyword">and</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(t, GraphPath)])</span><br><span class="line">        <span class="keyword">if</span> device:</span><br><span class="line">            self.device = device</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">        <span class="keyword">if</span> is_test:</span><br><span class="line">            self.config = config</span><br><span class="line">            self.priors = config.priors.to(self.device)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x: torch.Tensor</span>) -&gt; <span class="type">Tuple</span>[torch.Tensor, torch.Tensor]:</span></span><br><span class="line">        confidences = []</span><br><span class="line">        locations = []</span><br><span class="line">        start_layer_index = <span class="number">0</span></span><br><span class="line">        header_index = <span class="number">0</span></span><br><span class="line">        end_layer_index = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> end_layer_index <span class="keyword">in</span> self.source_layer_indexes:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(end_layer_index, GraphPath):</span><br><span class="line">                path = end_layer_index</span><br><span class="line">                end_layer_index = end_layer_index.s0</span><br><span class="line">                added_layer = <span class="literal">None</span></span><br><span class="line">            <span class="keyword">elif</span> <span class="built_in">isinstance</span>(end_layer_index, <span class="built_in">tuple</span>):</span><br><span class="line">                added_layer = end_layer_index[<span class="number">1</span>]</span><br><span class="line">                end_layer_index = end_layer_index[<span class="number">0</span>]</span><br><span class="line">                path = <span class="literal">None</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                added_layer = <span class="literal">None</span></span><br><span class="line">                path = <span class="literal">None</span></span><br><span class="line">            <span class="keyword">for</span> layer <span class="keyword">in</span> self.base_net[start_layer_index: end_layer_index]:</span><br><span class="line">                x = layer(x)</span><br><span class="line">            <span class="keyword">if</span> added_layer:</span><br><span class="line">                y = added_layer(x)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                y = x</span><br><span class="line">            <span class="keyword">if</span> path:</span><br><span class="line">                sub = <span class="built_in">getattr</span>(self.base_net[end_layer_index], path.name)</span><br><span class="line">                <span class="keyword">for</span> layer <span class="keyword">in</span> sub[:path.s1]:</span><br><span class="line">                    x = layer(x)</span><br><span class="line">                y = x</span><br><span class="line">                <span class="keyword">for</span> layer <span class="keyword">in</span> sub[path.s1:]:</span><br><span class="line">                    x = layer(x)</span><br><span class="line">                end_layer_index += <span class="number">1</span></span><br><span class="line">            start_layer_index = end_layer_index</span><br><span class="line">            confidence, location = self.compute_header(header_index, y)</span><br><span class="line">            header_index += <span class="number">1</span></span><br><span class="line">            confidences.append(confidence)</span><br><span class="line">            locations.append(location)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.base_net[end_layer_index:]:</span><br><span class="line">            x = layer(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.extras:</span><br><span class="line">            x = layer(x)</span><br><span class="line">            confidence, location = self.compute_header(header_index, x)</span><br><span class="line">            header_index += <span class="number">1</span></span><br><span class="line">            confidences.append(confidence)</span><br><span class="line">            locations.append(location)</span><br><span class="line"></span><br><span class="line">        confidences = torch.cat(confidences, <span class="number">1</span>)</span><br><span class="line">        locations = torch.cat(locations, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.is_test:</span><br><span class="line">            confidences = F.softmax(confidences, dim=<span class="number">2</span>)</span><br><span class="line">            boxes = box_utils.convert_locations_to_boxes(</span><br><span class="line">                locations, self.priors, self.config.center_variance, self.config.size_variance</span><br><span class="line">            )</span><br><span class="line">            boxes = box_utils.center_form_to_corner_form(boxes)</span><br><span class="line">            <span class="keyword">return</span> confidences, boxes</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> confidences, locations</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compute_header</span>(<span class="params">self, i, x</span>):</span></span><br><span class="line">        confidence = self.classification_headers[i](x)</span><br><span class="line">        confidence = confidence.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>).contiguous()</span><br><span class="line">        confidence = confidence.view(confidence.size(<span class="number">0</span>), -<span class="number">1</span>, self.num_classes)</span><br><span class="line"></span><br><span class="line">        location = self.regression_headers[i](x)</span><br><span class="line">        location = location.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>).contiguous()</span><br><span class="line">        location = location.view(location.size(<span class="number">0</span>), -<span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> confidence, location</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_from_base_net</span>(<span class="params">self, model</span>):</span></span><br><span class="line">        self.base_net.load_state_dict(torch.load(model, map_location=<span class="keyword">lambda</span> storage, loc: storage), strict=<span class="literal">True</span>)</span><br><span class="line">        self.source_layer_add_ons.apply(_xavier_init_)</span><br><span class="line">        self.extras.apply(_xavier_init_)</span><br><span class="line">        self.classification_headers.apply(_xavier_init_)</span><br><span class="line">        self.regression_headers.apply(_xavier_init_)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_from_pretrained_ssd</span>(<span class="params">self, model</span>):</span></span><br><span class="line">        state_dict = torch.load(model, map_location=<span class="keyword">lambda</span> storage, loc: storage)</span><br><span class="line">        state_dict = &#123;k: v <span class="keyword">for</span> k, v <span class="keyword">in</span> state_dict.items() <span class="keyword">if</span> <span class="keyword">not</span> (k.startswith(<span class="string">&quot;classification_headers&quot;</span>) <span class="keyword">or</span> k.startswith(<span class="string">&quot;regression_headers&quot;</span>))&#125;</span><br><span class="line">        model_dict = self.state_dict()</span><br><span class="line">        model_dict.update(state_dict)</span><br><span class="line">        self.load_state_dict(model_dict)</span><br><span class="line">        self.classification_headers.apply(_xavier_init_)</span><br><span class="line">        self.regression_headers.apply(_xavier_init_)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.base_net.apply(_xavier_init_)</span><br><span class="line">        self.source_layer_add_ons.apply(_xavier_init_)</span><br><span class="line">        self.extras.apply(_xavier_init_)</span><br><span class="line">        self.classification_headers.apply(_xavier_init_)</span><br><span class="line">        self.regression_headers.apply(_xavier_init_)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">load</span>(<span class="params">self, model</span>):</span></span><br><span class="line">        self.load_state_dict(torch.load(model, map_location=<span class="keyword">lambda</span> storage, loc: storage))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save</span>(<span class="params">self, model_path</span>):</span></span><br><span class="line">        torch.save(self.state_dict(), model_path)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="损失函数">损失函数</h2><p>损失函数作者选择使用的依旧是SSD的Smooth L1 Loss以及Cross Entropy Loss，其中Smooth L1 Loss用于边界框的回归，而Cross Entropy Loss则用于分类。</p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210527/8.png" width = "500" align=center /><p>具体pytorch实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiboxLoss</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, priors, neg_pos_ratio,</span></span></span><br><span class="line"><span class="params"><span class="function">                 center_variance, size_variance, device</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Implement SSD Multibox Loss.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Basically, Multibox loss combines classification loss</span></span><br><span class="line"><span class="string">         and Smooth L1 regression loss.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(MultiboxLoss, self).__init__()</span><br><span class="line">        self.neg_pos_ratio = neg_pos_ratio</span><br><span class="line">        self.center_variance = center_variance</span><br><span class="line">        self.size_variance = size_variance</span><br><span class="line">        self.priors = priors</span><br><span class="line">        self.priors.to(device)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, confidence, predicted_locations, labels, gt_locations</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Compute classification loss and smooth l1 loss.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            confidence (batch_size, num_priors, num_classes): class predictions.</span></span><br><span class="line"><span class="string">            locations (batch_size, num_priors, 4): predicted locations.</span></span><br><span class="line"><span class="string">            labels (batch_size, num_priors): real labels of all the priors.</span></span><br><span class="line"><span class="string">            boxes (batch_size, num_priors, 4): real boxes corresponding all the priors.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        num_classes = confidence.size(<span class="number">2</span>)</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            <span class="comment"># derived from cross_entropy=sum(log(p))</span></span><br><span class="line">            loss = -F.log_softmax(confidence, dim=<span class="number">2</span>)[:, :, <span class="number">0</span>]</span><br><span class="line">            mask = box_utils.hard_negative_mining(loss, labels, self.neg_pos_ratio)</span><br><span class="line"></span><br><span class="line">        confidence = confidence[mask, :]</span><br><span class="line">        <span class="comment"># 分类损失函数</span></span><br><span class="line">        classification_loss = F.cross_entropy(confidence.reshape(-<span class="number">1</span>, num_classes), labels[mask], reduction=<span class="string">&#x27;sum&#x27;</span>)</span><br><span class="line">        pos_mask = labels &gt; <span class="number">0</span></span><br><span class="line">        predicted_locations = predicted_locations[pos_mask, :].reshape(-<span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line">        gt_locations = gt_locations[pos_mask, :].reshape(-<span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line">        <span class="comment"># 边界框回归损失函数</span></span><br><span class="line">        smooth_l1_loss = F.smooth_l1_loss(predicted_locations, gt_locations, reduction=<span class="string">&#x27;sum&#x27;</span>)  <span class="comment"># smooth_l1_loss</span></span><br><span class="line">        <span class="comment"># smooth_l1_loss = F.mse_loss(predicted_locations, gt_locations, reduction=&#x27;sum&#x27;)  #l2 loss</span></span><br><span class="line">        num_pos = gt_locations.size(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> smooth_l1_loss / num_pos, classification_loss / num_pos</span><br></pre></td></tr></table></figure><h2 id="结果预测">结果预测</h2><p>输入为：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210527/9.png" alt=""></p><p>输出为：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210527/10.png" alt=""></p><p>输入为：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210527/11.png" alt=""></p><p>输出为：<br><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210527/12.png" alt=""></p><h2 id="模型转换">模型转换</h2><p>由于部署使用的是Tengine边缘推理框架，由于pytorch输出的模型无法直接转换到tmfile模型下，因此还是选择使用onnx中间件的形式进行过度，具体实现代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">model_path = <span class="string">&quot;models/pretrained/version-RFB-320.pth&quot;</span></span><br><span class="line">net = create_Mb_Tiny_RFB_fd(<span class="built_in">len</span>(class_names), is_test=<span class="literal">True</span>)</span><br><span class="line">net.load(model_path)</span><br><span class="line">net.<span class="built_in">eval</span>()</span><br><span class="line">net.to(<span class="string">&quot;cuda&quot;</span>)</span><br><span class="line"></span><br><span class="line">model_name = model_path.split(<span class="string">&quot;/&quot;</span>)[-<span class="number">1</span>].split(<span class="string">&quot;.&quot;</span>)[<span class="number">0</span>]</span><br><span class="line">model_path = <span class="string">f&quot;models/onnx/<span class="subst">&#123;model_name&#125;</span>.onnx&quot;</span></span><br><span class="line"></span><br><span class="line">dummy_input = torch.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">240</span>, <span class="number">320</span>).to(<span class="string">&quot;cuda&quot;</span>)</span><br><span class="line"><span class="comment"># dummy_input = torch.randn(1, 3, 480, 640).to(&quot;cuda&quot;) #if input size is 640*480</span></span><br><span class="line">torch.onnx.export(net, dummy_input, model_path, verbose=<span class="literal">False</span>, input_names=[<span class="string">&#x27;input&#x27;</span>], output_names=[<span class="string">&#x27;scores&#x27;</span>, <span class="string">&#x27;boxes&#x27;</span>])</span><br></pre></td></tr></table></figure><p>得到onnx模型后便可以进行Tengine模型的转换和部署，该部分将在下一篇文章继续讨论。</p><h2 id="参考">参考</h2><p>[1].<a href="https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB">https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB</a><br></p><p>[2].<a href="https://github.com/onnx/onnx">https://github.com/onnx/onnx</a><br></p>]]></content>
      
      
      <categories>
          
          <category> 项目实践 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人脸识别 </tag>
            
            <tag> 人脸检测 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>详细解读Transformer那些有趣的特性</title>
      <link href="/2021/05/25/2/"/>
      <url>/2021/05/25/2/</url>
      
        <content type="html"><![CDATA[<p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210525/1.png" alt=""></p><blockquote><p>本文发现了Transformer的一些重要特性，如<strong>Transformer对严重的遮挡，扰动和域偏移具有很高的鲁棒性</strong>、<strong>与CNN相比，ViT更符合人类视觉系统，泛化性更强</strong>，等等…  代码即将开源！<br><br><strong>作者单位</strong>：澳大利亚国立大学, 蒙纳士大学, 谷歌等7家高校/企业</p></blockquote><h2 id="简介">简介</h2><p>近期Vision Transformer（ViT）在各个垂直任务上均表现出非常不错的性能。这些模型基于multi-head自注意力机制，该机制可以灵活地处理一系列图像patches以对上下文cues进行编码。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210525/2.png" alt=""></p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210525/3.png" alt=""></p><p>一个重要的问题是，在以给定patch为条件的图像范围内，如何灵活地处理图像中的干扰，例如严重的遮挡问题、域偏移问题、空间排列问题、对抗性和自然扰动等等问题。作者通过涵盖3个ViT系列的大量实验，以及与高性能卷积神经网络（CNN）的比较，系统地研究了这些问题。并通过分析得出了ViT的以下的特性：</p><ol><li><p>Transformer对严重的遮挡，扰动和域偏移具有很高的鲁棒性，例如，即使随机遮挡80％的图像内容，其在ImageNet上仍可保持高达60％的top-1精度;</p></li><li><p>Transformer对于遮挡的良好表现并不是由于依赖局部纹理信息，与CNN相比，ViT对纹理的依赖要小得多。当经过适当训练以对基于shape的特征进行编码时，ViT可以展现出与人类视觉系统相当的shape识别能力;</p></li><li><p>使用ViT对shape进行编码会产生有趣的现象，在即使没有像素级监督的情况下也可以进行精确的语义分割;</p></li><li><p>可以将单个ViT模型提取的特征进行组合以创建特征集合，从而在传统学习模型和少量学习模型中的一系列分类数据集上实现较高的准确率。实验表明，ViT的有效特征是由于通过自注意力机制可以产生的灵活和动态的感受野所带来的。</p></li></ol><h2 id="本文讨论主题">本文讨论主题</h2><h3 id="ViT对遮挡鲁棒否？">ViT对遮挡鲁棒否？</h3><p>这里假设有一个网络模型$f$，它通过处理一个输入图像$x$来预测一个标签$y$，其中$x$可以表示为一个patch $x={x_i}_{i=1}^N$的序列，$N$是图像patch的总数。</p><p>虽然可以有很多种方法来建模遮挡，但本文还是选择了采用一个简单的掩蔽策略，选择整个图像patch的一个子集，$M &lt; N$，并将这些patch的像素值设为0，这样便创建一个遮挡图像$x’$。</p><p>作者将上述方法称为<strong>PatchDrop</strong>。目的是观察鲁棒性$f(x’)_{argmax}=y$。</p><p>作者总共实验了3种遮挡方法:</p><ol><li>Random PatchDrop</li><li>Salient(foreground) PatchDrop</li><li>Non-salient (background) PatchDrop</li></ol><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210525/4.png" alt=""></p><h4 id="1、Random-PatchDrop"><em><strong>1、Random PatchDrop</strong></em></h4><p>ViT通常将图像划分为196个patch，每个patch为14x14网格，这样一幅224x224x3大小的图像分割成196个patches，每个patch的大小为16x16x3。例如，随机从输入中删除100个这样的补丁就相当于丢失了51%的图像内容。而这个随机删除的过程即为<strong>Random PatchDrop</strong>。</p><h4 id="2、Salient-foreground-PatchDrop"><em><strong>2、Salient(foreground) PatchDrop</strong></em></h4><p>对于分类器来说，并不是所有的像素都具有相同的值。为了估计显著区域，作者利用了一个自监督的ViT模型DINO，该模型使用注意力分割图像中的显著目标。按照这种方法可以从196个包含前n个百分比的前景信息的patches中选择一个子集并删除它们。而这种通过自监督模型删除显著区域的过程即为<strong>Salient (foreground) PatchDrop</strong>。</p><h4 id="3、Non-salient-background-PatchDrop"><em><strong>3、Non-salient(background) PatchDrop</strong></em></h4><p>采用与SP（Salient(foreground) PatchDrop）相同的方法选择图像中最不显著的区域。包含前景信息中最低n%的patch被选中并删除。同样，而这种通过自监督模型删除非显著区域的过程即为<strong>Non-salient(background) PatchDrop</strong>。</p><h4 id="鲁棒性分析">鲁棒性分析</h4><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210525/5.png" alt=""></p><p>以Random PatchDrop为例，作者给出5次测试的平均值和标准偏差。对于显著性和非显著性Patchdrop，由于获得的遮挡掩模是确定性的，作者只给出了1次运行的精度值。</p><p>Random PatchDrop 50%的图像信息几乎完全破坏了CNN的识别能力。例如，当去掉50%的图像内容时ResNet50的准确率为0.1%，而DeiT-S的准确率为70%。一个极端的例子可以观察到，当90%的图像信息丢失，但Deit-B仍然显示出37%的识别精度。这个结果在不同的ViT体系结构中是一致的。同样，ViT对前景(显著)和背景(非显著)内容的去除也有很不错的表现。</p><h4 id="Class-Token-Preserves-Information">Class Token Preserves Information</h4><p>为了更好地理解模型在这种遮挡下的性能鲁棒的原有，作者将不同层的注意力可视化(图4)。 通过下图可以看出浅层更关注遮挡区域，而较深的层更关注图像中的遮挡以外的信息。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210525/6.png" alt=""></p><p>然后作者还研究这种从浅层到更深层次的变化是否是导致针对遮挡的Token不变性的原因，而这对分类是非常重要的。作者测量了原始图像和遮挡图像的特征/标记之间的相关系数。在ResNet50的情况下测试在logit层之前的特性，对于ViT模型，Class Token从最后一个Transformer block中提取。与ResNet50特性相比，来自Transformer的Class Token明显更鲁棒，也不会遭受太多信息损失(表1)。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210525/7.png" alt=""></p><p>此外，作者还可视化了ImageNet中12个选择的超类的相关系数，并注意到这种趋势在不同的类类型中都存在，即使是相对较小的对象类型，如昆虫，食物和鸟类。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210525/8.png" alt=""></p><h3 id="ViT能否同时学习Shape和Texture这2种特性？">ViT能否同时学习Shape和Texture这2种特性？</h3><p>Geirhos等人引入了Shape vs Texture的假设，并提出了一种训练框架来增强卷积神经网络(CNNs)中的shape偏差。</p><p>首先，作者对ViT模型进行了类似的分析，得出了比CNN更强的shape偏差，与人类视觉系统识别形状的能力相当。然而，这种方法会导致自然图像的精度显著下降。</p><p>为了解决这种问题，在第2种方法中，作者将shape token引入到Transformer体系结构中，专门学习shape信息，使用一组不同的Token在同一体系结构中建模Shape和Texture相关的特征。为此，作者从预训练的高shape偏差CNN模型中提取shape信息。而作者的这种蒸馏方法提供了一种平衡，既保持合理的分类精度，又提供比原始ViT模型更好的shape偏差。</p><h4 id="Training-without-Local-Texture">Training without Local Texture</h4><p>在训练中首先通过创建一个SIN风格化的ImageNet数据（从训练数据中删除局部纹理信息）。在这个数据集上训练非常小的DeiT模型。通常情况下，vit在训练期间需要大量的数据增强。然而，由于较少的纹理细节，使用SIN进行学习是一项困难的任务，并且在风格化样本上进行进一步的扩展会破坏shape信息，使训练不稳定。因此，在SIN上训练模型不使用任何augmentation、label smoothing或Mixup。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210525/9.png" alt=""></p><p>作者观察到，在ImageNet上训练的ViT模型比类似参数量的CNN模型表现出更高的shape偏差，例如，具有2200万个参数的DeiT-S比ResNet50表现更好(右图)。当比较SIN训练模型时，ViT模型始终优于cnn模型。有趣的是，DeiT-S在SIN数据集上训练时达到了人类水平(左图)。</p><h4 id="Shape-Distillation">Shape Distillation</h4><p>通过学习Teacher models 提供的soft labels，知识蒸馏可以将大teacher models压缩成较小的Student Model。本文作者引入了一种新的shape token，并采用 Adapt Attentive Distillation从SIN dataset(ResNet50-SIN)训练的CNN中提取Shape特征。作者注意到，ViT特性本质上是动态的，可以通过Auxiliary Token来控制其学习所需的特征。这意味着单个ViT模型可以同时使用单独的标记显示high shape和texture bias(下表)。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210525/10.png" alt=""></p><p>当引入shape token时在分类和形状偏差度量方面获得了更平衡的性能(图6)。为了证明这些不同的token(用于分类和shape)可以确实模型独特的特征，作者计算了所蒸馏的模型DeiT-T-SIN和DeiT-S-SIN的class和shape token之间的余弦相似度，结果分别是0.35和0.68。这明显低于class和distill token之间的相似性；DeiT-T和Deit-S分别为0.96和0.94。这证实了关于在Transformer中使用单独的Token可以用来建模不同特征的假设，这是一种独特的能力，但是不能直接用在CNN模型中。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210525/11.png" alt=""></p><h4 id="Shape-biased-ViT-Offers-Automated-Object-Segmentation">Shape-biased ViT Offers Automated Object Segmentation</h4><p>有趣的是，没有局部纹理或形状蒸馏的训练可以让ViT专注于场景中的前景物体而忽略背景(图4)。这为图像提供了自动语义分割的特征，尽管该模型从未显示像素级对象标签。这也表明，在ViT中促进shape偏差作为一个自监督信号，模型可以学习不同shape相关的特征，这有助于定位正确的前景对象。值得注意的是，没有使用shape token的训练中ViT表现得比较差(Table 3)。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210525/12.png" alt=""></p><h3 id="位置编码是否真的可以表征Global-Context？">位置编码是否真的可以表征Global Context？</h3><p>Transformer使用self-attention(而不是RNN中的顺序设计)并行处理长序列，其序列排序是不变的。但是它的明显缺点是忽略了输入序列元素的顺序，这可能很重要。</p><p>在视觉领域patch的排列顺序代表了图像的整体结构和整体构成。由于ViT对图像块进行序列处理，改变序列的顺序，例如对图像块进行shuffle操作但是该操作会破坏图像结构。</p><p>当前的ViT使用位置编码来保存Context。在这里问题是，如果序列顺序建模的位置编码允许ViT在遮挡处理是否依然有效?</p><p>然而，分析表明，Transformer显示排列不变的patch位置。位置编码对向ViT模型注入图像结构信息的作用是有限的。这一观察结果也与语言领域的发现相一致。</p><h4 id="Sensitivity-to-Spatial-Structure">Sensitivity to Spatial Structure</h4><p>通过对输入图像patch使用shuffle操作来消除下图所示的图像(空间关系)中的结构信息。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210525/13.png" alt=""></p><p>作者观察到，当输入图像的空间结构受到干扰时，DeiT模型比CNN模型保持了更高程度的准确性。这也一方面证明了位置编码对于做出正确的分类决策并不是至关重要的，并且该模型并没有使用位置编码中保存的patch序列信息来恢复全局图像context。即使在没有这种编码的情况下，与使用位置编码的ViT相比，ViT也能够保持其性能，并表现出更好的排列不变性(下图)。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210525/14.png" alt=""></p><p>最后，在ViT训练过程中，当patch大小发生变化时，对自然图像进行非混叠处理时，其排列不变性也会随着精度的降低而降低(下图)。作者将ViT的排列不变性归因于它们的动态感受野，该感受野依赖于输入小patch，可以与其他序列元素调整注意，从而在中等变换速率下，改变小patch的顺序不会显著降低表现。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210525/15.png" alt=""></p><blockquote><p>从上面的分析可以看出，就像texture bias假设是错误的一样，依赖位置编码来在遮挡下表现良好也是不准确的。作者得出这样的结论，这种鲁棒性可能只是由于ViT灵活和动态的感受野所带来的，这同时也取决于输入图像的内容。</p></blockquote><h3 id="ViT对对抗信息和自然扰动的鲁棒性又如何？">ViT对对抗信息和自然扰动的鲁棒性又如何？</h3><p>作者通过计算针对雨、雾、雪和噪声等多种综合常见干扰的平均损坏误差(mCE)来研究这一问题。具有类似CNN参数的ViT(例如，DeiT-S)比经过增强训练的ResNet50(Augmix)对图像干扰更加鲁棒。有趣的是，在ImageNet或SIN上未经增强训练的卷积和Transformer模型更容易受到图像干扰的影响(表6)。这些发现与此一致，表明数据增强对于提高常见干扰的鲁棒性是很必要的。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210525/16.png" alt=""></p><p>作者还观察到adversarial patch攻击的类似问题。ViT的鲁棒性高于CNN，通用adversarial patch在白盒设置(完全了解模型参数)。在SIN上训练的ViT和CNN比在ImageNet上训练的模型更容易受到adversarial patch攻击(图10)，这是由于shape偏差与鲁棒性权衡导致的。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210525/17.png" alt=""></p><h3 id="当前ViT的最佳Token是什么？">当前ViT的最佳Token是什么？</h3><p>ViT模型的一个独特特征是模型中的每个patch产生一个class token，class head可以单独处理该class token(下图所示)。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210525/18.png" alt=""></p><p>使得可以测量一个ImageNet预先训练的ViT的每个单独patch的区分能力，如图12所示，由更深的区块产生的class token更具鉴别性，作者利用这一结果来识别其token具有best downstream transferability最优patch token集合。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210525/19.png" alt=""></p><h4 id="Transfer-Methodology">Transfer Methodology</h4><p>如图12所示，作者分析了DeiT模型的block的分类精度，发现在最后几个block的class token中捕获了最优的判别信息。为了验证是否可以将这些信息组合起来以获得更好的性能，作者使用DeiT-S对细粒度分类数据集上现成的迁移学习进行了消融研究(CUB)，如下表所示。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210525/20.png" alt=""></p><p>在这里，作者从不同的块连接class token(可选地结合平均补丁标记)，并训练一个线性分类器来将特征转移到下游任务。</p><p>请注意，一个patch token是通过沿着patch维度进行平均生成的。最后4个块的class token连接得到了最好的迁移学习性能。</p><p>作者将这种迁移方法称为<strong>DeiT-S(ensemble)</strong>。来自所有块的class token和averaged patch tokens的拼接表现出与来自最后4个块的token相似的性能，但是需要显著的大参数来训练。作者进一步在更大范围的任务中使用DeiT-S(集成)进行进一步实验，以验证假设。在接下来的实验中，同时还将CNN Baseline与在预训练的ResNet50的logit层之前提取的特征进行比较。</p><h4 id="General-Classification">General Classification</h4><p>作者还研究了几个数据集的现成特征的可迁移性，包括Aircraft, CUB, DTD, GTSRB, Fungi, Places365和iNaturalist数据集。这些数据集分别用于细粒度识别、纹理分类、交通标志识别、真菌种类分类和场景识别，分别有100、200、47、43、1394、365和1010类。在每个数据集上训练一个线性分类器，并在测试分割上评估其性能。与CNN Baseline相比，ViT特征有了明显的改善(图13)。事实上，参数比ResNet50少5倍左右的DeiT-T性能更好。此外，本文提出的集成策略在所有数据集上都获得了最好的结果。</p><h4 id="Few-Shot-Learning">Few-Shot Learning</h4><p>在few-shot learning的情况下，元数据集是一个大规模的benchmark，包含一个不同的数据集集覆盖多个领域。作者使用提取的特征为每个query学习support set上的线性分类器，并使用标准FSL协议评估。ViT特征在这些不同的领域之间转移得更好(图13)。作者还强调了QuickDraw的一个改进，包含手绘草图的数据集，这与研究结果一致。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210525/21.png" alt=""></p><h2 id="参考">参考</h2><p>[1].Intriguing Properties of Vision Transformers.<br></p>]]></content>
      
      
      <categories>
          
          <category> Transformer </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>在ResNet与Transformer均适用的Skip Connection解读</title>
      <link href="/2021/05/21/1/"/>
      <url>/2021/05/21/1/</url>
      
        <content type="html"><![CDATA[<p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210521/1.png" alt=""></p><blockquote><p>该文主要是分析和讨论了跳跃连接的一些局限，同时分析了BN的一些限制，提出了通过递归的Skip connection和layer normalization来自适应地调整输入scale的策略，可以很好的提升跳Skip connection的性能，该方法在CV和NLP领域均适用。</p></blockquote><h2 id="简介">简介</h2><p>Skip connection是一种广泛应用于提高深度神经网络性能和收敛性的技术，它通过神经网络层传播的线性分量，缓解了非线性带来的优化困难。但是，从另一个角度来看，它也可以看作是输入和输出之间的调制机制，输入按预定义值1进行缩放。</p><p>在本文中，作者通过研究Skip connection的有效性和scale factors显示，一个微不足道的调整将导致spurious gradient爆炸或消失，这可以通过normalization来解决，特别是layer normalization。受此启发作者进一步提出通过递归的Skip connection和layer normalization来自适应地调整输入scale，这大大提高了性能，并且在包括机器翻译和图像分类数据集在内的各种任务中具有很好的泛化效果。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210521/2.png" alt="图1 常用skip connections"></p><h3 id="这项工作的特点：">这项工作的特点：</h3><ol><li>主要关注LN和skip connection的结合；</li><li>重新思考了层归一化的作用，选择不进行缩放；</li><li>在具有代表性的计算机视觉和自然语言处理任务上进行实验；</li><li>摆脱了泛化了所有以前工作的残差块的一般形式，并提出了一种新的递归残差块结构，它具有层归一化，优于本工作中检查的所有一般形式的变体；</li></ol><h2 id="方法">方法</h2><h3 id="connection-problem">connection problem</h3><p>在进行尺度scaling时，会出现梯度爆炸或消失的问题，阻碍了深度神经网络的高效优化。</p><h3 id="optimization-problem">optimization problem</h3><p>由于早期的工作已经确定，将Skip connection直接结合到神经网络的前向传播中就足够了，不需要任何尺度，后续的优化问题研究大多遵循Skip connection结构。</p><h3 id="架构说明">架构说明</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210521/3.png" alt="图2 常见LN与skip connections组合"></p><h4 id="Expanded-Skip-Connection-xSkip-："><strong>Expanded Skip Connection (xSkip)</strong>：</h4><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210521/4.png" alt=""></p><p>其中，$x$和$y$分别为残差块的输入和输出。$F$为weighted neural network layer，$\lambda$为modulating scalar。</p><p>考虑到神经网络层可能具有不同的表示能力和优化难度，这种结构自然调整了跳跃的重要性。然而，需要注意的是，在这项工作中$\lambda$是固定的，目的是隔离缩放的影响。虽然学习过的$\lambda$可能更好地捕捉到这2个部分之间的平衡，但是学习$\lambda$变成了另一个变量。</p><h4 id="Expanded-Skip-Connection-with-Layer-Normalization-xSkip-LN-："><strong>Expanded Skip Connection with Layer Normalization (xSkip+LN)</strong>：</h4><p>在Transformer将跳跃连接与层规范化相结合的激励下，作者进一步研究了层规范化对扩展跳跃连接的影响：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210521/5.png" alt=""></p><p>实验表明层归一化有助于缓解调制因子在优化过程中引起的梯度畸变。不同于作用于“样本空间”的BN，LN则是作用于“特征空间”。同时在神经网络难以优化的情况下，LN仍然可以帮助学习shortcut，而BN可能会失败。</p><h4 id="Recursive-Skip-Connection-with-Layer-Normalization-rSkip-LN-："><strong>Recursive Skip Connection with Layer Normalization (rSkip+LN)</strong>：</h4><p>另一种稳定梯度的方法是每次保持$\lambda$=1，但重复添加带有LN的shortcut，这样更多的输入信息也被建模。它被递归定义为：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210521/6.png" alt=""></p><p>$\lambda$应该是一个不小于1的整数。例如，当$\lambda$=1时，上式便回归到Transformer中使用的block，并符合跳过不需要缩放的结果。</p><p>通过recursive skip connection with layer normalization，该模型鼓励多次使用层归一化来改进优化，通过跳跃连接可以包含更多的x信息。此外，与一次性简单地合并比例跳跃相比，该模型可能获得更强的表达能力，因为每一个递归步骤本质上构建了一个不同的特征分布，递归结构可以学习自适应的x与F(x,W)。</p><h2 id="实验">实验</h2><h3 id="实验1：PreAct-ResNet-110-on-cifar10">实验1：PreAct-ResNet-110 on cifar10</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210521/7.png" alt=""></p><h3 id="实验2：EN-VI-machine-translation">实验2：EN-VI machine translation</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210521/8.png" alt=""></p><h3 id="实验3：BN代替LN">实验3：BN代替LN</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210521/9.png" alt=""></p><p>可以看出，与LN结合跳跃连接相比，BN的效果较差。而本文所提出的递归策略可以帮助BN提升效果。</p><h3 id="实验结论">实验结论</h3><p>作者通过对不同任务的实验（Transformer和ResNet），得出如下结论:</p><ul><li><p>没有经过任何归一化的expanded skip connection确实会造成梯度畸形，导致神经网络的学习效果不理想。层归一化在一定程度上有助于解决 expanded skip connection带来的优化问题。</p></li><li><p>本文提出的带有LN的recursive skip connection，通过将expanded skip connection划分为多个阶段，以更好地融合转换输入的效果，进一步简化了优化过程。</p></li><li><p>利用Transformer在WMT-2014 EN-DE机器翻译数据集上的实验结果进一步证明了递归架构的有效性和效率，模型性能甚至优于3倍大的模型。</p></li></ul><h2 id="参考">参考</h2><p>[1].Rethinking Skip Connection with Layer Normalization in Transformers and ResNets<br></p>]]></content>
      
      
      <categories>
          
          <category> 卷积CNN </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 残差连接 </tag>
            
            <tag> CNN </tag>
            
            <tag> Tansformer </tag>
            
            <tag> ResNet </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
