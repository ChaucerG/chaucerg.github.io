<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>85FPS！CNN+Transformer语义分割的又一境界，真的很快！</title>
      <link href="/2021/09/23/17/"/>
      <url>/2021/09/23/17/</url>
      
        <content type="html"><![CDATA[<p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210923/1.png" alt=""></p><blockquote><p>本文提出了一种用于城市场景语义分割的高效混合Transformer(EHT),其利用CNN和Transformer结合学习全局-局部上下文来加强特征表征,性能优于ABCNet等网络,速度高达83.4FPS!代码将开源!<br><strong>作者单位</strong>:武汉大学,兰卡斯特大学等 </p></blockquote><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>高分辨率城市场景图像的语义分割在土地覆盖制图、城市变化检测、环境保护和经济评估等广泛的实际应用中起着至关重要的作用。卷积神经网络采用分层特征表示,具有很强的局部上下文特征提取的能力。然而,卷积层的局部特性限制了网络捕获全局信息,而这个特点对于改善高分辨率图像分割至关重要。</p><p>最近, Transformer成为计算机视觉领域的热门话题。Vision Transformer也展示了其全局信息建模的强大能力,推动了许多视觉任务,例如图像分类、目标检测,尤其是语义分割。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210923/2.png" alt=""></p><p>在本文中提出了一种用于城市场景图像语义分割的高效混合Transformer(EHT)。EHT利用CNN和ransformer结合设计学习全局-局部上下文来加强特征表示。</p><p>大量实验表明,与最先进的方法相比, EHT具有更高的效率和具有竞争力的准确性。具体来说,所提出的EHT在UAVid测试集上实现了67.0%的mloU,并且明显优于其他轻量级模型。 </p><h2 id="本文方法"><a href="#本文方法" class="headerlink" title="本文方法"></a>本文方法</h2><p>所提出的efficient hybrid Transformer如图所示。将Global-Local Transformer Block附加到ResNet18 Backbone的顶部，就像BottleNeck Transformer一样。利用3个具有3个跨尺度连接的跨尺度融合模块来聚合多层特征。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210923/3.png" alt=""></p><h3 id="2-1-Global-local-Transformer-Block"><a href="#2-1-Global-local-Transformer-Block" class="headerlink" title="2.1 Global-local Transformer Block"></a>2.1 Global-local Transformer Block</h3><p>提出的Global-local Transformer Block(GLTB)的细节如下图所示。主要模块global-local attention block是一种混合结构，采用linear multi-head self-attention捕获全局上下文信息，采用卷积层提取局部上下文信息。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210923/4.png" alt=""></p><p>最后，对全局上下文和局部上下文应用一个add操作来提取全局-局部上下文。</p><h4 id="1、Linear-multi-head-self-attention"><a href="#1、Linear-multi-head-self-attention" class="headerlink" title="1、Linear multi-head self-attention"></a>1、Linear multi-head self-attention</h4><p>本文提出了一种线性注意力机制，用泰勒展开的一阶近似来代替softmax函数。本文将线性注意力改进为线性多头自注意力，以获得更高的效率和更强的序列建模。具体公式推导过程如下:</p><p>设归一化函数为softmax，则自注意力注意产生的结果矩阵的第$i$行可表示为:<br><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210923/5.png" width = "300" align=center /></p><p>其中$v<em>j$是第$j$个特征。根据泰勒的扩展:<br>&lt;img src=”<a href="https://gitee.com/chaucerg/pic">https://gitee.com/chaucerg/pic</a></em>-web/raw/master/images_20210923/6.png” width = “300” align=center /&gt;</p><p>为了保证上述近似是非负的，$𝒒<em>𝑖$和$𝒌</em>𝑗$被归一化$𝑙_2 -norm$,从而确保$q_i^Tk_j≥−1$:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210923/7.png" width = "400" align=center /></p><p>因此，(1)式可以重写为(4)式，并简化为(5)式:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210923/8.png" width = "400" align=center /></p><p>进而有：<br><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210923/9.png" width = "400" align=center /></p><p>上式可以转化为矢量形式：<br><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210923/10.png" width = "400" align=center />  </p><p>$\sum<em>{j=1}^N(\frac{k_j}{||k_j||_2})v_j^{T}$ 和 $\sum</em>{j=1}^N(\frac{k_j}{||k_j||_2})$ 可以计算得到并可以为每个query重用。</p><p><strong>注意：</strong> 在线性多头自注意力的输出上部署了一个可训练的尺度因子，以实现稳定的上下文聚合。</p><h4 id="2、Locality-enhanced模块"><a href="#2、Locality-enhanced模块" class="headerlink" title="2、Locality-enhanced模块"></a>2、Locality-enhanced模块</h4><p>采用2个并行卷积层，然后是一个BN操作来提取局部上下文信息。</p><p>生成的全局局部上下文进一步进行深度卷积、批归一化操作和$1\times 1$卷积，以增强泛化能力。</p><h3 id="2-2-Cross-scale融合模块"><a href="#2-2-Cross-scale融合模块" class="headerlink" title="2.2 Cross-scale融合模块"></a>2.2 Cross-scale融合模块</h3><h4 id="1、Cross-scale连接"><a href="#1、Cross-scale连接" class="headerlink" title="1、Cross-scale连接"></a>1、Cross-scale连接</h4><p>采用两个并行卷积层，然后是一个BN操作来提取局部上下文信息。Cross-scale连接的细节如下图所示。上采样操作的比例因子为2。L为重复次数。3个跨尺度连接对应3个跨尺度融合模块。3个跨尺度连接的Atrous卷积扩张率分别为6、12和18。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210923/11.png" alt=""></p><h4 id="2、加权特征融合"><a href="#2、加权特征融合" class="headerlink" title="2、加权特征融合"></a>2、加权特征融合</h4><p>将Cross-scale连接生成的3种语义特征通过加权元素求和运算与相应的残差特征和上采样的全局局部语义特征进行聚合，以增强泛化能力。公式如下:<br><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210923/12.png" width = "600" align=center /></p><p>其中$f<em>{\mu}$为Resize操作，用来统一$GRF</em>{i+1}$和$CSF<em>i$；$f</em>{\delta}$为$1\times 1$卷积操作,用来统一$RF_i$和$CSF_i$通道的数量；而$\alpha_1,\alpha_2,\alpha_3$为3个特征的权重系数，其中$\alpha_1+\alpha_2+\alpha_3=1$。</p><p>进一步聚合$GFL<em>1,GFL</em>@,GFL_3,GFL_4$作为Head的输入，用于最终的分割。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p><strong>Backbone</strong>：可以通过ResNet-18和像UNet一样的逐层特征融合来构建。</p><p><strong>Backbone+CFM</strong>：用跨尺度融合模块代替逐层特征融合来构建一个简单的变体。利用该变体验证了跨尺度融合模块的有效性。</p><p><strong>Backbone+CFM+GLTB</strong>：将Global-Local Transformer块插入到Baseline+CFM来生成整个EHT，可以证明所提方法的有效性。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210923/13.png" alt=""></p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210923/14.png" alt=""></p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210923/15.png" alt=""></p><p>可以看出本文所提模块可以很好的兼顾全局和局部的上下文信息，值得小伙伴们进行学习和借鉴。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>[1].Efficient Hybrid Transformer: Learning Global-local Context for Urban Sence Segmentation<br></p>]]></content>
      
      
      <categories>
          
          <category> Transformer </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CNN </tag>
            
            <tag> Transformer </tag>
            
            <tag> 语义分割 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>教你How to train自己的Transformer模型</title>
      <link href="/2021/09/18/16/"/>
      <url>/2021/09/18/16/</url>
      
        <content type="html"><![CDATA[<p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210621/1.png" alt=""></p><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>Vision Transformers(Vision transformer, ViT)在图像分类、目标检测和语义分割等视觉应用中得到了具有竞争力得性能。</p><p>与卷积神经网络相比，当在较小的训练数据集上训练时，通常发现Vision Transformer较弱的归纳偏差导致对模型正则化或数据增强(简称AugReg)的依赖增加。为了更好地理解训练数据量、AugReg、模型大小和计算预算之间的相互作用，作者进行了系统的实验研究。</p><p>研究的一个结果是，作者发现增加的计算和AugReg相结合，可以产生与在更多训练数据上训练的模型具有相同性能的模型:在ImageNet-21k数据集上训练各种大小的ViT模型，这些模型与在更大的JFT-300M数据集上训练的模型比较甚至可以得到更好得结果。</p><h2 id="论文作者主要说了什么？"><a href="#论文作者主要说了什么？" class="headerlink" title="论文作者主要说了什么？"></a>论文作者主要说了什么？</h2><ul><li><p>第一次系统的、大规模的研究在训练Vision Transformer之前，正则化、数据增强、模型大小和训练数据大小之间的相互作用，包括它们各自对达到一定性能水平所需的计算预算的影响。</p></li><li><p>通过迁移学习的视角来评估预训练模型。因此，作者描述了一个相当复杂的训练设置训练前的Vision Transformer跨越不同的模型尺寸。实验得出了许多关于各种技术的影响的令人惊讶的见解，以及什么时候增强和正则化是有益的，什么时候无益的情况。</p></li><li><p>作者还对Vision Transformer的迁移学习配置进行了深入分析。结论是<strong>在广泛的数据集中，即使下游数据似乎与用于前训练的数据只有微弱的关联，迁移学习仍然是最佳的可用选择</strong>。作者分析还表明，<strong>在执行类似的预训练模型中，对于迁移学习来说，具有更多训练数据的模型可能比具有更多数据增强的模型更受青睐</strong>。</p></li><li><p>本文研究将有助于指导未来的Vision Transformer的研究，并将成为一个有效的训练设置的有用来源，以寻求在给定的计算预算下优化他们的最终模型性能。</p></li></ul><h2 id="Findings"><a href="#Findings" class="headerlink" title="Findings"></a>Findings</h2><h3 id="3-1-Scaling-datasets-with-AugReg-and-compute"><a href="#3-1-Scaling-datasets-with-AugReg-and-compute" class="headerlink" title="3.1 Scaling datasets with AugReg and compute"></a>3.1 Scaling datasets with AugReg and compute</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210621/2.png" alt=""></p><p>研究的一个主要发现(如图1(左)所示)是，通过使用图像增强和模型正则化预训练一个模型，使其达到与增加数据集大小约一个数量级相同的精度。更准确地说，在AugReg ImageNet-1k上训练的最佳模型的性能与在10倍大的普通ImageNet-21k数据集上训练的相同模型的性能差不多。</p><p>类似地，在AugReg ImageNet-21k上训练的最佳模型，当计算量也增加时，将匹配或优于在普通JFT-300M数据集上训练的模型。因此，可以将这些结果与公开可用的数据集进行匹配，可以想象，在JFT-300M上进行更长时间的训练和使用AugReg可能会进一步提高性能。</p><p>当然，这些结果不能适用于任意小的数据集。只对ImageNet-1k的10%进行大量数据增强的ResNet50训练可以改善结果，但不能恢复对完整数据集的训练。</p><h3 id="3-2-Transfer-is-the-better-option"><a href="#3-2-Transfer-is-the-better-option" class="headerlink" title="3.2 Transfer is the better option"></a>3.2 Transfer is the better option</h3><p>在这里，作者调查了对于从业者可能遇到的合理规模的数据集，是否建议尝试使用AugReg从头开始进行训练，或者是否把时间和金钱花在迁移预训练模型上更好。其结果是，就大多数实际目的而言，迁移预先训练的模型不仅成本效益更高，而且会带来更好的结果。</p><p>作者在一个与ImageNet-1k数据集相似大小的数据集上对小的ViT-Ti/16模型进行了搜索，寻找一个好的训练策略。Resisc45包含大约3万幅训练图像，由一种非常不同的卫星图像组成，ImageNet-1k或ImageNet-21k都没有很好地覆盖这些图像。图1(右)和图2显示了这一广泛搜索的结果。</p><p>最惊人的发现是，无论花费多少训练时间，对于微小的Pet37数据集，似乎不可能从头开始训练ViT模型，使其达到接近迁移模型的精度。此外，由于预训练模型可以免费获取，所以从业者的预训练成本实际上为零，只有用于迁移学习的计算损失，因此迁移预训练的模型同时也大大便宜。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210621/3.png" alt=""></p><p>对于更大的Resisc45数据集，这个结果仍然成立，尽管多花费2个数量级的计算和执行大量搜索可能接近(但达不到)预先训练的模型的精度。</p><p>值得注意的是，这并没有考虑到很难量化的“exploration cost”。对于训练前的模型，我们强调那些在训练前验证集上表现最好的模型，可以称为推荐模型。可以看到，使用推荐的模型有很高的可能性在几次尝试中就能获得良好的结果。</p><h3 id="3-3-More-data-yields-more-generic-models"><a href="#3-3-More-data-yields-more-generic-models" class="headerlink" title="3.3 More data yields more generic models"></a>3.3 More data yields more generic models</h3><p>通过将预训练模型迁移到下游任务来研究预训练数据集大小的影响。作者在VTAB上评估了训练前的模型，包括19个不同的任务。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210621/4.png" alt=""></p><p>图3显示了3个VTAB类别的结果:natural、specialized和structured。模型按推理时间进行排序，模型越大推理速度越慢。</p><p>首先比较使用相同计算预算的2个模型，唯一的区别是ImageNet-1k(1.3M图像)和ImageNet-21k (13M图像)的数据集大小。作者实验对比ImageNet-1k训练300个epoch的模型和ImageNet-21k上训练30个epoch模型发现，在ImageNet-21k上进行预训练的模型3个VTAB类别上都明显优于ImageNet-1k。</p><p>随着计算预算的不断增长，作者观察到ImageNet-21k数据集在10倍长的调度上的一致改进。在一些几乎已经解决的任务中，例如花，获得的绝对数量很小。对于剩下的任务，与短期训练的模型相比，改进是显著的。</p><p>总的来说得出的结论是，<strong>数据越多，模型就越通用</strong>，这一趋势适用于不同的任务。作者<strong>建议设计选择使用更多的数据和一个固定的计算预算</strong>。</p><h3 id="3-4-Prefer-augmentation-to-regularization"><a href="#3-4-Prefer-augmentation-to-regularization" class="headerlink" title="3.4 Prefer augmentation to regularization"></a>3.4 Prefer augmentation to regularization</h3><p>目前尚不清楚在RandAugment和Mixup等数据增强和Dropout和randomdepth等模型正则化之间有哪些取舍。在本节的目标是发现这些通用模式，当将Vision transformer应用到一个新任务时，可以作为经验规则使用。</p><p>在图4中，作者展示了为每个单独设置获得的上游验证得分，即在更改数据集时，数字是不具有可比性的。</p><p>一个单元格的颜色编码其分数的改善或变差，与非正则化的，未增强的设置，即最左边的列。增强强度从左到右依次增大，模型容量从上到下依次增大。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210621/5.png" alt=""></p><p>第1个可见的观察结果是，对于中等规模的ImageNet-1k数据集，任何类型的AugReg都有帮助。然而，当使用10倍大的ImageNet-21k数据集并保持计算固定时，即运行30个epoch，任何一种AugReg都会影响除最大模型之外的所有模型的性能。只有当计算预算增加到300个时，AugReg才帮助更多的模型，尽管即使那样，它也继续影响较小的模型。</p><p>一般来说，<strong>增加增广效果比增加正规化效果好得多</strong>。更具体地说，图4中每个映射右侧的细列显示，对于任何给定的模型，其最佳正则化分数减去最佳非正则化分数。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210621/6.png" alt=""></p><p>在图7中，作者通过dropout和random depth的方式向模型添加正则化时，显示了精度上的增益(绿色，正数)或损失(红色，负数)。在早期的实验中证实，两者结合(峰值)下降概率0.1确实是最好的设置。</p><p>这表明，模型正规化主要帮助较大的模型，但是当训练时间较长的情况下，特别是ImageNet-21的预训练，除了最大的模型它对所有的模型都有害的。</p><h3 id="3-5-Choosing-which-pre-trained-model-to-transfer"><a href="#3-5-Choosing-which-pre-trained-model-to-transfer" class="headerlink" title="3.5 Choosing which pre-trained model to transfer"></a>3.5 Choosing which pre-trained model to transfer</h3><p>如上所述，在对ViT模型进行预训练时，各种正则化和数据增强设置会导致模型具有显著不同的性能。</p><p>然后，从实践者的观点来看，一个自然的问题出现了:<strong>如何选择一个模型进一步适应最终的应用程序</strong>?</p><ul><li><p>一种方法是：对所有可用的预训练模型进行下游适应，然后根据下游任务的验证分数选择表现最好的模型。但是这在实践中可能是相当麻烦的。</p></li><li><p>另一种方法是：可以根据上游验证精度选择一个单独的预训练模型，然后只使用该模型进行自适应，这要简单得多。</p></li></ul><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210621/7.png" alt=""></p><p>在这里作者将分析这2种策略之间的权衡。在5个不同的数据集上对它们进行了大量的预训练模型的比较。具体来说，在图5(左)中强调了只适应最好的预训练模型的简单策略和适应所有预训练模型(然后选择最好的)的复杂策略之间的性能差异。</p><p>结果好坏参半，但总体上反映出，在大多数情况下，成本较低的策略与成本较高的策略效果相同。然而，有一些显著的异常值，当它有利于适应所有的模型。</p><p>因此，作者得出结论，<strong>选择一个基于上游分数的单一预训练模型是一种具有成本效益的实用策略</strong>，并在整个论文中使用它。然而，作者也强调，<strong>如果有额外的计算资源可用，那么在某些情况下，可以通过微调额外的预训练模型来进一步提高自适应性能</strong>。</p><h4 id="关于ImageNet-1k数据集验证数据的说明"><a href="#关于ImageNet-1k数据集验证数据的说明" class="headerlink" title="关于ImageNet-1k数据集验证数据的说明"></a>关于ImageNet-1k数据集验证数据的说明</h4><p>在执行上述分析时，作者发现在ImageNet-21k上预先训练并迁移到ImageNet-1k数据集的模型存在一个微小但严重的问题。这些模型(特别是大型模型)的验证分数与观察到的测试性能没有很好的关联，见图5(左)。这是因为ImageNet-21k数据包含ImageNet-1k训练数据，作者使用训练数据的一个小split来进行评估(见3.1节)。</p><p>因此，在较长训练计划上的大型模型记忆了来训练集的数据，这使得在小评估集中计算的评估指标存在偏差。为了解决这个问题并支持公平的超参数选择，作者使用独立收集的ImageNetV2数据作为传输到ImageNet-1k的验证split。如图5(右)所示。作者没有在其他数据集中观察到类似的问题。</p><p>作者建议<strong>将ImageNet-21k模型迁移到ImageNet-1k的研究人员遵循这一策略</strong>。</p><h3 id="3-6-Prefer-increasing-patch-size-to-shrinking-model-size"><a href="#3-6-Prefer-increasing-patch-size-to-shrinking-model-size" class="headerlink" title="3.6 Prefer increasing patch-size to shrinking model-size"></a>3.6 Prefer increasing patch-size to shrinking model-size</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210621/8.png" alt=""></p><p>作者研究的一个意想不到的结果是，训练了几个模型，它们在推理吞吐量方面大致相等，但在质量方面差异很大。</p><p>具体地说，图6(右)显示了包含Tiny变体的模型比具有32-patch-size的类似快速的更大模型的性能要差得多。对于给定的分辨率，patch-size会影响self-attention执行的token数量，因此会影响模型容量，而<strong>参数计数并不能反映模型容量。参数计数既不反映速度，也不反映容量</strong>。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210621/9.png" alt=""></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>[1].How to train your ViT? Data, Augmentation,and Regularization in Vision Transformers<br></p>]]></content>
      
      
      <categories>
          
          <category> Transformer </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 详细解读GooGle新作 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AdaConv自适应卷积让你的GAN比AdaIN更看重细节</title>
      <link href="/2021/09/18/15/"/>
      <url>/2021/09/18/15/</url>
      
        <content type="html"><![CDATA[<p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210620/1.png" alt=""></p><blockquote><p>本文提出了AdaIN的改进版本，称为自适应卷积 (AdaConv)，它可以同时适应统计和结构风格，表现SOTA！性能优于AdaIN等网络，已收录于CVPR 2021！<br><strong>作者单位</strong>：迪士尼研究院, ETH Zurich</p></blockquote><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>图像的风格迁移是CNN在艺术领域的一种应用，这里的风格迁移是指将其中一幅图像的“风格”迁移到另一幅图像上，同时保留后者的内容。</p><p>近期的SOTA风格迁移模型大多数都是基于最新的自适应实例归一化(AdaIN)，这是一种将风格特征的统计特性迁移到内容图像的技术，可以实时迁移大量风格。</p><p>然而，AdaIN是一个全局的操作；因此，在迁移过程中，风格图像中的局部几何结构常常被忽略。于是作者提出了自适应卷积(AdaConv)，这是AdaIN的通用扩展，允许同时传输统计和结构风格。除了风格迁移，本文的方法还可以很容易地扩展到基于风格的图像生成，以及其他已经采用AdaIN的任务。</p><h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><h3 id="2-1-Neural-Style-Transfer-based-on-CNNs"><a href="#2-1-Neural-Style-Transfer-based-on-CNNs" class="headerlink" title="2.1 Neural Style Transfer based on CNNs"></a>2.1 Neural Style Transfer based on CNNs</h3><p>基于CNN的神经网络风格转移最初是由Gatys等人提出的。虽然该方法允许在图像之间转换任意样式，但它的优化过程是比较缓慢的。</p><p>Johnson等人通过引入感知损失(perceptual loss)来解决优化慢的问题，允许显著加速优化并实现实时结果。同时，Ulyanov等人提出了一种新的风格迁移方法，通过评估预先训练特定风格的前馈神经网络，进一步加快了推理速度。在后续工作中，他们还<strong>用实例标准化层(IN)取代了批处理标准化层(BN)</strong>，该方法在不影响速度的情况下产生更高质量的结果。</p><p>为了进一步改善对风格迁移结果的控制，Gatys等人随后在基于优化和前馈的方法中重新制定了损失函数，引入了显式的颜色、规模和空间控制。</p><p>在IN思想的基础上，Dumoulin等人提出了<strong>条件实例规范化(CIN)</strong>，并将CIN层设置在Style上，允许单个模型从32种预定义的Style或它们的插值中执行样式转换。Ghiasi等人则进一步扩展了CIN，允许转换为任意风格;这是通过使用大量的风格语料库来训练一个将风格图像转换为条件反射潜在向量的编码器来实现的。</p><p>Cheng等人提出了基于Patch的风格交换方法来实现任意的风格转移。同时，Huang等人提出了一种任意风格迁移的方法，通过有效地使IN适应风格特征的均值和标准差，从而产生了<strong>AdaIN</strong>。</p><p>Li等人对该方法<strong>AdaIN</strong>进行了扩展，对给定风格的潜在特征进行了增白和着色。Sheng等人进一步扩展了这一想法，并采用了风格装饰器模块和多尺度风格适配。</p><p>最近，Jing等人注意到，直接用样式特性的统计数据替换内容特性的统计数据可能是次优选择;相反，<strong>动态实例标准化</strong>(DIN)方法训练style编码器输出内容特性的新统计数据，同时还调整后续卷积层的大小和采样位置。</p><p>除了实例规范化，Kotovenko等人也探索了对抗学习，以更好地将风格与内容分离。</p><p>而本文工作的目的是进一步扩展AdaIN，根据风格图像预测整个卷积核和偏差，传递统计数据和风格的局部结构。</p><h3 id="2-2-Modulation-layers-in-generative-models"><a href="#2-2-Modulation-layers-in-generative-models" class="headerlink" title="2.2 Modulation layers in generative models"></a>2.2 Modulation layers in generative models</h3><p>生成模型中的Modulation layers也促成了风格迁移提升的一个突破口。诸如StyleGAN使用了原始版本的AdaIN，但是输入风格统计数据是由MLP从高斯噪声向量中预测的。为了减轻AdaIN造成的一些可见的伪影，StyleGAN-v2用一个权重Modulation layer代替它，它只对标准差进行归一化和调制，而不改变平均值。</p><p>由于AdaIN及其变体只转换全局统计信息，它们对style输入中的局部空间语义不敏感。为了解决这一限制，有学者提出了新的方法，即从输入空间布局图像中预测空间变化的归一化参数。</p><p>SPADE用从输入语义掩码回归的逐像素变换替换AdaIN的全局仿射变换。SEAN进一步扩展了SPADE，考虑了一个附加的带有输入布局掩码的样式向量。SPADE和SEAN都保留了用于语义图像生成的条件空间布局;它们可以有效地控制每个kernel在特定的图像位置是如何被强调或抑制的。</p><p>相反，本文的AdaConv方法在测试时生成全新的kernel。另外，SPADE和SEAN也不直接适用于风格迁移，而是在样风格迁移中必须保留内容图像的空间布局。</p><h3 id="2-3-Kernel-prediction"><a href="#2-3-Kernel-prediction" class="headerlink" title="2.3 Kernel prediction"></a>2.3 Kernel prediction</h3><p>Kernel prediction也在以前的工作中进行了探讨。</p><p>请注意，上述特征归一化和调制的所有方法都遵循类似的过程:<strong>它们定义了单独应用于每个特征通道的标量仿射变换</strong>。</p><p><strong>主要区别在于</strong>:<br>1) 转换参数是手工制作的，还是在训练中学习的，还是在测试时预测的;<br>2) 每个通道的转换是全局的还是空间变化的。</p><p>那些回归全局转换的方法也可以理解为在测试时预测1×1 2D kernel。</p><p>对于风格迁移，Chen等人在内容图像特征上学习了卷积的风格特定的滤波器组。该方法局限于过滤训练时学到的组;它不能为在测试时给出的不可见style生成新的kernel。</p><p>Jing等声称使用通用DIN块能够从输入中回归动态卷积;然而，实验结果仅限于1×1转换。Kernel prediction的相关工作也不仅仅只是style transfer。</p><p>最新的蒙特卡罗渲染去噪方法使用神经网络预测动态kernel，用于重建最终去噪的帧。</p><p>神经网络也被提出用于预测手持相机以突发模式拍摄的自然图像的去噪核。Niklaus等人的预测视频帧插值核;他们后来将这项工作扩展到预测可分离卷积参数。</p><p>Xue等利用CNN从随机高斯变量中预测动态kernel用于合成可信的下一帧。</p><p>Esquivel等人的预测自适应kernel用于减少在有限的计算资源下准确分类图像所需的层数。</p><p>在本文中作者探讨了一个类似的想法，即<strong>利用测试时的Kernel prediction来改进生成模型中的风格迁移和基于风格的调制</strong>。</p><h2 id="Feature-Modulation-with-AdaConv"><a href="#Feature-Modulation-with-AdaConv" class="headerlink" title="Feature Modulation with AdaConv"></a>Feature Modulation with AdaConv</h2><p>这里先描述AdaConv和Kernel prediction，展示AdaConv如何泛化以及扩展特征调制中的1×1仿射变换。</p><p>首先在风格迁移的背景下画一个与AdaIN平行的例子，然后展示AdaConv如何更好地调节局部特征结构，更好地迁移空间风格，同时该方法也适用于风格迁移之外的高质量生成模型。</p><h3 id="3-1-Overview"><a href="#3-1-Overview" class="headerlink" title="3.1 Overview"></a>3.1 Overview</h3><p>考虑通常的style表示法${a,b}\in R^2$，其中$a$和$b$分别表示风格为尺度和偏差项(例如，对于风格迁移，$a$和$b$是风格图像特征的平均值和标准差)。</p><p>给定一个值为$x\in R$的输入特征通道和所需的style，AdaIN将style定义的仿射变换应用于标准化的输入特征，</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210620/2.png" alt=""></p><p>其中，$\mu _x$和$\sigma_x$为特征通道上的均值和标准差。</p><p>因此，AdaIN只改变每个通道基于条件设置样式参数$(a,b)$的全局统计。注意，无论每个样本$x$周围的特征值的空间分布(结构)如何，整个通道都是相等调制的。</p><p>因此，作者扩展AdaIN的<strong>第1步</strong>是引入一个条件2D style filter $f \in R^{k_h×k_w}$，取代scale term和产生扩展的风格参数${f,b}$。该filter允许根据样本$x$周围邻域$N(x)$的局部结构以空间变化的方式调制特征通道:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210620/3.png" alt=""></p><p>注意，这个depthwise AdaConv变种包含AdaIN，这是1个特殊情况1×1 filter f和$N(x)={x}$</p><p>完整AdaConv调制的<strong>第2步</strong>是通过扩展输入style参数，也包括一个separable-pointwise卷积$p\in R^C$，该卷积用于C特征通道的输入，来扩展这个深度变体。这使得AdaConv可以基于一种风格进行调制，这种风格不仅可以捕获全局统计数据和空间结构，还可以捕获不同输入通道中特征$x_c$之间的关联。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210620/4.png" alt=""></p><p>AdaConv的输入风格${p,f,b}$有效地包含了一个深度可分离的3D卷积核，具有深度和逐点卷积分量，以及每个通道的偏差。</p><p>用于调制输入的深度和逐点卷积核的实际数量是一种设计选择，可以任意大，这可以通过使用深度可分离卷积层中的$n_g$组的数量来控制。</p><p>接下来，作者还提出了AdaConv的kernel prediction框架，并展示了它如何作为AdaIN的一般替代来实现更全面的基于风格的条件转换，也在其他高质量的生成模型。</p><h3 id="3-2-Style-Transfer-with-AdaConv"><a href="#3-2-Style-Transfer-with-AdaConv" class="headerlink" title="3.2 Style Transfer with AdaConv"></a>3.2 Style Transfer with AdaConv</h3><p>下图给出了风格迁移架构的概述。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210620/5.png" alt=""></p><p>输入风格和内容图像编码使用pre-trained VGG-19编码器获取潜在的风格特征S和内容C。</p><p>对于kernel prediction来说，风格特征编码进一步通过风格编码器ES获得全局风格描述符W；对于W kernel prediction网络$K={K_1、,K_2,…,K_N}$输出具有每通道偏差的深度可分卷积核。这些预测被输入到解码器D的所有层中来输出风格迁移的结果。</p><p>本文的风格迁移架构使用了4个kernel prediction，它们用于解码图像的4种不同分辨率，每个kernel具有不同的维度。</p><p>每个解码层都有一个自适应卷积块(下图)，其中预测的深度卷积和逐点卷积先于标准卷积。这些标准卷积层负责学习与风格无关的kernel，这些kernel对于重建自然图像很有用，并且在测试时保持固定。在VGG-19潜在特征空间内，联合训练编码器ES、kernel prediction K和解码器D。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210620/6.png" alt=""></p><h4 id="1-风格编码"><a href="#1-风格编码" class="headerlink" title="1 风格编码"></a>1 风格编码</h4><p>现在转向从风格特征S预测卷积核的目标，用于图像解码器的每个尺度上的内容特征C。</p><p>在这里，一个中间步骤是计算一个综合描述不同尺度的风格图像的风格表示W，同时以风格传递损失为指导。这种设计选择也是通过与最先进的生成建模的类比而产生的，其中术语“style”表示图像的全局和局部属性。</p><p>预训练的VGG-19网络将尺寸为(3,256,256)的原始输入修改为尺寸为(512,32,32)的VGG-19 $relu4_1$层样式张量S。这里，感受野并没有覆盖整个风格图像。因此,需要通过训练一个额外的编码器组件ES，将S减少到全局嵌入W中，如图3所示。</p><p>这里的风格编码器ES包括3个初始块，每个块具有3×3卷积、一个平均池化操作和一个Leaky ReLU激活。</p><p>然后，ES的输出被Reshape并输入到最后一个完全连接的层，该层提供全局风格描述符，该层反过来又被Reshape为大小为W的输出张量$(s_d,s_h,s_W)$。这种嵌入的尺寸是超参数定义为要预测的kernel大小的一个因素。</p><p>由于使用了这个完全连接层，网络只能处理固定尺寸(3,256,256)的输入风格的图像。然而，内容图像的尺寸不受限制，因为它流经网络的一个全卷积的组件。</p><h4 id="2-预测深度可分离卷积"><a href="#2-预测深度可分离卷积" class="headerlink" title="2 预测深度可分离卷积"></a>2 预测深度可分离卷积</h4><p>图2中的每个kernel predictor K都是一个简单的卷积网络，它的输入是风格描述符W，而输出是一个深度可分离的kernel。</p><p>选择预测深度可分离的kernel的动机是希望保持kernel predictor的简单和计算效率，同时也使随后的卷积更快。</p><p>标准卷积层取一个维数为1的输入特征张量$(1,c<em>{in},h,w)$，并将其与一个大小为$(c</em>{out}, c<em>{in}, k_h, k_w)$的kernel张量进行卷积，其中$c</em>{in}$和$c<em>{out}$是输入和输出通道的数量。每通道偏置也被添加到输出。因此，该层所需的权重数为:$c</em>{out}\times c<em>{in}\times k_h\times k_w+ c</em>{out}$。</p><p>深度可分离卷积通过将输入通道聚集到$n<em>g$个独立的组中，并通过应用独立的spatial和pointwise kernel(分别学习结构和交叉固定空间卷积适应通道相关)来减少这个数量。所需重量减少为$c</em>{out}\times \frac{c<em>{in}}{n_g} \times k_h\times k_w+ c</em>{out}$。对于带有$n<em>g=c</em>{in}$的卷积层，输入的每个通道都与自己的$c<em>{out}/c</em>{in}$卷积核进行卷积。</p><p>接下来是对1×1卷积核的逐点卷积，以扩展输出中的通道数，并在最终输出中添加每通道的偏置。</p><p>这里，需要注意的是，解码器中的4个AdaConv层的$c_{in}$随着空间分辨率的增加而减少,分别为512、256、128和64。</p><p>因此，最低空间分辨率的kernel predictor通常具有最高的参数数。为了将网络容量均匀分布在连续的分辨率层上，作者在较低的分辨率上设置了较大的$n_g$，并在连续的层上逐渐降低$n_g$，从而得到更好的结果。对于深度卷积核和depthwise卷积核，$n_g$的设置是相同的。</p><p>因此，每个kernel predictor K在该解码器内为深度卷积AdaConv层输出必要的权值。这些权重包括:</p><p>1) spatial kernel的size $(c<em>{out},\frac{c</em>{in}}{n<em>g},k_h,k_w)$;<br>2) pointwise kernel的size $(c</em>{out},\frac{c_{out}}{n_g},1,1)$<br>3) bias项$b\in R^{out}$。</p><p>每个kernel predictor K的输入是大小为$(s_d,s_h,s_w)$的全局风格描述符W，它通过卷积和池化层得到，这些层输出目标维度的spatial kernel，如图3所示。</p><p>这些层可能由标准卷积或转置卷积组成，其参数在设计时确定，并取决于要预测的kernel的大小。</p><p>为了预测pointwise 1×1 kernels，作者将W集合到一个大小$(s<em>d,1,1)$，然后执行一维卷积来预测pointwise的$c</em>{out}$核。</p><p>作者对每个通道的偏差使用一个单独的预测器，类似于pointwise kernels的预测器。一旦kernel和偏差被预测，它们被用来调制如图3右半部分所示的输入。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="4-1-风格迁移"><a href="#4-1-风格迁移" class="headerlink" title="4.1 风格迁移"></a>4.1 风格迁移</h3><p>对比实验如下：</p><p>与AdaIN的对比如下，可以看出有明显的改善：</p><h3 id="4-2-生成模型的扩展"><a href="#4-2-生成模型的扩展" class="headerlink" title="4.2 生成模型的扩展"></a>4.2 生成模型的扩展</h3><p>基于StarGAN-v2的改进如下：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210620/7.png" alt=""></p><p>实验结果如下：<br><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210620/8.png" alt=""></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>[1].Adaptive Convolutions for Structure-Aware Style Transfer<br></p>]]></content>
      
      
      <categories>
          
          <category> CVPR2021 GAN解读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AdaConv自适应卷积 </tag>
            
            <tag> GAN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>详细解读：HP-x激活函数</title>
      <link href="/2021/09/18/14/"/>
      <url>/2021/09/18/14/</url>
      
        <content type="html"><![CDATA[<p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210618/1.png" alt=""></p><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>本文提出了orthogonal-Padé激活函数，它是可以训练的激活函数，在标准深度学习数据集和模型中具有更快的学习能力，同时可以提高模型的准确率。根据实验，在六种orthogonal-Padé激活中找到了2种最佳的候选函数，作者称之为 safe Hermite-Pade(HP)激活函数，即HP-1和HP-2。</p><p>与ReLU相比,HP-1和HP-2帮助PreActResNet-34带来不同程度的提升(top-1精度提升分别为5.06%和4.63%),在CIFAR100数据集上MobileNet V2模型提升分别为3.02%和2.75%分别，在CIFAR10数据集上PreActResNet-34的top-1精度分别增加了2.02%和1.78%,LeNet的top-1精度分别提升为2.24%和2.06%,Efficientnet B0的top-1精度分别提升为2.15%和2.03%。</p><h2 id="前人工作简介"><a href="#前人工作简介" class="headerlink" title="前人工作简介"></a>前人工作简介</h2><p>深度卷积神经网络由多个隐藏层和神经元构成。然后通过每个神经元的激活函数引入非线性。</p><p>ReLU由于其简单性，是深度学习中最受欢迎的激活函数。虽然ReLU有一个缺点叫做 dying ReLU，在这种情况下，多达50%的神经元可能会因为消失梯度问题，即有大量的神经元对网络性能没有影响。为了克服这一问题，后来又提出了Leaky Relu、Parametric Relu、ELU、Softplus，虽然找到最佳的激活函数仍是一个有待研究的问题，但这些方法都提高了网络的性能。最近，研究人员使用了自动搜索技术发现了Swish激活函数。与ReLU相比，Swish的精确度有了一些提高。GELU、Mish、TanhSoft、EIS是目前少数几个可以替代ReLU和Swish的候选激活函数。</p><p>近年来，人们对可训练激活函数的研究也越来越感兴趣。可训练激活函数具有可学习的超参数(s)，在训练过程中通过反向传播算法更新。本文提出了Orthogonal-Padé激活函数。Orthogonal-Padé函数可以近似大多数连续函数。</p><h2 id="Pade-activation-Unit-PAU-and-Orthogonal-PAU"><a href="#Pade-activation-Unit-PAU-and-Orthogonal-PAU" class="headerlink" title="Padé activation Unit (PAU) and Orthogonal-PAU"></a>Padé activation Unit (PAU) and Orthogonal-PAU</h2><p>考虑实线的一个闭合间隔为[a,b]。设$P_n(x)$是$x$中次数小于等于$n$的所有多项式的空间。对于一个非负连续函数$w(x)$,在[a, b]上定义Pn(x)上的内积为:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210618/2.png" alt=""></p><p>有多项式${P_1(x);P_2(x);···;P_k(x)}$是正交的，如果：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210618/3.png" alt=""></p><p>$P_n(x)$的一组基是由$P_n(x)$张成的n个多项式的集合。一组正交基也是一组正交集。</p><p>$P_n(x)$的标准基是${1;x, x^2;···;x^n}$。但是标准基与式1中定义的内积并不是正交的。</p><p>在许多应用中，使用正交基可以简化表达式并减少计算。多项式空间有几个众所周知的正交基。下表列出了其中一些多项式基。注意，它们有的由递归关系给出，有的由直接表达式给出。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210618/4.png" alt="表1 一些著名的正交多项式基"></p><h3 id="3-1-Pade-activation-Unit-PAU"><a href="#3-1-Pade-activation-Unit-PAU" class="headerlink" title="3.1 Padé activation Unit (PAU)"></a>3.1 Padé activation Unit (PAU)</h3><p>f(x)由有理函数F1(x)的Padé近似定义为：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210618/5.png" alt=""></p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210618/6.png" alt=""></p><p>其中P(x)和Q(x)分别是k次和l次的多项式，它们没有公因式。PAU是式(3)的可学习激活函数，其中多项式系数$a_i;b_j;0≤i≤k;1≤j≤l$为可学习参数，在反向传播过程中进行更新。为了将F1(x)的极点从Q(x)的0中移除，有学者提出了safe PAU。safe PAU定义为：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210618/7.png" alt=""></p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210618/8.png" alt=""></p><p>在分母中引入绝对值可以确保分母不会消失。实际上，也可以取和的绝对值来定义：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210618/9.png" alt=""></p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210618/10.png" alt=""></p><p>在许多任务中，F3定义的激活函数比F2定义的safe PAU能够提供更好的结果。</p><h3 id="3-2-Orthogonal-Pade-activation-Unit-OPAU"><a href="#3-2-Orthogonal-Pade-activation-Unit-OPAU" class="headerlink" title="3.2 Orthogonal-Padé activation Unit (OPAU)"></a>3.2 Orthogonal-Padé activation Unit (OPAU)</h3><p>g(x)由有理函数G(x)的orthogonal-Padé近似定义为：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210618/11.png" alt=""></p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210618/12.png" alt=""></p><p>其中$f_t(x)$属于正交多项式集合。与PAU一样，可学习激活函数OPAU由(6)定义，其中$c_i;d_j;0≤i≤k;1≤j≤l$为可学习参数。参数的初始化是通过近似的形式的如ReLU, Leaky ReLU等。为了去掉G(x)的极点，提出如下的safe OPAU。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210618/13.png" alt=""></p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210618/14.png" alt=""></p><p>作者考虑了6种正交多项式基- Chebyshev(两种)，Hermite(两种)，Laguerre和Legendre多项式的基。关于这些多项式基的详细信息见表1。</p><h3 id="3-3-通过反向传播学习激活参数"><a href="#3-3-通过反向传播学习激活参数" class="headerlink" title="3.3 通过反向传播学习激活参数"></a>3.3 通过反向传播学习激活参数</h3><p>利用反向传播算法和梯度更新神经网络模型中的权值和偏差。这里也采用相同的方法更新激活函数的参数。作者已经在Pytorch和Tensorflow-Keras API实现了自动化更新参数。对输入x和参数$c_i’s、d_j’s$计算公式(6)的梯度如下:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210618/15.png" alt=""></p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210618/16.png" alt=""></p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210618/17.png" alt=""></p><h2 id="具有orthogonal-Pade激活以及函数近似的网络"><a href="#具有orthogonal-Pade激活以及函数近似的网络" class="headerlink" title="具有orthogonal-Padé激活以及函数近似的网络"></a>具有orthogonal-Padé激活以及函数近似的网络</h2><p>Orthogonal-Padé网络类似于Padé网络，即将具有PAU或safe PAU的网络替换为OPAU或safe OPAU。在本文中，将safe OPAUs视为不同正交基的激活函数，如表1所示。用(7)中给出的函数形式近似Leaky ReLU对可学习参数(多项式系数)进行初始化，初始化参数值如下表所示。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210618/18.png" alt=""></p><p>利用反向传播方法对网络参数进行了优化。作者对所有的网络都保持了PAU的类似设计，例如每层的权重共享和可学习激活参数。由式(5)可知，每层总共有(k+l)个额外参数。因此，如果网络中有L层，网络中就会有额外的L(k+L)个可学习参数。为了训练网络，作者采用了Leaky ReLU初始化(α=0.01)，而不是随机初始化方法。</p><p>使用正交基的一个主要优点是，与标准基相比，可以在运行时间上更快地找到多项式系数。此外，目前广泛使用的激活函数在大多数情况下是零中心的。因此作者在Padé和Orthogonal-Padé近似上施加一些条件，以使已知函数近似为零中心，并检查是否有任何对模型性能的优势(一个明显的优势是每一层的参数量减少了)。</p><p>为了使Padé以零为中心，将式(4)中的$a_0=0$替换，并计算其他参数。为了保证OPAU的safe，会有几个bad case，作者研究了所有可能的bad case。</p><p>例如，如果选择HP-1作为基，如果分子中的常数项为零，则安全的OPAU函数近似可以以零为中心。由式(6)和表1可知，$c_0-c_2+3c_4=0$。可以推导出以下情况:</p><p>case 1:</p><script type="math/tex; mode=display">c_0=c_2=c_4=0</script><p>case 2:</p><p>$c_0, c_2, c_4$其中一个等于0。例如，如果$c_0 = 0$，那么$c_2 = 3c_4$等等;</p><p>case 3:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210618/19.png" alt=""></p><p>在上述PAU和HP-1的所有情况下，作者已经在CIFAR10和CIFAR100数据集上对几个经典的模型进行了实验和测试（Leaky ReLU近似）。作者发现在大多数情况下，模型在top-1准确率下降了0.2%-0.6%。</p><p>此外，需要注意的是，具有safe OPAU激活函数的神经网络在C(K)中是dense的，其中K是$R_n$的一个紧凑子集，而C(K)是K上所有连续函数的空间。</p><h3 id="Proposition"><a href="#Proposition" class="headerlink" title="Proposition"></a>Proposition</h3><ul><li><p>设$\rho : R\to R$是任意连续函数。设$N_n^\rho$表示一类具有激活函数$\rho$的神经网络，输入层有n个神经元，输出层有1个神经元，隐层有任意数量的神经元。设$K\subseteq R_n$是compact的。当且仅当$\rho$是非多项式时，$N_n^\rho$在C(K)中是dense的。</p></li><li><p>设$\rho : R\to R$是任意连续函数，它至少在一点上是连续可微的，且在这一点上导数为非零。设$K\subseteq R<em>n$是compact的。那么在$C(K;R^m)$中，$NN^\rho</em>{n;m;n+m+2}$是dense的。</p></li></ul><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="5-1-CIFAR-100"><a href="#5-1-CIFAR-100" class="headerlink" title="5.1 CIFAR-100"></a>5.1 CIFAR-100</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210618/20.png" alt=""></p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210618/21.png" alt=""></p><h3 id="5-2-Tiny-Imagenet"><a href="#5-2-Tiny-Imagenet" class="headerlink" title="5.2 Tiny Imagenet"></a>5.2 Tiny Imagenet</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210618/22.png" alt=""></p><h3 id="5-3-VOC-2007"><a href="#5-3-VOC-2007" class="headerlink" title="5.3 VOC 2007"></a>5.3 VOC 2007</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210618/23.png" alt=""></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>[1].ORTHOGONAL-PADÉ ACTIVATION FUNCTIONS: TRAINABLE ACTIVATION FUNCTIONS FOR SMOOTH AND FASTER CONVERGENCE IN DEEP NETWORKS<br></p>]]></content>
      
      
      
        <tags>
            
            <tag> 全新激活函数 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>详细解读Attention-Based方法解决遮挡人脸识别问题</title>
      <link href="/2021/09/18/13/"/>
      <url>/2021/09/18/13/</url>
      
        <content type="html"><![CDATA[<p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210616/0.png" alt=""></p><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>在非约束性环境(如大量人群)中捕获的人脸照片，仍然对当前的人脸识别方法构成挑战，因为人脸经常被前景中的物体或人遮挡。然而，很少有研究涉及到识别部分面孔的任务。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210616/1.png" alt=""></p><p>本文提出了一种新的遮挡人脸识别方法，能够识别不同遮挡区域的人脸。通过将一个ResNet中间特征映射的attentional pooling与一个单独的聚合模块相结合来实现这一点。为了保证attention map的多样性，并处理被遮挡的部分，作者进一步对遮挡Face的常见损失函数进行了调整。实验表明，在多个benchmark下本文方法的性能优于所有baseline。</p><p>本文工作贡献可以概括为以下几点:</p><ul><li><p>以ResNet为例，利用attentional pooling和聚合网络提出了一种新的扩展，并使用2种适用于部分FR的常见损失函数进行训练；</p></li><li><p>在多个局部FR的详尽分析中表明，本文的改进大大提高了识别性能。</p></li></ul><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><h3 id="2-1-Network-Architecture"><a href="#2-1-Network-Architecture" class="headerlink" title="2.1 Network Architecture"></a>2.1 Network Architecture</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210616/2.png" alt=""></p><p>下图描述了partial FR方法，分为3个模块:Extract、Attend和Aggregate。</p><p>Extract模块从输入图像中提取特征图$F\in R^{20×20×1024}$和attention maps  $A\in R^{20×20×K}$，其中K表示attention maps的个数。</p><p>在Attend模块中，使用重新校准的attention maps将特征图合并为K个中间特征向量。</p><p>Aggregate模块将这些中间特征向量映射到联合特征空间中，得到最终特征向量$f\in R^{256}。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210616/3.png" alt=""></p><h4 id="1-Extract"><a href="#1-Extract" class="headerlink" title="1 Extract"></a>1 Extract</h4><p>受Comparator networks启发，作者使用了一个删减的ResNet-50架构，它在第4个block之后结束。因此，只进行了3次空间降采样，得到了大小为20×20的特征图，其中区域仍然具有很好的可区分性。与Comparator networks不同的是，在第3个block之后分离ResNet，以允许2个分支专注于各自的任务。而在第4个block之后直接得到F，然后再加上一个1×1的卷积以及ReLU激活函数获取a。具体架构总结如表1所示。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210616/4.png" alt=""></p><p>生成的attention maps应满足以下2个关键属性:<br>1) attention maps应是互斥的，即不同的attention maps聚焦于人脸图像的不同区域;<br>2) attention maps的激活与区域的可见性相关。</p><p>值得注意的是，implicitly-defined attention maps激活并不一定遵循人类定义的面部标志(如眼睛或鼻子)的直觉。</p><h4 id="2-Attend"><a href="#2-Attend" class="headerlink" title="2 Attend"></a>2 Attend</h4><p>和Comparator networks一样，attention maps A需要重新校准。Xie等人提出了基于集的FR归一化A的attentional pooling方法，对集合内的所有图像分别进行归一化，从而确保从A中激活程度最大的图像中提取出各自的信息。</p><p>本文作者只考虑一个单一的图像，并期望不同的attention maps是相关的，因为这些主要取决于脸部的区域，即，如果眼睛被遮挡，相应的attention maps应该包含低激活值。因此，建议使用无参数的重新标定：</p><p>首先，用sigmoid函数$f<em>{norm(·)}= sigmoid(·)$对A进行normalize。这样，每个attention maps的每个像素分别归一化为(0,1);此外，先使用Global Average Pooling (GAP)，然后使用$f</em>{ex(·)}= softmax(·)$，计算一个向量$s\in R^K}表示每个attention maps的重要性:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210616/5.png" alt=""></p><p>索引$i,j,k$表示第$k$个attention maps的第$i$行和第$j$列的像素。通过引入GAP获得了所有attention maps的全局信息，并利用softmax函数将其转化为指示各attention maps重要性的概率分布。接下来，将第$k$个自归一化的attention maps $A_k$与其相应的重要性$s_k$相乘，得到最终的重新校准的attention maps $A$。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210616/6.png" alt=""></p><p>因此，在重新校准中将每个attention maps中的局部信息与跨attention maps的全局信息结合在一起。</p><p>重新校准后，应用attentional pooling，得到K个特征描述子$v_k \in R^{1024}$:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210616/7.png" alt=""></p><p>这样，第$k$个特征描述符中就包含了对应attention maps $A_k$激活时$F$的信息。</p><h4 id="3-Aggregate"><a href="#3-Aggregate" class="headerlink" title="3 Aggregate"></a>3 Aggregate</h4><p>用Aggregate模块来总结partial FR模型。由于所有的特征描述符$v_k$依赖于它们对应的attention maps $A_k$聚焦于$F$内的不同区域，所以不可能进行直接聚合。因此，将每个$v_k$分别映射到一个联合特征空间$f_k\in R^{256}$，每个$v_k$使用一个单独的全连接层。</p><p>注意，由于每个$v_k$都在不同的特征空间中，所以权重不是共享的。由于$f_k$同样对身份信息进行编码，所以通过计算平均值得到最终的特征向量$f\in R^{256}$:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210616/8.png" alt=""></p><h3 id="2-2-Loss-Functions"><a href="#2-2-Loss-Functions" class="headerlink" title="2.2 Loss Functions"></a>2.2 Loss Functions</h3><p>为了训练模型，作者使用3个损失的加权和，其描述如下:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210616/9.png" alt=""></p><p>用$\lambda<em>{wCE}$、$\lambda</em>{wDIV}$和$\lambda<em>{REG}$表示超参数来平衡损失，$L</em>{REG}$为所有可训练权重的$L_2$范数。</p><h4 id="1-Weighted-Cross-Entropy-L-wCE"><a href="#1-Weighted-Cross-Entropy-L-wCE" class="headerlink" title="1 Weighted Cross-Entropy $L_{wCE}$"></a>1 Weighted Cross-Entropy $L_{wCE}$</h4><p>为了处理一些代表被遮挡区域的向量，从而降低相关性，作者提出了一种加权的softmax CrossEntropy(CE)。对于CE损失添加一个全连接层到每个特征向量$f<em>k$匹配训练数据集中类的数量。通过这种方法得到了K CE损失$L</em>{CE,K}$。为了得到最终加权CE损失，对每个$L_{CE,K}$及其重要性$s_k$进行了scale:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210616/10.png" alt=""></p><p>通过这种方式，该网络学习强调代表可见人脸区域的attention maps，同时减轻代表遮挡区域的attention maps的影响。需要注意的是，由于最后一个全连接层的权值是共享的，所以每个$f_k$的转换是相等的，因此，要保证它们同样编码身份信息，即位于相同的特征空间。此外，由于训练数据集中有大量的类，$f_k$作为瓶颈层提高了网络的泛化能力。</p><h4 id="2-Weighted-Diversity-Regularizer-L-wDIV"><a href="#2-Weighted-Diversity-Regularizer-L-wDIV" class="headerlink" title="2 Weighted Diversity Regularizer $L_{wDIV}$"></a>2 Weighted Diversity Regularizer $L_{wDIV}$</h4><p>多样性正则化的目的是确保attention maps的多样性，因为如果不进行正则化，网络容易倾向于只使用一个attention maps或生成K个相同的attention maps。因此作者使用多样性正则化算法来惩罚不同attention maps之间的相互重叠。首先，使用softmax函数将每个attention maps $A_k$自归一化为概率分布$P_k$:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210616/11.png" alt=""></p><p>接下来，计算所有$P<em>k$的像素级最大值，并得到所有像素的和。对于互不重叠的attention maps，这个和接近于1，可以计算加权多样性损失$L</em>{wDIV}$如下:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210616/12.png" alt=""></p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210616/13.png" alt=""></p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210616/14.png" alt=""></p><p>表2描述了LFW数据集上不同benchmark protocols的聚合精度。当考虑一个ResNet-50(没有微调)，它在训练期间从未暴露于部分脸，可以观察到标准FR模型非常容易受到partial faces的影响。通过对partial faces进行微调，该模型在partial protocols上表现得更好。ResNet-50在非non-centered protocols上的性能优于ResNet-41，但在centered protocols上的性能较差。作者认为这是由于ResNet-50包含更多可训练参数。因此，由于中心不是数据扩充的一部分，它更容易对训练过程中呈现的空间信息进行过拟合。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210616/15.png" alt=""></p><p>在图中，中心部分面非遮挡区域a的影响:partial - cross protocol。虽然识别左眼-右眼的准确性只受到a的轻微影响，但验证左眼-嘴是否属于同一身份被认为是最具挑战性的。总的来说可以得出结论，本文模型比所有centered: partial-cross的baseline更稳健。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>[1].ATTENTION-BASED PARTIAL FACE RECOGNITION<br>[2].<a href="https://github.com/stefhoer/PartialLFW">https://github.com/stefhoer/PartialLFW</a><br></p>]]></content>
      
      
      
        <tags>
            
            <tag> Attention-Based方法 </tag>
            
            <tag> 遮挡人脸识别问题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>详细解读：如何让EfficientNet更加高效、速度更快</title>
      <link href="/2021/09/18/12/"/>
      <url>/2021/09/18/12/</url>
      
        <content type="html"><![CDATA[<p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210612/1.png" alt=""></p><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>近年来，许多研究致力于提高图像分类训练和推理的效率。这种研究通常集中于提高理论效率，通常以每个FLOP的ImageNet验证精度来衡量。然而，事实证明，这些理论上的改进在实践中很难实现，特别是在高性能训练加速器上。</p><p>在这项工作中，作者关注的是在一个新的加速器类<strong>Graphcore IPU</strong>上提高最先进的EfficientNet模型的实际效率。本文主要通过以下方式扩展这类模型:</p><ul><li>将Depthwise CNN推广为Group CNN;</li><li>添加proxy-normalized激活，以使batch normalization性能与batch-independent statistics相匹配;</li><li>通过降低训练分辨率和低成本的微调来减少计算量。</li></ul><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210612/2.png" alt=""></p><p>作者发现这3种方法都提高了训练和推理的实际效率。</p><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><h3 id="2-1-Efficient-CNNs分析"><a href="#2-1-Efficient-CNNs分析" class="headerlink" title="2.1 Efficient CNNs分析"></a>2.1 Efficient CNNs分析</h3><p>在CNN的发展过程中，实际训练效率的提高是创新的重要力量。比如说，AlexNet的成功很大一部分因素便得益于GPU加速，ResNet的成功不仅可以归因于其良好的性能，而且其在GPU上的高吞吐量也相对比较高。</p><p>最近，在理论效率方面也取得了重大改进。最引人注目的创新是在空间操作中引入Group卷积和Depthwise卷积。ResNet-50单独引入Group卷积可以提高理论效率。类似地，通过将Group规模减少到1，即利用Depthwise卷积，相对于原始的CNN模型也实现了理论效率的提高。特别是，该方法为实现基于“mobile”级别的落地应用提供了可能。</p><p>通过使用NAS直接减少FLOPs进一步提高了这些理论上的效率增益。这带来了整个模型尺度范围内的效率提高，从mobile-sized的模型如MobileNetV3和MNasNet 到大型模型如NASNet和AmoebaNet。</p><p>值得注意的是，在ImageNet模型的最高精度前100名的所有NAS模型都使用了某种形式的Group卷积或Depthwise卷积，进一步突出了这些操作相对于CNN操作的优势，在高效的MNasNet基础上，EfficientNet进一步改进了训练方法并扩展到更大的模型，以在FLOP范围内实现SOTA性能。</p><p>虽然低功耗cpu的高效模型通常能实现实际改进，但这些模型通常难以将理论收益转化为高性能硬件上更高的训练吞吐量。例如，虽然EfficientNets在理论训练效率方面远远优于ResNets，但当考虑到GPU上的实际训练效率时经常被发现表现不佳。最近的一些工作也已经开始使用NAS来优化GPU的实际效率。</p><h3 id="2-2-硬件角度考虑与分析"><a href="#2-2-硬件角度考虑与分析" class="headerlink" title="2.2 硬件角度考虑与分析"></a>2.2 硬件角度考虑与分析</h3><p>在研究模型的实际效率时，了解它所运行的硬件的特征是很重要的。关于这个问题的讨论通常主要集中在峰值计算速率上，以每秒浮点运算(FLOPS)衡量，这是计算操作的理论最大速率。虽然峰值率是需要考虑的一个重要因素，但了解实现峰值率所需的假设也同样重要，例如，计算的结构和数据的可用性。</p><p>计算结构很重要，因为现代硬件通常使用向量指令，允许用一条指令计算给定长度的点积。然而，如果计算的体系结构不能使这些向量指令被填满，那么FLOPs就可能被浪费掉。此外，如果数据不能立即在计算引擎上获得，那么将需要循环来移动它。这种操作将高度依赖于内存的带宽或者位宽。</p><p>对内存带宽的依赖依赖于模型，可以通过计算与数据传输的比率来表征，即算术<strong>arithmetic intensity</strong>——在这种情况下，低<strong>arithmetic intensity</strong>强度的操作更依赖于内存带宽。对于一个简单的Group卷积，<strong>arithmetic intensity</strong>强度随着Group大小、Kernel大小、field大小和Batch大小单调地增加。值得注意的是，这意味着Group卷积和Depthwise卷积在Group较小时的效率更可能受到可用内存带宽的限制。</p><p>在这项工作中，作者使用了一种新的硬件加速器<strong>Graphcore IPU</strong>。这种加速器与通常用于神经网络训练的GPU有很大的区别。IPU计算在芯片上分布在1472个核心中，尽管它的指令仍然是向量化的，但要充分利用计算引擎，只需要16项的点积即可。这有助于减少对计算结构的依赖。此外，IPU拥有超过900MB的高带宽片上内存，远远超过其他硬件。这大大降低了低<strong>arithmetic intensity</strong>强度操作的代价。</p><p>为了最大化IPU上的性能，保持尽可能多的工作内存(例如芯片上的激活状态)变得非常重要。这自然促进了更小批次的使用、内存节约优化和分布式处理的创新形式。同时，它确实需要重新考虑使用BN，因为在视觉模型中，最常见的归一化方法它很依赖于大的Batchsize。</p><h2 id="本文方法"><a href="#本文方法" class="headerlink" title="本文方法"></a>本文方法</h2><h3 id="3-1-改用Group卷积"><a href="#3-1-改用Group卷积" class="headerlink" title="3.1 改用Group卷积"></a>3.1 改用Group卷积</h3><p>NAS方法倾向于将它们的spatial卷积分组，通常分组大小为G=1(Depthwise卷积)。而Depthwise卷积具有很低的FLOP和参数，使用G&gt;1作为一个更大的Group将更有效地利用现代硬件加速器:</p><ul><li>(i) 增加<strong>arithmetic intensity</strong>强度;</li><li>(ii) 增加点积的长度(用于卷积)，允许使用更大的向量指令。</li></ul><p>作者的目的是研究在EfficientNet模型中增加spatial卷积的Group大小所涉及的权衡问题。单单增加G就会增加参数量和FLOPs。因此，为了保持相似的模型复杂度，作者相应地降低了扩展比（扩展比定义为输入到first pointwise CNN和spatial CNN之间的通道比）。这类似于ResNeXt的Flop等效扩展。</p><p>因此，<strong>对于相同的FLOP具有更大G的网络将更窄，更窄的网络模型将通过减少存储激活状态的大小和使用更大的BatchSize而获得计算优势</strong>。请注意，虽然这种补偿的目的是保持总FLOPs和参数量，但为简单起见，作者只在全局级别更改扩展比率。因此，并不需要保持与深度完全相同的参数和FLOPs分布。</p><p>与EfficientNet一样，其他NAS派生的架构通常只使用depthwise卷积，这表明depthwise卷积在验证准确性方面是最优的。在ResNeXts中，在保持总FLOPs的同时增加G会导致验证准确性下降。这也表明与类似G&gt;1的网络对比G=1的vanilla EfficientNet将实现更高的准确度。 然而，作者希望改进的网络提供更好的性能和训练时间之间的权衡。因此对EfficientNet B0和B2的Group规模在G=1和G=64之间进行了测试。</p><h3 id="3-2-Batch-Independent-Normalization"><a href="#3-2-Batch-Independent-Normalization" class="headerlink" title="3.2 Batch-Independent Normalization"></a>3.2 Batch-Independent Normalization</h3><h4 id="BN的问题在哪？"><a href="#BN的问题在哪？" class="headerlink" title="BN的问题在哪？"></a>BN的问题在哪？</h4><p>我们都知道BN通常应用于没有归一化的pre-activations X进而产生归一化pre-activations Y，然后再进行仿射变换和非线性$\phi$，最终产生post-activations Z。形式上，每个通道c:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210612/3.png" alt=""></p><p>式中BN的归一化确保了Y被规范化，这意味着每个通道c的均值和单位方差都为零，因此BN对于将模型扩展到大型和深度模型是成为了可能:</p><ul><li><p>通过确保非线性$\phi$在每个通道中“sees”接近归一化的数据分布，$\phi$可以有效地形成非线性的分布。因此，额外的层可以增加表达能力，网络可以有效地利用其整个深度。这与$\phi$会“see”一个“collapsed”的数据分布的情况相反，这样它会在一阶上很好地近似于一个关于这个分布的线性函数;</p></li><li><p>通过保证不同通道的方差接近相等，网络可以有效地利用其整个带宽。这与一种情况相反，在这种情况下，一个单一的通道会任意支配其他渠通道，从而成为唯一的通道被后续层“seen”。</p></li></ul><p>尽管这一基本原则取得了实际的成功应用，但BN对小batchsize数据的依赖有时会产生问题。最值得注意的是，当batchsize较小或数据集较大时，来自小batchsize统计数据$(\mu_c \sigma_c)$中噪声的正则化可能会过大或不必要，从而导致性能下降。</p><h4 id="突破点在哪？"><a href="#突破点在哪？" class="headerlink" title="突破点在哪？"></a>突破点在哪？</h4><p>为了解决这些问题，研究者们也提出了各种Batch-Independent相关的归一化技术:层归一化(LN)、组归一化(GN)、实例归一化(IN)、权重归一化(WN)、权重标准化(WS)、在线归一化(ON)、滤波器响应归一化(FRN)、EvoNorm等。虽然这些技术在其他环境中很有用，但在本工作中，没有一种技术能够缩小与大 Batch BN的性能差距，重点关注在ImageNet上使用RMSProp训练的EfficientNets。</p><p>这也促使作者重新思考如何执行独立于batch的Norm，并在工作中提出Proxy Normalized Activations。在本研究中，作者提出了一个假设，即除了提高对小batch的依赖外，与batch无关的归一化还应保持每个通道中归一化预激活Y的BN原则。</p><p>这一假设的第1个理由是BN的归纳偏差。第2个理由是，在更实际的层面上，BN被用于架构搜索，比如产生了EfficientNet模型系列的搜索。因此，坚持相同的标准化原则可以避免重新执行这些搜索。</p><p>为了保留BN原则，同时消除对Batchsize的依赖，作者扩展的工作如下:</p><ul><li><p>(i)将Eq.(1)的BN步骤替换为基于LN或GN的Batch无关的标准化步骤;</p></li><li><p>(ii)将式(2)的激活步骤替换为<em>proxy-normalized activation</em>步骤。</p></li></ul><p>这一步通过将$\phi(\gamma Y<em>{…c} + \beta_c)$与$\phi(\gamma \tilde Y</em>{c} + \beta<em>c)$同化，使$\phi(\gamma Y</em>{…c} + \beta<em>c)$归一化，其中$\tilde Y</em>{c} \sim N(\tilde \beta_c, (1+\tilde \gamma_c)^2)$是一个高斯proxy变量，具有均值$\tilde \beta_c$和方差$(1+\tilde \gamma_c)^2$，如果选择LN作为Batch无关的归一化，对于每个batch元素b和通道c，这表示为:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210612/4.png" alt=""></p><p>其中，$\tilde Y_{c} \sim N(\tilde \beta_c, (1+\tilde \gamma_c)^2)$，$\mu_b, \sigma_b$是X的batch元素b在空间和通道维度上的均值和标准差。</p><p>当与LN结合时，这种激活的proxy标准化(PN)迭代确保预激活Y保持接近于标准化（论文中有推导）。</p><h3 id="3-3-Image分辨率"><a href="#3-3-Image分辨率" class="headerlink" title="3.3 Image分辨率"></a>3.3 Image分辨率</h3><p>引入全局平均池化允许CNN对任意分辨率的输入进行操作。虽然这已经在图像分割等任务中得到了探索，但在图像分类中，其影响仍有待更加深入的挖掘。EfficientNet模型将图像分辨率作为一个可调的超参数，使用更大的图像来训练更大的网络。Hoffer等人同时对多个图像尺寸的网络进行训练发现：</p><ul><li>i) 大分辨率可以加速训练以达到目标精度</li><li>ii) 大分辨率可以提高相同训练的最终性能。</li></ul><p>或许与目标最接近的是，<strong>Howard建议从低分辨率图像开始训练，在训练过程中逐步增加图像的大小，以减少总的训练时间</strong>。</p><p>Touvron等人研究表明，少量的微调可以使网络收敛的更好。微调步骤只需要对网络的最后部分起作用，而且只需要几个epoch就可以提高总体精度。因此，与其他训练相比，微调的计算成本几乎可以忽略不计。</p><p>从这一研究中获得了灵感，研究了在低分辨率图像上训练的网络的微调，并从效率的角度将其推广到更大的分辨率。在训练过程中使用较小的图像可以使用更少的内存更快地训练出一个给定的模型，或者在相同的时间内训练一个较大的模型。为了测试这一想法，作者在固有的EfficientNet图像大小以大约为原来像素数的一半进行训练，这里表示为半分辨率。结果与EfficientNet模型的FLOPs大致相当。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210612/5.png" alt=""></p><p>然后，微调和测试的图像尺寸范围高达700x700。在选择用于验证的精确分辨率时，注意到性能可能会受到混叠效应的影响。这种人工干扰是由于非对称下采样层的位置造成的，其中输入的维度是奇数，这取决于输入分辨率在不同的深度上决定的。作者还发现在训练和测试之间保持这些降采样层的位置一致是很重要的。这可以通过选择测试分辨率$r<em>{test}$来实现，使$r</em>{train}≡r_{test}(mod 2^n)$，其中n是模型中的下采样层数(对于EfficientNet, n=5)。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="4-1-Group卷积的影响"><a href="#4-1-Group卷积的影响" class="headerlink" title="4.1 Group卷积的影响"></a>4.1 Group卷积的影响</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210612/6.png" alt=""></p><p>通过上表可以看出虽然组大小为G=4的情况下在这些测试中获得了最好的准确性，但发现组大小为G=16的增加的计算效益在实践中产生了比较好的权衡。</p><h3 id="4-2-Proxy-Normalized-Activations的影响"><a href="#4-2-Proxy-Normalized-Activations的影响" class="headerlink" title="4.2 Proxy-Normalized Activations的影响"></a>4.2 Proxy-Normalized Activations的影响</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210612/7.png" alt=""></p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210612/8.png" alt=""></p><p>从表中可以看出，对于B0和B2，在G=16上直接比较2种方法时，LN+PN得到的准确率与BN得到的准确率最匹配。</p><h3 id="4-3-分辨率的影响"><a href="#4-3-分辨率的影响" class="headerlink" title="4.3 分辨率的影响"></a>4.3 分辨率的影响</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210612/9.png" alt=""></p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210612/10.png" alt=""></p><h3 id="4-4-推理效率"><a href="#4-4-推理效率" class="headerlink" title="4.4 推理效率"></a>4.4 推理效率</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210612/11.png" alt=""></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>[1].Making EfficientNet More Efficient<br></p>]]></content>
      
      
      
        <tags>
            
            <tag> EfficientNet </tag>
            
            <tag> 效率新秀 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>详细解读：如何再CNN模型中插入Transformer后速度不变精度剧增？</title>
      <link href="/2021/09/18/11/"/>
      <url>/2021/09/18/11/</url>
      
        <content type="html"><![CDATA[<p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210611/1.png" alt=""></p><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>本文工作解决了Multi-Head Self-Attention(MHSA)中由于计算/空间复杂度高而导致的vision transformer效率低的缺陷。为此，作者提出了分层的MHSA(H-MHSA)，其表示以分层的方式计算。</p><p>具体来说，H-MHSA首先通过把图像patch作为tokens来学习小网格内的特征关系。然后将小网格合并到大网格中，通过将上一步中的每个小网格作为token来学习大网格中的特征关系。这个过程多次迭代以逐渐减少token的数量。</p><p>H-MHSA模块很容易插入到任何CNN架构中，并且可以通过反向传播进行训练。作者称这种新的Backbone为<strong>TransCNN</strong>，它本质上继承了transformer和CNN的优点。实验证明，<strong>TransCNN</strong>在图像识别中具有最先进的准确性。</p><h2 id="Vision-Transformer回顾"><a href="#Vision-Transformer回顾" class="headerlink" title="Vision Transformer回顾"></a>Vision Transformer回顾</h2><p>大家应该都很清楚Transformer严重依赖MHSA来建模长时间依赖关系。假设$X\in R^{N×C}$为输入，其中N和C分别为Token的数量和每个Token的特征维数。这里定义了Query $Q=XW^q$、key $K=XW^k$和 value $V=XW^v$，其中$Wq\in R^{C×C}$, $Wk\in R^{C×C}$, $Wv\in R^{C×C}$为线性变换的权重矩阵。在假设输入和输出具有相同维度的情况下，传统的MHSA可以表示为:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210611/2.png" alt=""></p><p>其中$\sqrt d$表示近似归一化，对矩阵行应用Softmax函数。注意，为了简单起见在这里省略了多个Head的概念。在上式中$QK^T$的矩阵乘积首先计算每对Token之间的相似度。然后，在所有Token的组合之上派生出每个新Token。MHSA计算后，进一步添加残差连接以方便优化，如:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210611/3.png" alt=""></p><p>其中，$W^p\in R^{C×C}$为特征映射的权重矩阵。最后，采用MLP层增强表示，表示形式为：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210611/4.png" alt=""></p><p>其中Y表示transformer block的输出。</p><p>有前面的等式可以得到MHSA的计算复杂度：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210611/5.png" alt=""></p><p>很容易推断出空间复杂度(内存消耗)。对于高分辨率的输入，$O(N^2)$可能变得非常大，这限制了Transformer在视觉任务中的适用性。基于此，本文的目标是在不降低性能的情况下降低这种复杂性，并保持全局关系建模的能力。</p><p>Transformer Block Pytorch实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Mlp</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_features, hidden_features=<span class="literal">None</span>, out_features=<span class="literal">None</span>, act_layer=nn.GELU, drop=<span class="number">0.</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        out_features = out_features <span class="keyword">or</span> in_features</span><br><span class="line">        hidden_features = hidden_features <span class="keyword">or</span> in_features</span><br><span class="line">        self.fc1 = nn.Linear(in_features, hidden_features)</span><br><span class="line">        self.act = act_layer()</span><br><span class="line">        self.fc2 = nn.Linear(hidden_features, out_features)</span><br><span class="line">        self.drop = nn.Dropout(drop)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.fc1(x)</span><br><span class="line">        x = self.act(x)</span><br><span class="line">        x = self.drop(x)</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        x = self.drop(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Attention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="comment"># Muliti-Head Self-Attention Block</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, dim, num_heads=<span class="number">8</span>, qkv_bias=<span class="literal">False</span>, qk_scale=<span class="literal">None</span>, attn_drop=<span class="number">0.</span>, proj_drop=<span class="number">0.</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.num_heads = num_heads</span><br><span class="line">        head_dim = dim // num_heads</span><br><span class="line">        self.scale = qk_scale <span class="keyword">or</span> head_dim ** -<span class="number">0.5</span></span><br><span class="line">        <span class="comment"># 输出 Q K V</span></span><br><span class="line">        self.qkv = nn.Linear(dim, dim * <span class="number">3</span>, bias=qkv_bias)</span><br><span class="line">        self.attn_drop = nn.Dropout(attn_drop)</span><br><span class="line">        self.proj = nn.Linear(dim, dim)</span><br><span class="line">        self.proj_drop = nn.Dropout(proj_drop)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        B, N, C = x.shape</span><br><span class="line">        qkv = self.qkv(x).reshape(B, N, <span class="number">3</span>, self.num_heads, C // self.num_heads).permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line">        q, k, v = qkv[<span class="number">0</span>], qkv[<span class="number">1</span>], qkv[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># q matmul k.T</span></span><br><span class="line">        attn = (q @ k.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) * self.scale</span><br><span class="line">        attn = attn.softmax(dim=-<span class="number">1</span>)</span><br><span class="line">        attn = self.attn_drop(attn)</span><br><span class="line">        <span class="comment"># attn&#x27; matmul v ==&gt; output</span></span><br><span class="line">        x = (attn @ v).transpose(<span class="number">1</span>, <span class="number">2</span>).reshape(B, N, C)</span><br><span class="line">        x = self.proj(x)</span><br><span class="line">        x = self.proj_drop(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Block</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="comment"># Transformer Encoder Block</span></span><br><span class="line">    <span class="comment"># Embedded Patches ==&gt; Layer Norm ==&gt; Muliti-Head Attention + ==&gt; Layer Norm ==&gt; MLP + ==&gt;</span></span><br><span class="line">    <span class="comment">#                 |_________________________________________|     |__________________|</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, dim, num_heads, mlp_ratio=<span class="number">4.</span>, qkv_bias=<span class="literal">False</span>, qk_scale=<span class="literal">None</span>, drop=<span class="number">0.</span>, attn_drop=<span class="number">0.</span>, drop_path=<span class="number">0.</span>, act_layer=nn.GELU, norm_layer=nn.LayerNorm</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.norm1 = norm_layer(dim)</span><br><span class="line">        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)</span><br><span class="line">        <span class="comment"># 进行稀疏化操作，可以得到更好的结果</span></span><br><span class="line">        self.drop_path = DropPath(drop_path) <span class="keyword">if</span> drop_path &gt; <span class="number">0.</span> <span class="keyword">else</span> nn.Identity()</span><br><span class="line">        self.norm2 = norm_layer(dim)</span><br><span class="line">        mlp_hidden_dim = <span class="built_in">int</span>(dim * mlp_ratio)</span><br><span class="line">        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = x + self.drop_path(self.attn(self.norm1(x)))</span><br><span class="line">        x = x + self.drop_path(self.mlp(self.norm2(x)))</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><h2 id="Hierarchical-Multi-Head-Self-Attention"><a href="#Hierarchical-Multi-Head-Self-Attention" class="headerlink" title="Hierarchical Multi-Head Self-Attention"></a>Hierarchical Multi-Head Self-Attention</h2><p>在这里，作者介绍了如何使用H-MHSA降低MHSA的计算/空间复杂度。这里不是在整个输入中计算注意力，而是以分层的方式计算，这样每个步骤只处理有限数量的Token。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210611/6.png" alt=""></p><p>图b为H-MHSA的范式。假设输入特征映射$X\in R^{H_0×W_0×C}$的高度为$H_0$，宽度为$W_0$，有$N=H_0×W_0$。然后将特征图划分为大小为$G_0×G_0$的小网格，并将特征图Reshape为:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210611/7.png" alt=""></p><p>当$Q=X’W^q$, $K=X’W^k$和$V=X’W^v$时，式(1)生成局部注意$A_0$。为了简化网络优化，这里将$A_0$ Reshape为X的shape：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210611/8.png" alt=""></p><p>并添加一个残差连接：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210611/9.png" alt=""></p><p>由于$A_0$是在每个小$G_0×G_0$网格内计算的，因此计算/空间复杂度显著降低。</p><p>对于第i步(i&gt;0)，将第(i-1)步处的每个更小的网格$G<em>{i−1}×G</em>{i−1}$视为一个Token，这可以简单地通过对注意力特征$A_{i−1}$进行降采样来实现:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210611/10.png" alt=""></p><p>其中$MaxPool G<em>{i−1}(·)$和$AvgPoolG</em>{i−1}(·)$分别表示使用最大池化和平均池化(内核大小和步长为$G<em>{i−1}$)将样本$A</em>{i−1}$降为$G<em>{i−1}$次。因此，有$A’</em>{i-1}\in R^{H<em>i×W_i×C}$, 其中$H_i=H_0/(G_0G_1···G</em>{i−1})$，$W<em>i=W_0/(G_0G_1···G</em>{i−1})$。然后，将$A’_{i-1}$划分为$G_i×G_i$网格，并将其Reshape为:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210611/11.png" alt=""></p><p>当$Q=A’<em>{i−1}W^q$, $K=A’</em>{i−1}W^k$, $V=A’_{i−1}W^v$时，方程(1)获取注意特征$A_i$。$A_i$最终被Reshape为为输入的shape，比如：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210611/12.png" alt=""></p><p>并添加一个残差连接：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210611/13.png" alt=""></p><p>这个过程不断迭代，直到$H_i×W_i$足够小而不能在进行split。H-MHSA的最终输出为：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210611/14.png" alt=""></p><p>如果Upsample(·)表示将注意力特征上采样到原始大小，则$W^p$与Equ(2)含义相同， M为最大步数。通过这种方式，H-MHSA可以等价于传统的MHSA来模拟全局关系。</p><p>很容易证明，在所有$G_i$都相同的假设下，H-MHSA的计算复杂度近似:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210611/15.png" alt=""></p><p>与MHSA的计算复杂度相比较，本文所提方法显著降低了计算复杂度。</p><h2 id="将Transformer插入到CNN中"><a href="#将Transformer插入到CNN中" class="headerlink" title="将Transformer插入到CNN中"></a>将Transformer插入到CNN中</h2><p>本文和之前将CNN与Transformer的方法一样遵循普遍做法，在网络Backbone中保留3D特征图，并使用全局平均池化层和全连接层来预测图像类别。这与现有的依赖另一个1D类标记进行预测的Transformer不同。</p><p>作者还观察到以往的Transformer网络通常采用GELU函数进行非线性激活。然而，在网络训练中，<strong>GELU函数非常耗费内存</strong>。作者通过经验发现，SiLU的功能与GELUs不相上下，而且更节省内存。因此，TransCNN选择使用SiLU函数进行非线性激活。</p><blockquote><p>作者做了一组实验。在ImageNet验证集上，当训练为100个epoch时，提出的具有SiLU的跨网络网络(TransCNN)在ImageNet验证集上获得80.1%的top-1精度。GELU的TransCNN得到79.7%的top-1精度，略低于SiLU。当每个GPU的batchsize=128时，SiLU在训练阶段占用20.2GB的GPU内存，而GELU占用23.8GB的GPU内存。</p></blockquote><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210611/16.png" alt=""></p><p>TransCNN的总体架构如图所示。</p><p>在TransCNN的开始阶段使用了2个连续的$3\times 3$个卷积，每个卷积的步长为2，将输入图像降采样到1/4的尺度。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210611/17.png" alt=""></p><p>然后，将H-MHSA和卷积块交替叠加，将其分为4个阶段，分别以1/4,1/8,1/16,1/32的金字塔特征尺度进行划分。这里采用的卷积模块是广泛使用的<strong>Inverted Residual Bottleneck</strong>(IRB，图c)，卷积是深度可分离卷积。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210611/18.png" alt=""></p><p>在每个阶段的末尾，作者设计了一个简单的二分支降采样块(TDB，图d)。它由2个分支组成:一个分支是一个典型的$3\times 3$卷积，步长为2;另一个分支是池化层和$1\times 1$卷积。在特征降采样中，这2个分支通过元素求和的方式融合，以保留更多的上下文信息。实验表明，<strong>TDB的性能优于直接降采样</strong>。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210611/19.png" alt=""></p><p>TransCNN的详细配置如表所示。提供了2个版本的TransCNN: TransCNN-Small和TransCNN-Base。TransCNN-Base的参数个数与ResNet50相似。需要注意的是，这里只采用了最简单的参数设置，没有进行仔细的调优，以证明所提概念H-MHSA和trannn的有效性和通用性。例如，作者使用典型的通道数，即64、128、256和512。MHSA中每个Head的尺寸被设置为64。作者提到对这些参数设置进行细致的工程调整可以进一步提高性能。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="5-1-ImageNet图像分类"><a href="#5-1-ImageNet图像分类" class="headerlink" title="5.1 ImageNet图像分类"></a>5.1 ImageNet图像分类</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210611/20.png" alt=""></p><p>通过上表可以看出，将H-MHSA插入到相应的卷积模型中，可以以很少的参数量和FLOPs换取很大的精度提升。</p><h3 id="5-2-MS-COCO-2017目标检测"><a href="#5-2-MS-COCO-2017目标检测" class="headerlink" title="5.2 MS-COCO 2017目标检测"></a>5.2 MS-COCO 2017目标检测</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210611/21.png" alt=""></p><p>通过上表可以看出，在比ResNet50更少的参数量的同时，RetinaNet的AP得到了很大的提升。</p><h3 id="5-3-MS-COCO-2017语义分割"><a href="#5-3-MS-COCO-2017语义分割" class="headerlink" title="5.3 MS-COCO 2017语义分割"></a>5.3 MS-COCO 2017语义分割</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210611/22.png" alt=""></p><p>通过上表可以看出，在比ResNet50更少的参数量的同时，Mask R-CNN的AP得到了很大的提升。可见本文所提方法的实用性还是很强的。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>[1].Transformer in Convolutional Neural Networks<br></p>]]></content>
      
      
      <categories>
          
          <category> Transformer </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
            <tag> 卷积CNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>太顶流！Scaling ViT将ImageNet Top-1 Acc刷到90.45%！</title>
      <link href="/2021/09/18/10/"/>
      <url>/2021/09/18/10/</url>
      
        <content type="html"><![CDATA[<p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210609/1.png" alt=""></p><blockquote><p>本文改进了ViT的架构和训练，减少了内存消耗并提高了模型的准确性！最终成功训练了一个具有20亿参数的ViT模型：ViT-G，在ImageNet上达到了90.45%的Top-1准确率.<br><strong>作者单位</strong>：谷歌大脑（苏黎世），有原ViT一作和二作</p></blockquote><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>视觉Transformer(ViT)等基于注意力的神经网络最近在许多计算机视觉基准测试中取得了最先进的结果。比例是获得出色结果的主要因素，因此，了解模型的scaling属性是有效设计的关键。虽然已经研究了扩展Transformer语言模型的规律，但尚不清楚Vision Transformers如何扩展。</p><p>为了解决这个问题，作者向上和向下扩展ViT模型和数据，并描述错误率、数据和计算之间的关系。在此过程中，作者改进了ViT的架构和训练，减少了内存消耗并提高了结果模型的准确性。结果，作者成功地训练了一个具有20亿个参数的ViT模型，该模型在ImageNet上达到了90.45%的Top-1准确率。该模型在小样本学习上也表现良好，例如，在ImageNet上每类只有10个examples的情况下可以达到84.86%的Top-1准确率。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210609/2.png" alt=""></p><h2 id="主要结论"><a href="#主要结论" class="headerlink" title="主要结论"></a>主要结论</h2><p>作者首先介绍了关于扩展趋势的主要结果。在接下来的实验中在多达30亿个weakly-labelled图像上训练了几个ViT模型。作者期间改变了体系结构的大小，训练图像的数量和训练时间。为了评估模型学习的质量，作者测量了一下方面：</p><ul><li><p>通过在冻结的权值上训练线性分类器来实现的few-shot transfer；</p></li><li><p>通过对所有数据进行fine-tuning来实现对整个模型的transfer。</p></li></ul><h3 id="2-1-将计算、模型和数据一起放大"><a href="#2-1-将计算、模型和数据一起放大" class="headerlink" title="2.1 将计算、模型和数据一起放大"></a>2.1 将计算、模型和数据一起放大</h3><p>下图显示了ImageNet上的10-shot线性评估和微调评估。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210609/3.png" alt=""></p><p>对于模型大小和数据大小的每个组合，作者预训练了不同数量的Steps。在图中，连接点表示经过不同步骤训练的同一模型。</p><p>结论：</p><ul><li><p>首先，将计算、模型和数据一起放大可以提高模型表征能力。</p></li><li><p>其次，模型的大小会影响表征能力。</p></li><li><p>第三，大模型受益于额外的数据，甚至需要超过1 Billion图像。</p></li></ul><h3 id="2-2-Double-saturating-power-law"><a href="#2-2-Double-saturating-power-law" class="headerlink" title="2.2 Double-saturating power law"></a>2.2 Double-saturating power law</h3><p>对于超过2个数量级的计算，计算和性能之间的关系遵循power-law($E=aC^b$)，在log-log图上形成一条直线。然而，在compute spectrum的2端都观察到了饱和。在计算的最后最大模型误差率并不趋向于零错误率。如果按照作者的观察进行推断，无限容量模型将得到一个非零误差。</p><p>其实之前生成模型也观察到了这种类似的效应,作者将这种残差称为任务的<strong>不可约熵</strong>。通过作者绘制错误率图像，信息论的解释并不适用，但作者的观察支持ImageNet的基本性能上限的概念。根据该定律，这个饱和对应于错误率的一个附加常数c:$E = aC^b+c$。</p><p>在compute spectrum的前面看到小模型的饱和；最小模型的性能优于幂律模型的预测。出现这种饱和是因为即使是普通的解决方案也可以实现非零误差。例如，预测大多数类(几乎为零计算)将获得与其在测试集中出现频率相关的精度。在生成模型中没有观察到这个下界，要么是因为它们的最小模型大到足以避免这个区域，要么是因为对数损失在性能比精度更差的情况下达到饱和(最终会达到饱和)。这个饱和对应于x轴上的位移$E=a(C+d)^{-b}+c$中的d。这个常数表明零计算模型仍将获得非零精度。</p><h3 id="2-3-大模型的样本效率更高"><a href="#2-3-大模型的样本效率更高" class="headerlink" title="2.3 大模型的样本效率更高"></a>2.3 大模型的样本效率更高</h3><p>下图显示了预处理过程中“seen”的图像总数(批量大小乘以step数)的表征质量。除了ImageNet微调和公共验证集上的线性10-shot结果，作者还给出了ImageNet微调模型的结果，其中ImageNet-v2测试集作为泛化性的指标。在这幅图中展示了对30亿张图像进行预训练的3个ViT模型的结果。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210609/4.png" alt=""></p><p>作者观察到<strong>更大的模型样本效率更高</strong>，可以用更少的图像达到相同的错误率水平。对于10-shot Ti/16模型需要看到近100倍以上的图像来匹配L/16模型的表征质量。当进行微调时，这个因数从100降低到大约20。实验结果表明，在有足够的数据的情况下，<strong>训练一个更大的模型以较少的step训练是更好的</strong>。</p><h3 id="2-4-ViT-G-14结果"><a href="#2-4-ViT-G-14结果" class="headerlink" title="2.4 ViT-G/14结果"></a>2.4 ViT-G/14结果</h3><p>ViT-G/14比以前最好的ViT-H/14模型有很大的优势(超过5%)，每类10个样本的准确率达到84.86%。每类10张图片不到1%ImageNet数据(每个类13个例子)，通常用于自监督和半监督学习。作为参考，下图显示了2个最先进的自监督学习模型，SimCLR v2和BYOL，使用了1%的ImageNet数据。但是请注意，这些方法有很大的不同:ViT-G/14使用大量缺乏监督的数据，并且只进行一次预训练，然后转移到不同的任务中。同时，自监督学习模型使用无标记但领域内的数据进行训练，并以单个任务为目标。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210609/5.png" alt=""></p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210609/6.png" alt=""></p><p>表1显示了其余基准测试的结果。ViT-G/14在ImageNet上达到90.45%的top-1精度，设置新的艺术状态。在ImageNet-v2上，ViT-G/14比基于EfficientNet-L2的Noisy Student模型改进了3%。对于ReaL, ViT-G/14仅比ViT-H和BiT-L略胜一筹，这再次表明<strong>ImageNet分类任务很可能达到饱和点</strong>。对于ObjectNet来说，ViT-G/14比BiT-L表现要好很多，比Noisy Student好2%，但比CLIP落后2%。请注意，与其他方法不同，CLIP不会在ImageNet上进行微调，而是直接在ObjectNet上进行评估，这可能提高了它的健壮性。最后，当将ViT-G/14模型转移到VTAB时，它在所有任务中只使用一个超参数就能得到一致更好的结果。</p><h2 id="Method-details"><a href="#Method-details" class="headerlink" title="Method details"></a>Method details</h2><p>作者对ViT模型和训练过程提出了一些改进。这些改进大都很容易实现，并且可以显著提高内存利用率和模型质量。它们允许单独使用数据并行性来训练ViT-G/14，整个模型拟合在一个单独的模型上TPUv3核心。</p><h3 id="3-1-“Head”解耦权重衰减"><a href="#3-1-“Head”解耦权重衰减" class="headerlink" title="3.1 “Head”解耦权重衰减"></a>3.1 “Head”解耦权重衰减</h3><p>在低数据条件下，权值衰减对model adaptation有显著影响。作者对这一现象进行了mid-size规模的研究。</p><p>作者发现，对于模型中的最后一个线性层(“Head”)和其余的权重(“Body”)可以从解耦权值衰减强度中获益。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210609/7.png" alt=""></p><p>上图展示了这种效应:在JFT-300M上训练一个collection ViT-B/32模型，每个cell对应不同的head/body权重衰减值。对角线对应于对2个衰变使用相同的值。可以观察到最佳的性能表现是非对角线的(例如，head和body的权值衰减是解耦的)。有趣的是，作者观察到head的权值衰减尽管提高了迁移性能，但是也降低了pre-training(upstream)任务的表现。</p><p>对于这种现象，作者还没有一个完整的解释。然而，如果假设Head的权值衰减越大，代表就会在类别之间有更大的差距，从而更好地few-shot adaptation。这与支持向量机背后的主要思想相似。这种大的衰减使得上游预训练中更难获得高精确度，但作者的主要目标是高质量的迁移。</p><h3 id="3-2-通过删除-class-token来节省内存"><a href="#3-2-通过删除-class-token来节省内存" class="headerlink" title="3.2 通过删除[class] token来节省内存"></a>3.2 通过删除[class] token来节省内存</h3><p>我们都知道最大的VIT模型使用14×14个patch, 224×224的图像产生了256个visual “tokens”，每个token对应一个图像patch。在此之上，ViT模型有一个额外的[class] token，它用于产生最终的全局表征，最终使得toekn的总数达到256+1=257个。</p><p>对于ViT模型，当前的TPU硬件将token维度设置为128的倍数，这可能导致高达50%的内存开销。为了克服这个问题，作者研究了使用额外的[class] token的替代方法。评估了全局平均池(GAP)和多头注意力池(MAP)来聚合来自所有patch token的表示。将MAP中的heads数设置为与模型其他部分中的注意heads数相等。为了进一步简化head设计，在最终预测层之前去掉了最终的非线性投影。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210609/8.png" alt=""></p><p>为了选择最好的head，作者对[class] token和GAP/MAP head进行并行比较。结果上图所示。作者发现所有heads的表现都是相似的，而由于前面提到的padding考虑，GAP和MAP的内存效率更高。作者还观察到非线性映射可以移除。因此，作者选择了MAP  head，因为它是最具表现力和泛化性。</p><h3 id="3-3-Scaling-up-data"><a href="#3-3-Scaling-up-data" class="headerlink" title="3.3 Scaling up data"></a>3.3 Scaling up data</h3><p>在本研究中作者使用了专有的JFT-3B数据集，这是JFT-300M数据集的一个更大的版本，JFT-300M数据集在以前的大规模计算机视觉模型研究中使用过。该数据集由近30亿张图像组成，通过半自动管道标注了约30k个标签的类层次结构。因此，数据和相关的标签是有噪声的。这里忽略标签的层次方面，只使用分配的标签作为目标，通过sigmoid交叉熵损失进行多标签分类。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210609/9.png" alt=""></p><p>如图，左边显示了整个线性10-shot的ImageNet性能评估。作者观察到，JFT-3B甚至在JFT-300M未完全训练一个epoch之前就产生了一个更好的模型。因此，JFT-300M在small B/32模型以及larger L/16上过拟合并不是改善的唯一原因。</p><p>作者将模型微调到完整的ImageNet数据集(右)，并确认这些改进转移到完整的微调设置。总体而言，无论是小型还是大型模型，数据集的改变提高了ImageNet的1%左右。JFT-300M和JFT-3B的训练行为除了表现上的改善外，还有相似之处。最重要的是，JFT-3B可以让我们在不担心过拟合和正则化的情况下进一步扩大规模。</p><h3 id="3-4-Memory-efficient-optimizers"><a href="#3-4-Memory-efficient-optimizers" class="headerlink" title="3.4 Memory-efficient optimizers"></a>3.4 Memory-efficient optimizers</h3><p>当训练大型模型时，模型参数所需的存储成为瓶颈。本文设计最大的模型ViT-G，大约有20亿个参数，占用8GiB的显存。更糟糕的是，通常用于训练transformer的Adam优化器为每个参数存储2个额外的浮点标量，这导致了额外的2倍开销(额外的16GiB)。为了解决Adam优化器带来的开销，作者做了2个修改：</p><h4 id="Adam-with-half-precision-momentum"><a href="#Adam-with-half-precision-momentum" class="headerlink" title="Adam with half-precision momentum"></a>Adam with half-precision momentum</h4><p>通过经验观察到，半精度(bfloat16类型)的动量存储不会影响训练，对结果也没有影响。这可以将优化器的开销从原来的2倍减少到1.5倍。值得注意的是，<strong>使用半精度存储第2个动量会导致性能显著下降</strong>。</p><h4 id="Adafactor优化器"><a href="#Adafactor优化器" class="headerlink" title="Adafactor优化器"></a>Adafactor优化器</h4><p>上述Adam优化器仍然会导致较大的内存开销。因此，作者将注意力转向adfactor优化器，它使用秩1因式分解存储第2动量。从实用的角度来看，这将导致微不足道的内存开销。然而，作者并没有直接使用Adafactor优化器，而是做了以下修改:</p><ul><li><p>1 重新将半精确引入第1个动量，而推荐的设置根本不使用第1个动量。</p></li><li><p>2 禁用了相对于权重Norm的学习率的缩放，在Adafactor默认开启的。</p></li></ul><p>由此产生的优化器只引入了50%的内存开销。作者观察到，这2个建议的优化器的性能与最初的Adam优化器相当，甚至稍还要好一些。</p><h3 id="3-5-Learning-rate-schedule"><a href="#3-5-Learning-rate-schedule" class="headerlink" title="3.5 Learning-rate schedule"></a>3.5 Learning-rate schedule</h3><p>在研究中，作者希望使用几个不同的训练时间训练一个模型，以衡量模型大小和训练时间之间的权衡。当使用线性衰减时，每个训练时间都需要从头开始运行自己的训练，这是一种低效的方法。</p><p>通过探索学习率计划来解决这个问题，类似于开始的warmup阶段，包括训练结束的cooldown阶段，在这个阶段学习率线性趋于零。在warmup和cooldown阶段之间，学习率不应该很快下降到零。这可以通过使用一个常量，或一个倒数平方根schedule的主要训练部分来实现。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210609/10.png" alt=""></p><p>上图述了这些选项中的几个，在大约200k、400k和500k步之后有一个cooldown时间。图的上半部分显示了这些选项及其间隔期的验证分数(越高越好)，以及2个线性时间表。</p><p>当事先知道训练时间而不打算继续训练时，线性计划仍然是可取的，所有3种选择都相当接近，其优点是允许不确定的训练，并从一次跑步中评估多个训练时间。对于每一个schedule作者优化了学习率和exact shape。作者也简单地尝试了循环学习率计划，但是它们似乎表现得更差，因此，选择了平方根倒数schedule。</p><h3 id="3-6-选择模型的维度"><a href="#3-6-选择模型的维度" class="headerlink" title="3.6 选择模型的维度"></a>3.6 选择模型的维度</h3><p>ViT模型有许多参数可以用来控制模型的shape，作者参考原始版本的完整细节。简单地说，这些包括patch-size，编码器block的数量(深度)，patch embeddings和self-attention(宽度)，attention heads的数量，MLP块的维度(MLP-宽度)。</p><p>在此基础上，作者依赖XLA编译器来优化模型以提高运行时速度和内存占用。在底层，XLA使用复杂的启发式方法将模型编译为特定硬件的代码，以最佳的方式权衡内存和速度。因此，很难预测哪个模型配置适合一个设备的内存。</p><p>因此，作者运行了一个广泛的模拟，其中实例化了大量不同shape的ViT，并试图训练它们几个step。作者通过改变深度、宽度、头部和mlp宽度，但保持patch大小在14像素。通过这种方式，可以测量它们的速度，以及给定的模型是否适合设备的内存。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210609/11.png" alt=""></p><p>上图总结了这个模拟的结果。每个块对应一个模型配置，块的阴影对应其训练速度(越亮越快)。橙色块显示哪个没有任何修改原始ViT模型适合。绿色块还包括第3.2节中描述的内存节约，以及第3.4节中描述的半精度Adam。最后，蓝色块是修改的AdaFactor优化器。通过修改和实验能够适应深度最多为100个编码器块的thin ViT模型。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210609/12.png" alt=""></p><p>最初的Vision Transformer 结论是伸缩所有方面是最有效的(深度、宽度、mlp宽度和patch大小)同时以相似的数量。遵循这一建议，并在相应的内存容量限制下为ViT-g和ViT-G选择形状，并在表中进行总结。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>[1].Scaling Vision Transformers<br></p>]]></content>
      
      
      <categories>
          
          <category> Transformer </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Scaling ViT </tag>
            
            <tag> ImageNet Top-1 Acc </tag>
            
            <tag> Transformer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>详细解读NMS-Loss是如何解决目标检测中的遮挡问题</title>
      <link href="/2021/09/18/9%20%E8%AF%A6%E7%BB%86%E8%A7%A3%E8%AF%BBNMS-Loss%E6%98%AF%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%AD%E7%9A%84%E9%81%AE%E6%8C%A1%E9%97%AE%E9%A2%98/"/>
      <url>/2021/09/18/9%20%E8%AF%A6%E7%BB%86%E8%A7%A3%E8%AF%BBNMS-Loss%E6%98%AF%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%AD%E7%9A%84%E9%81%AE%E6%8C%A1%E9%97%AE%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210608/1.png" alt=""></p><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>非极大值抑制(Non-Maximum Suppression, NMS)在目标检测中至关重要，它通过合并假阳性(FP)和假阴性(FN)影响目标检测结果，尤其是在人群遮挡场景中。在本文中提出了NMS造成的训练目标和评估指标之间的弱连接问题，并提出了一种新的损失函数<strong>NMS-loss</strong>，使NMS过程可以端到端地被训练而不需要任何附加的网络参数。</p><p>NMS-loss惩罚2种情况，即FP没有被抑制，而FN被NMS错误地删除。具体来说，NMS-Loss提出了pull loss将具有相同目标的预测拉得很近，以及push loss将具有不同目标的预测推得很远。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210608/2.png" alt=""></p><p>实验结果表明，在NMS-Loss的帮助下NMS-Ped检测器在Caltech数据集上的Miss Rate为5.92%，在CityPersons数据集上的Miss Rate为10.08%，均优于现有的同类检测器。</p><h4 id="本文主要贡献"><a href="#本文主要贡献" class="headerlink" title="本文主要贡献"></a>本文主要贡献</h4><ul><li><p>首先提出了行人检测中训练目标与评估指标之间的弱连接问题，并提出了一种新的NMS-loss，使NMS过程在不引入任何参数和运行时间开销的情况下可以端到端进行训练。</p></li><li><p>作者提出了精心设计的pull loss和push loss，分别考虑预测坐标和置信度，帮助网络提高精度和召回性能。</p></li><li><p>在行人检测中，作者借助NMS-Loss提出的NMS-Ped在Caltech和CityPersons数据集上优于现有的SOTA方法。</p></li></ul><h2 id="NMS-LOSS"><a href="#NMS-LOSS" class="headerlink" title="NMS-LOSS"></a>NMS-LOSS</h2><p>传统的NMS流程如Alg.1中所示，没有考虑红色字体。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210608/3.png" alt=""></p><p>NMS从一组得分为S的检测框$B$开始，</p><p><strong>首先</strong>，将得分最大的proposal $𝑏_𝑚$从$B$集合移动到最终保留检测的集合$K$;</p><p><strong>然后</strong>，删除$B$中得分为$S$的且与$𝑏<em>𝑚$的重叠高于阈值$𝑁</em>𝑡$的框。</p><p>对剩下的$B$集重复此过程。</p><p>但是，现有的方法没有将NMS纳入训练过程中来调整检测框，使得学习目标与评价指标不一致，这意味着NMS未抑制FP和NMS消除FN分别会损害精度和召回率。为了避免不一致，作者提出NMS-loss将NMS程序引入到训练过程中，自适应地选择由NMS引起的错误预测，并使用精心设计的pull和push两种损失来最小化FP和FN。具体来说NMS-Loss定义为:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210608/4.png" alt=""></p><p>其中$L<em>{pull}$为pull损失用来惩罚FP同时不抑制由NMS，$L</em>{push}$为push损失用来惩罚NMS的FN错误删除。系数$\lambda<em>{pull}$和$\lambda</em>{push}$是平衡损失的权重。</p><p>NMS-Loss的细节在Alg.1中用红色文本强调。与传统pipeline不同，这里使用一组$G$，包含相应的检测框ground truth index，用于识别FP和FN。在NMS-Loss计算过程中，M是一个辅助字典，以ground truth指数为key，对应最大检测得分为value，用来记录每个ground truth的max score预测。</p><p>NMS-loss自然地合并到NMS过程中，而不包含任何额外的训练参数。对于测试来说，NMS-Loss的运行时成本为零。</p><h3 id="2-1-定义Pull-Loss"><a href="#2-1-定义Pull-Loss" class="headerlink" title="2.1 定义Pull Loss"></a>2.1 定义Pull Loss</h3><p>以降低FP为目标需要找出错误的预测。为此，在每次迭代中检查当前的max score预测$𝑏<em>𝑚$是否为其对应的$g</em>𝑚$ ground truth的max score预测。如果不是，则说明$𝑏<em>𝑚$是一个未被NMS抑制的FP，pull loss应在$𝑏</em>𝑚$和$g<em>𝑚$ ground truth的max score prediction $𝑏</em>{𝑚𝑎𝑥}$之间执行(见图1)。形式上pull loss计算如下:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210608/5.png" alt=""></p><p>其中$N_t$为预定义的NMS阈值，$s_m$为对应于$b_m$的预测score。</p><p>作者注意到pull loss的2个特性:</p><ul><li><p>当$b<em>{max}$和$b_m$之间的IoU较小时，pull loss有增加的趋势，迫使网络学会将$b_m$拉向$b</em>{max}$。NMS的阈值$N_t$用于防止异常值的梯度对模型学习的影响过大。另外，对于NMS只需要使FP和TP之间的IoU高于$N_t$即可。在pull loss中使用$N_t$来减小异常值的梯度，可以使网络易于学习和收敛。</p></li><li><p>FP预测得分对pull loss也有较大影响。FP得分越高，对评价结果的影响越大，直观上需要更多的关注。此外，它使网络学习修正FP不仅要制约box坐标，而且要考虑降低预测分数。</p></li></ul><h3 id="2-2-定义Push-Loss"><a href="#2-2-定义Push-Loss" class="headerlink" title="2.2 定义Push Loss"></a>2.2 定义Push Loss</h3><p>在NMS中，当前的最大score预测$𝑏<em>𝑚$用$𝑏</em>𝑚$消除了获得高于$𝑁<em>𝑡$的IoU的box。如果剔除的框$b_i$对应的ground truth index 与$b_m$不同，则$b_i$为FN，降低召回率(见图1)。为了避免错误地删除$𝑏</em>𝑖$提出push loss来惩罚FN:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210608/6.png" alt=""></p><p>其中$s_i$为$b_i$对应的预测得分。与pull loss不同当$IoU(b_i,b_m)\to1$时，push loss增大，模型学会将$b_i$推离$b_m$。为了避免模型倾向于通过降低FN的分数来减少push loss，作者只使用$s_i$来重新加权损失，而不使用反向传播梯度。</p><p>对于拥挤的场景，特别是在CityPersons数据集中，边界框的ground truths是相互重叠的。在IoU=0的情况下，将他们的预测相互排斥是不合理的。为了处理这个问题，作者只在预测IoU高于其对应ground truth box的IoU时才计算$𝐿_{𝑝𝑢𝑠ℎ}$。</p><p>本文所提的Pull Loss和Push Loss是根据预测来执行的。当pull/push loss被激活时，网络会尝试pull/push两个预测，分别pull/push彼此。因为高分预测通常会得到一个更准确的位置，所以在一个不准确的预测基础上移动一个准确的预测是不合理的。为了解决这个问题，作者停止了高分预测的梯度向后传播，导致网络专注于错误的预测。</p><h3 id="2-3-与RepLoss的不同之处在哪里？"><a href="#2-3-与RepLoss的不同之处在哪里？" class="headerlink" title="2.3 与RepLoss的不同之处在哪里？"></a>2.3 与RepLoss的不同之处在哪里？</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210608/7.png" alt=""></p><p>RepLoss通过设置损失函数的方式，使预测框和所负责的真实目标框的距离缩小，而使得其与周围非负责目标框（包含真实目标框和预测框）的距离加大 。如下式，如果与周围目标的距离越大，损失值会越小。<br><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210608/8.png" alt=""></p><p>作者对NMS-Loss和RepLoss进行了详细的比较，因为这2种方法都是基于它们的目标进行pull/push预测的。</p><p>主要有3个区别:</p><ul><li><p>RepLoss在所有实例上执行，而NMS-Loss只在被NMS错误处理的实例上执行，从而实现了端到端训练。</p></li><li><p>RepLoss只考虑回归，而score在NMS-Loss中也用于实例重加权。</p></li><li><p>在密集人群场景下RepLoss将实例推开，即使它们的目标本来很接近，使RepLoss与回归损失相矛盾。相反，NMS-Loss会推送与其他IoU高于其对应ground truth box IoU的实例，这样可以消除RepLoss的矛盾。</p></li></ul><p>如表所示，NMS-Loss不仅比RepLoss表现更好，而且在CityPersons上有更高的相对改善。这表明，NMS-Loss可以在广泛使用的数据集上实现稳定的相对改进(高于10%)。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210608/9.png" alt=""></p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210608/10.png" alt=""></p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210608/11.png" alt=""></p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210608/12.png" alt=""></p><h2 id="参考阅读"><a href="#参考阅读" class="headerlink" title="参考阅读"></a>参考阅读</h2><p>[1].NMS-Loss: Learning with Non-Maximum Suppression for Crowded Pedestrian Detection<br></p><p>[2].Repulsion Loss: Detecting Pedestrians in a Crowd<br></p>]]></content>
      
      
      
        <tags>
            
            <tag> NMS-Loss </tag>
            
            <tag> 让检测告别遮挡 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>卷积与Self-Attention完美融合X-volution插入CV模型将带来全任务的涨点</title>
      <link href="/2021/09/05/8%20%E5%8D%B7%E7%A7%AF%E4%B8%8ESelf-Attention%E5%AE%8C%E7%BE%8E%E8%9E%8D%E5%90%88X-volution%E6%8F%92%E5%85%A5CV%E6%A8%A1%E5%9E%8B%E5%B0%86%E5%B8%A6%E6%9D%A5%E5%85%A8%E4%BB%BB%E5%8A%A1%E7%9A%84%E6%B6%A8%E7%82%B9/"/>
      <url>/2021/09/05/8%20%E5%8D%B7%E7%A7%AF%E4%B8%8ESelf-Attention%E5%AE%8C%E7%BE%8E%E8%9E%8D%E5%90%88X-volution%E6%8F%92%E5%85%A5CV%E6%A8%A1%E5%9E%8B%E5%B0%86%E5%B8%A6%E6%9D%A5%E5%85%A8%E4%BB%BB%E5%8A%A1%E7%9A%84%E6%B6%A8%E7%82%B9/</url>
      
        <content type="html"><![CDATA[<p><br></p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210607/1.png" alt=""></p><blockquote><p>本文建立了一个由卷积和self-attention组成的多分支基本模块，能够统一局部和非局部特征交互，然后可以结构重新参数化为一个纯卷积风格的算子：X-volution，即插即用！可助力分类、检测和分割任务的涨点！<br><strong>作者单位</strong>：上海交通大学(倪冰冰团队), 华为海思</p></blockquote><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>卷积和self-attention是深度神经网络中的2个基本构建块，前者以线性方式提取图像的局部特征，而后者通过非局部关系编码高阶上下文关系。尽管本质上是相互补充的，即一阶/高阶、最先进的架构，但是，CNN或Transformer均缺乏一种原则性的方法来在单个计算模块中同时应用这2种操作，因为它们的异构计算视觉任务的全局点积的模式和过度负担。</p><p>在这项工作中，作者从理论上推导出一种全局self-attention近似方案，该方案通过对变换特征的卷积运算来近似self-attention。基于近似方案建立了一个由卷积和self-attention操作组成的多分支基本模块，能够统一局部和非局部特征交互。重要的是，一旦经过训练，这个多分支模块可以通过结构重新参数化有条件地转换为单个标准卷积操作，呈现一个名为X-volution的纯卷积风格的算子，准备作为atomic操作插入任何现代网络。大量实验表明，所提出的X-volution实现了极具竞争力的视觉理解改进（ImageNet分类的top-1准确率+1.2%，COCO 检测和分割的+1.7box AP和+1.5mask AP）。</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>本文提出了一种新颖的原子算子<strong>X-volution</strong>，将基本卷积算子和self-attention算子集成到一个统一的计算块中，期望从<strong>局部vs非局部</strong>/<strong>线性vs非线性</strong>两方面获得令人印象深刻的性能改进。</p><p><strong>首先</strong>，回顾卷积和self-attention的基本数学公式；</p><p><strong>然后</strong>，解读全局self-attention近似方案，它可以直接转换为一个兼容的卷积模式。</p><p><strong>最后</strong>，解释在推断阶段如何有条件地合并卷积分支和所提出的self-attention近似到单个卷积风格原子操作符。</p><h3 id="2-1-回顾卷积和self-attention"><a href="#2-1-回顾卷积和self-attention" class="headerlink" title="2.1 回顾卷积和self-attention"></a>2.1 回顾卷积和self-attention</h3><p>这2个算子想必大家已经非常熟悉了，这里就简单的说一下哈！！！</p><h4 id="卷积Module"><a href="#卷积Module" class="headerlink" title="卷积Module"></a>卷积Module</h4><p>卷积算子是用于构建卷积神经网络(CNN)的基本算子，它通过有限局部区域内的线性加权来估计输出。给定一个特征张量$X\in R^{C_i×H×W}$, $C_i$表示输入通道的数量，H是高度，W是宽度。卷积算子的估计结果$Y\in R^{C_o×H×W}$由以下公式定义:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210607/2.png" width = "500" align=center /></p><p>其中$C<em>o$为输出通道数。$w\in R^{C_o×C_i×K×K}$为卷积核，$W</em>{c_o,c_i,δ_i+[K/2],δ_j+[K/2]}$为特定位置核标量值。$K$为卷积kernel大小，$B\in R^{C_o}$为偏差向量，$∆k\in Z^2$为$K × K$卷积kernel中所有可能偏移的集合。</p><h4 id="Self-Attention-Module"><a href="#Self-Attention-Module" class="headerlink" title="Self-Attention Module"></a>Self-Attention Module</h4><p>与卷积不同，self-attention不能直接处理图像张量，首先将输入特征张量reshape为向量$X\in R^{C×L}$。$L$表示向量的长度，$L=H×W$。$W^Q、W^K、W^V$分别表示Query、Key、Value的嵌入变换，是空间共享的线性变换。Self-Attention的定义如下:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210607/3.png" width = "500" align=center /></p><p>其中$\overline W(X)$表示最终的Self-Attention等价系数矩阵，可以认为是一个动态和空间变化的卷积kernel。</p><h3 id="2-2-全局self-attention近似方案"><a href="#2-2-全局self-attention近似方案" class="headerlink" title="2.2 全局self-attention近似方案"></a>2.2 全局self-attention近似方案</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210607/4.png" alt=""></p><p>全局自注意是最原始的attention方案，它得益于全局范围的优势而具有优异的性能。然而，它的复杂度太大了$O(n^2)(n表示总像素数)$使得其在CV任务中的应用受到严重限制。关键问题是<strong>能否在公式2中推导出$\overline W(X)$的适当近似结果，即能否找到$\overline W(X)$的兼容计算模式，即能否找到卷积、single element-wise product等现成的算子替代?</strong></p><p>在本部分中，作者展示了在简单的element-wise shift和dot-product之后，可以用卷积的形式近似全局self-attention算子。给定特征张量$X$中的一个位置，将其特征向量表示为$x_0$，其attention logit $s_0$可以写成如下公式:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210607/5.png" width = "500" align=center /></p><p>其中$\alpha _t = w^pw^qw^vx_t$， Ω为全局区域，A为以x0为中心的局部区域。在图1的左边说明了局部区域和非局部区域。图中灰框表示输入特征X的全局区域，绿框表示以$x_0$为中心的局部区域。</p><p>另外，non-local区域是指局部区域以外的区域。因为图像具有很强的说服力（根据马尔可夫性质），$x<em>0$可以用像素在其局部区域近似线性表示:$x_0≈\sum</em>{x_k\in A˚}\beta _kx_k$，其中$\beta_k$为线性权值。代入式3中第2项，可得:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210607/6.png" width = "500" align=center /></p><p>在不失一般性的情况下，可以在区域A中加入系数为零的项。通过设计，non-local区域也在局部区域的边界像素的接受域内。因此可以将上式转化为:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210607/7.png" width = "400" align=center /></p><p>根据图像的马尔可夫性质，可以假设对于$x_k\in A$，远离$x_k$的$x_i$与$x_k$之间的相互作用是弱的。因此，可以进一步简化式上式:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210607/8.png" width = "400" align=center /></p><p>其中$U(x_k)$为$x_k$的局部区域。将上式代入Eq.3中的第2项，可以改写为:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210607/10.png" width = "500" align=center /></p><p>注意,$x<em>k,x_i$是$x_k$和$x_i$之间的内积，它衡量了$x_k$和$x_i$之间的相似性。$\sum</em>{x_i\in U(x_k)}\alpha_i\beta_k (x_k,x_i)$是$x_k$在其邻近区域的attention结果。因此，在$x_0$处的全局注意力logit可以通过加权求和其邻域内像素的attention结果来近似。</p><p>根据以上理解，可以设计一个近似算子，通过逐点上下文关系传播来估计全局attention。因此，作者提出了一个全局注意力近似方案，Pixel Shift<br>Self-Attention (PSSA)，基于像素偏移和卷积来近似全局attention。</p><p>具体来说，首先将特征映射沿给定的方向(即左、右、上等)移动L个像素，然后将原始特征与移动的特征进行元素积，得到变换后的特征。</p><p>实际上，shift-product操作建立了邻域内点之间的上下文关系，通过分层叠加可以将上下文关系传播到全局区域。最后，对这些变换后的特征进行加权求和(可以通过卷积算子实现)，得到一个近似的自注意力映射。平移、元素积和加权求和的复杂度为O(n)，因此提出的PSSA是一个时间复杂度为O(n)的算子。值得注意的是，PSSA实际上是将self-attention转换为对转换特征的标准卷积运算。该结构通过层次叠加进而通过上下文关系传播实现全局self-attention logit的估计。</p><h3 id="2-3-卷积和Self-Attention的统一-X-volution"><a href="#2-3-卷积和Self-Attention的统一-X-volution" class="headerlink" title="2.3 卷积和Self-Attention的统一: X-volution"></a>2.3 卷积和Self-Attention的统一: X-volution</h3><h4 id="卷积和Self-Attention是相辅相成的"><a href="#卷积和Self-Attention是相辅相成的" class="headerlink" title="卷积和Self-Attention是相辅相成的"></a>卷积和Self-Attention是相辅相成的</h4><p>卷积采用局域性和各向同性的归纳偏差，使其具有平移等方差的能力。然而，局部固有的特性使卷积无法建立形成图所必需的长期关系。</p><p>与卷积相反，<strong>Self-Attention摒弃了提到的归纳偏差，即所谓的低偏差，并从数据集中发现自然模式，而没有明确的模型假设。低偏差原则给予Self-Attention以探索复杂关系的自由(例如，长期依赖、各向异性语义、CNN中的强局部相关性等)，因此该方案通常需要对超大数据集进行预训练(如JFT-300M、ImageNet21K)</strong>。</p><p>此外，Self-Attention很难优化，需要更长的训练周期和复杂的Tricks。有文献提出将卷积引入Self-Attention以提高Self-Attention的鲁棒性和性能。简而言之，采用不同的模型假设，使卷积和Self-Attention在优化特征、注意范围(即局部/长期)和内容依赖(内容依赖/独立)等方面得到相互补充。</p><h4 id="统一的多分支拓扑"><a href="#统一的多分支拓扑" class="headerlink" title="统一的多分支拓扑"></a>统一的多分支拓扑</h4><p>有一些工作试图将卷积和self-attention结合起来，然而，粗糙的拓扑组合(例如，分层堆叠，级联)阻止他们获得单个原子操作(在同一个模块中应用卷积和注意)，使结构不规则。例如，AANet将经过卷积层和Self-Attention层处理的结果直接连接起来，得到组合结果。说明单一的卷积或单一的Self-Attention都会导致性能下降，当它们同时存在时，性能会有显著的提高。</p><p>在这个工作中，作者研究卷积和self-attention的数学原理后找到了近似形式。作者还观察到全局元素相互作用(点积)可以用局部元素相互作用的传播来近似表示。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210607/11.png" alt=""></p><p>因此，这2种算子可以用统一的计算模式来处理，即卷积。从另一个角度来看，卷积运算可以看作是Self-Attention的空间不变偏差。考虑到这一点，可以将算子组合成一个多分支拓扑，如图所示，这可以同时受益于卷积和Self-Attention。多分支模块由2个主要分支组成。左边的分支由级联的Shift Pixel Self-Attention和batch-normalization组成起到近似全局Self-Attention操作的作用，右分支被设计成由级联卷积和批归一化组成的卷积分支。</p><h4 id="有条件地将多分支方案转换为Atomic-X-volution"><a href="#有条件地将多分支方案转换为Atomic-X-volution" class="headerlink" title="有条件地将多分支方案转换为Atomic X-volution"></a>有条件地将多分支方案转换为Atomic X-volution</h4><p>多分支模块实现了卷积与Self-Attention的功能组合。然而，它只是一种粗粒度的算子组合，这将使网络高度复杂和不规则。从硬件实现的角度来看，多分支结构需要更多的缓存来服务于多路径的处理。相反，单个算子操作效率更高，内存开销更低，这是硬件友好的。</p><p>为了简单起见，在这里省略批标准化的公式。实际上，批归一化可以看作是一个$1×1$组卷积(其组等于channel数)，可以合并到卷积/Self-Attention层中。实际上，一般采用分层叠加的PSSA，堆叠结构中的加权运算可以省略，因为分层叠加隐含了加权邻接像素的运算。本文提出的多分支模块的训练阶段如下：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210607/12.png" width = "500" align=center /></p><p>其中$w^c$为卷积权值，$b^c$为其对应的偏置。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210607/13.png" width = "500" align=center /></p><p>其中$w^A(x_0,x_i)=w^qw^kw^v(x_0,x_i)$表示来自pixel shift self-attention 分支的content-dependent/dynamic coefficients。$W_c$表示从卷积分支继承的content-independent/static coefficients，训练完成后会修复。</p><p>观察上式可以发现，经过一个简单的变换，多分支结构可以转换成卷积形式。值得指出的是，这个过程在CNN中被广泛使用，被称为structural re-parameterization。在这里首先把它扩展到卷积和self-attention的合并。根据上式将由卷积和self-attention组成的多分支模块等价地转换为一个动态卷积算子X-voultion。</p><p>请注意，这里建议X-volution可以作为一个原子操作插入主流网络(例如，ResNet)。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="3-1-图像分类"><a href="#3-1-图像分类" class="headerlink" title="3.1 图像分类"></a>3.1 图像分类</h3><h4 id="架构设计"><a href="#架构设计" class="headerlink" title="架构设计"></a>架构设计</h4><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210607/14.png" width = "500" align=center /><br><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210607/15.png" width = "400" align=center /></p><p>结果表明，第3阶段的替换效果最好，ResNet-34的top-1准确率为+1.2%，ResNet-50的top-1准确率为+0.9%。作者怀疑第4阶段替换的性能较差ResNet-50可以归因于可学习参数的增加，这减慢了网络的收敛。</p><h3 id="3-2-目标检测"><a href="#3-2-目标检测" class="headerlink" title="3.2 目标检测"></a>3.2 目标检测</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210607/16.png" alt=""></p><p>特别是，本文所提X-volution(SA)实现了最好的性能，与ResNet-50相比增加了+1.7boxes AP。通过结合低阶局部特征和高阶长依赖，所提出的X-volution算子比单独的卷积或自注意力算子具有更高的精度。</p><p>结果表明，图完备原子算符有助于视觉理解，而现有的计算算符忽略了这一性质。此外，基于PSSA的X-volution也取得了与X-volution(SA)相当的性能，表明在X-volution模块中，近似效果良好，对硬件实现和计算更加友好。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210607/17.png" alt=""></p><h3 id="3-3-语义分割"><a href="#3-3-语义分割" class="headerlink" title="3.3 语义分割"></a>3.3 语义分割</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210607/19.png" alt=""></p><p>可以观察到，作者提出的X-volution比其他算子的性能要好很多。其中，X-volution(SA)实现了41.1 box AP和37.2 mask AP。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210607/20.png" width = "400" align=center /></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>[1].X-volution: On the Unification of Convolution and Self-attention.<br></p>]]></content>
      
      
      <categories>
          
          <category> 卷积CNN </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 卷积 </tag>
            
            <tag> Self-Attention </tag>
            
            <tag> CV模型 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>详细解读Transformer怎样从零训练并超越ResNet</title>
      <link href="/2021/09/05/7%20%E8%AF%A6%E7%BB%86%E8%A7%A3%E8%AF%BBTransformer%E6%80%8E%E6%A0%B7%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%B9%B6%E8%B6%85%E8%B6%8AResNet/"/>
      <url>/2021/09/05/7%20%E8%AF%A6%E7%BB%86%E8%A7%A3%E8%AF%BBTransformer%E6%80%8E%E6%A0%B7%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%B9%B6%E8%B6%85%E8%B6%8AResNet/</url>
      
        <content type="html"><![CDATA[<p><br></p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210606/1.png" alt=""></p><blockquote><p>本文证明了在没有大规模预训练或强数据增广的情况下，在ImageNet上从头开始训练时，所得ViT的性能优于类似大小和吞吐量的ResNet！而且还拥有更敏锐的注意力图。<br><strong>作者单位</strong>：谷歌,UCLA</p></blockquote><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>Vision Transformers(ViTs)和MLPs标志着在用通用神经架构替换手动特征或归纳偏置方面的进一步努力。现有工作通过大量数据为模型赋能，例如大规模预训练和/或重复的强数据增广，并且还报告了与优化相关的问题（例如，对初始化和学习率的敏感性）。</p><p>因此，本文从损失几何的角度研究了ViTs和MLP-Mixer，旨在提高模型在训练和推理时的泛化效率。可视化和Hessian揭示了收敛模型极其敏感的局部最小值。</p><p>同时通过使用最近提出的<strong>锐度感知优化器</strong>提高平滑度，进而大大提高了ViT和MLP-Mixer在跨越监督、对抗、对比和迁移学习（例如，+5.3\% 和 +11.0\%）的各种任务上的准确性和鲁棒性使用简单的Inception进行预处理，ViT-B/16和Mixer-B/16在ImageNet上的准确率分别为Top-1）。</p><p>作者研究表明，改进的平滑度归因于前几层中较稀疏的活动神经元。在没有大规模预训练或强数据增强的情况下，在ImageNet上从头开始训练时，所得 ViT的性能优于类似大小和吞吐量的ResNet。同时还拥有更敏锐的注意力图。</p><h2 id="Background和Related-Work"><a href="#Background和Related-Work" class="headerlink" title="Background和Related Work"></a>Background和Related Work</h2><p>最近的研究发现，ViT中的self-attention对性能并不是至关重要的，因此出现了一些专门基于mlp的架构。这里作者以MLP-Mixer为例。MLP-Mixer与ViT共享相同的输入层;也就是说，它将一个图像分割成一系列不重叠的Patches/Toekns。然后，它在torkn mlp和channel mlp之间交替使用，其中前者允许来自不同空间位置的特征融合。</p><h2 id="ViTs和MLP-Mixers收敛到锐局部极小值"><a href="#ViTs和MLP-Mixers收敛到锐局部极小值" class="headerlink" title="ViTs和MLP-Mixers收敛到锐局部极小值"></a>ViTs和MLP-Mixers收敛到锐局部极小值</h2><p>目前的ViTs、mlp-mixer和相关的无卷积架构的训练方法很大程度上依赖于大量的预训练或强数据增强。它对数据和计算有很高的要求，并导致许多超参数需要调整。</p><p>现有的研究表明，当在ImageNet上从头开始训练时，如果不结合那些先进的数据增强，尽管使用了各种正则化技术(例如，权重衰减，Dropout等)ViTs的精度依然低于类似大小和吞吐量的卷积网络。同时在鲁棒性测试方面，vit和resnet之间也存在较大的差距。</p><p>此外，Chen等人发现，在训练vit时，梯度会出现峰值，导致精确度突然下降，Touvron等人也发现初始化和超参数对训练很敏感。这些问题其实都可以归咎于优化问题。</p><p>在本文中，作者研究了ViTs和mlp-mixer的损失情况，从优化的角度理解它们，旨在减少它们对大规模预训练或强数据增强的依赖。</p><h3 id="3-1-ViTs和MLP-Mixers收敛到极sharp局部极小值"><a href="#3-1-ViTs和MLP-Mixers收敛到极sharp局部极小值" class="headerlink" title="3.1 ViTs和MLP-Mixers收敛到极sharp局部极小值"></a>3.1 ViTs和MLP-Mixers收敛到极sharp局部极小值</h3><p>众所周知，当模型收敛到曲率小的平坦区域时模型会具有更好的泛化性能。在[36]之后，当resnet、vit和MLP-Mixers在ImageNet上使用基本的初始风格预处理从头开始训练时，作者绘制损失图：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210606/2.png" alt=""></p><p>如图1(a)到1(c)所示，ViTs和mlp-mixer比ResNets收敛到更清晰的区域。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210606/3.png" alt=""></p><p>在表1中，通过计算主要的Hessian特征值$\lambda<em>{max}$进一步验证了结果。ViT和MLP-Mixer的$\lambda</em>{max}$值比ResNet大一个数量级，并且MLP-Mixer的曲率在3种中是最大的(具体分析见4.4节)。</p><h3 id="3-2-Small-training-errors"><a href="#3-2-Small-training-errors" class="headerlink" title="3.2 Small training errors"></a>3.2 Small training errors</h3><p>这种向sharp区域的收敛与图2(左)所示的训练动态一致。尽管Mixer-B/16参数少于ViT-B/16(59M vs 87M)，同时它有一个小的训练误差，但测试性能还是比较差的，这意味着使用cross-token MLP学习的相互作用比ViTs’ self-attention机制更容易过度拟合。这种差异可能解释了mlp-mixer更容易陷入尖锐的局部最小值。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210606/4.png" alt=""></p><h3 id="3-3-ViTs和MLP-Mixers的可训练性较差"><a href="#3-3-ViTs和MLP-Mixers的可训练性较差" class="headerlink" title="3.3 ViTs和MLP-Mixers的可训练性较差"></a>3.3 ViTs和MLP-Mixers的可训练性较差</h3><p>此外，作者还发现ViTs和MLP-Mixers的可训练性较差，可训练性定义为通过梯度下降优化的网络的有效性。Xiao等人的研究表明，神经网络的可训练性可以用相关的神经切线核(NTK)的条件数来表征:</p><script type="math/tex; mode=display">Θ(x,x')= J(x)J(x')^T</script><p>其中$J$是雅可比矩阵。</p><p>用$\lambda<em>1≥··≥\lambda_m$表示NTK $Θ</em>{train}$的特征值，最小的特征值$\lambda_m$以条件数κ$=\lambda_1=\lambda_m$的速率指数收敛。如果κ是发散的，那么网络将变得不可训练。如表1所示，ResNets的κ是相当稳定的，这与之前的研究结果一致，即ResNets无论深度如何都具有优越的可训练性。然而，当涉及到ViT和时，条件数是不同的MLP-Mixer，证实了对ViTs的训练需要额外的辅助。</p><h2 id="CNN-Free视觉架构优化器原理"><a href="#CNN-Free视觉架构优化器原理" class="headerlink" title="CNN-Free视觉架构优化器原理"></a>CNN-Free视觉架构优化器原理</h2><p>常用的一阶优化器(如SGD,Adam)只寻求最小化训练损失。它们通常会忽略与泛化相关的高阶信息，如曲率。然而，深度神经网络的损失具有高度非凸性，在评估时容易达到接近0的训练误差，但泛化误差较高，更谈不上在测试集具有不同分布时的鲁棒性。</p><p>由于对视觉数据缺乏归纳偏差ViTs和MLPs放大了一阶优化器的这种缺陷，导致过度急剧的损失scene和较差的泛化性能，如前一节所示。假设平滑收敛时的损失scene可以显著提高那些无卷积架构的泛化能力，那么最近提出的锐度感知最小化(SAM)可以很好的避免锐度最小值。</p><h3 id="4-1-SAM-Overview"><a href="#4-1-SAM-Overview" class="headerlink" title="4.1 SAM:Overview"></a>4.1 SAM:Overview</h3><p>从直觉上看，SAM寻找的是可以使整个邻近训练损失最低的参数w，训练损失$L_{train}$通过构造极小极大目标:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210606/5.png" width = "350" align=center /></p><p>其中$\rho$是neighbourhood ball的大小。在不失一般性的情况下，这里使用$l_2$范数作为其强经验结果，这里为了简单起见省略了正则化项。</p><p>由于内部最大化下式的确切解很难获得：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210606/6.png" width = "350" align=center /></p><p>因此，这里采用了一个有效的一阶近似:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210606/7.png" width = "400" align=center /></p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210606/8.png" width = "350" align=center /></p><p>在$l_2$范数下，$\hat\epsilon(w)$是当前权值$w$的缩放梯度。计算$\hat\epsilon$后，SAM基于锐度感知梯度更新w。</p><h3 id="4-2-SAM优化器实质上改进了ViTs和MLP-Mixers"><a href="#4-2-SAM优化器实质上改进了ViTs和MLP-Mixers" class="headerlink" title="4.2 SAM优化器实质上改进了ViTs和MLP-Mixers"></a>4.2 SAM优化器实质上改进了ViTs和MLP-Mixers</h3><p>作者在没有大规模的预训练或强大的数据增强的情况下训练了vit和MLP-Mixers。直接将SAM应用于vit的原始ImageNet训练pipeline，而不改变任何超参数。<br>pipeline使用了基本的Inception-style的预处理。最初的mlp-mixer的训练设置包括强数据增强的组合;也用同样的Inception-style的预处理来替换它，以便进行公平的比较。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210606/9.png" alt=""></p><p>注意，在应用SAM之前，我们对学习速率、权重衰减、Dropout和随机深度进行网格搜索。</p><h4 id="1-局部极小值周围的平滑区域"><a href="#1-局部极小值周围的平滑区域" class="headerlink" title="1 局部极小值周围的平滑区域"></a>1 局部极小值周围的平滑区域</h4><p>由于SAM, ViTs和mlp-mixer都汇聚在更平滑的区域，如图1(d)和1(e)所示。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210606/10.png" alt=""></p><p>曲率测量，即Hessian矩阵的最大特征值$\lambda_{max}$，也减小到一个小值(见表1)。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210606/11.png" alt=""></p><h4 id="2-Higher-accuracy"><a href="#2-Higher-accuracy" class="headerlink" title="2 Higher accuracy"></a>2 Higher accuracy</h4><p>随之而来的是对泛化性能的极大改进。在ImageNet验证集上，SAM将ViT-B/16的top-1精度从74.6%提高到79.9%，将Mixer-B/16的top-1精度从66.4%提高到77.4%。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210606/12.png" alt=""></p><p>相比之下，类似规模的ResNet-152的性能提高了0.8%。根据经验，<strong>改进的程度与架构中内置的归纳偏差水平呈负相关</strong>。与基于注意力的ViTs相比，具有inherent translation equivalence和locality的ResNets从landscape smoothing中获益较少。MLP-Mixers从平滑的loss geometry中获得最多。</p><p>此外，SAM对更大容量(例如:+4.1%的Mixer-S/16 vs. +11.0%的Mixer-B/16)和更长的patch序列(例如:+2.1%的vits/32 vs. +5.3%的vits /8)的模型带来了更大的改进。</p><h4 id="3-Better-robustness"><a href="#3-Better-robustness" class="headerlink" title="3 Better robustness"></a>3 Better robustness</h4><p>作者还使用ImageNet-R和ImageNetC评估了模型的鲁棒性，并发现了smoothed loss landscapes的更大影响。在ImageNet-C上，它通过噪音、恶劣天气、模糊等来破坏图像，实验了5种严重程度上19种破坏的平均精度。如表1和表2所示， ViT-B/16和Mixer-B/16的精度分别增加了9.9%和15.0%。</p><h3 id="4-3-无需预训练或强大的数据增强ViTs优于ResNets"><a href="#4-3-无需预训练或强大的数据增强ViTs优于ResNets" class="headerlink" title="4.3 无需预训练或强大的数据增强ViTs优于ResNets"></a>4.3 无需预训练或强大的数据增强ViTs优于ResNets</h3><p>模型体系结构的性能通常与训练策略合并，其中数据增强起着关键作用。然而，数据增广的设计需要大量的领域专业知识，而且可能无法在图像和视频之间进行转换。由于有了锐度感知优化器SAM，可以删除高级的数据增强，并专注于体系结构本身(使用基本的Inception-style的预处理)。</p><p>当使用SAM在ImageNet上从0开始训练时，ViT的准确性(在ImageNet、ImageNet-Real和ImageNet V2上)和健壮性(在ImageNet-R和ImageNet-R上)方面都优于类似和更大的ResNet(在推理时也具有相当的吞吐量)。</p><p>ViT-B/16在ImageNet、ImageNet-r和ImageNet-C上分别达到79.9%、26.4%和56.6%的top精度，而对应的ResNet-152则分别达到79.3%、25.7%和52.2%(见表2)。对于小型架构，vit和resnet之间的差距甚至更大。<br>在ImageNet上，ViT-S/16的表现比同样大小的ResNet-50好1.4%，在ImageNet-C上好6.5%。SAM还显著改善了MLP-Mixers的结果。</p><h3 id="4-4-SAM后的内在变化"><a href="#4-4-SAM后的内在变化" class="headerlink" title="4.4 SAM后的内在变化"></a>4.4 SAM后的内在变化</h3><p>作者对模型进行了更深入的研究，以理解它们如何从本质上改变以减少Hessian的特征值$\lambda_{max}$以及除了增强泛化之外的变化意味着什么。</p><h4 id="结论1：每个网络组件具有Smoother-loss-landscapes"><a href="#结论1：每个网络组件具有Smoother-loss-landscapes" class="headerlink" title="结论1：每个网络组件具有Smoother loss landscapes"></a>结论1：每个网络组件具有Smoother loss landscapes</h4><p>在表3中，将整个体系结构的Hessian分解成与每一组参数相关的小的斜对角Hessian块，试图分析在没有SAM训练的模型中，是什么特定的成分导致$\lambda_{max}$爆炸。</p><p>作者观察到较浅的层具有较大的Hessian特征值$\lambda_{max}$，并且第1个linear embedding layer产生sharpest的几何形状。</p><p>此外，ViTs中的多头自注意(MSA)和MLP-Mixers中的token mlp(Token mlp)跨空间位置混合信息，其$\lambda<em>{max}$相对较低。SAM一致地降低了所有网络块的$\lambda</em>{max}$。</p><p>可以通过递归mlp的Hessian矩阵得到上述发现。设$h_k$和$a_k$分别为第k层激活前的值和激活后的值。它们满足$h_k=W_ka_k−1,a_k=f_k(h_k)$，其中$W_k$为权值矩阵，$f_k$为激活函数(mlp-mixer中的GELU)。为了简单起见，在这里省略偏置项。Hessian矩阵$H_k$相对于$W_k$的对角块可递归计算为:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210606/19.png" width = "500" align=center /></p><p>其中⊗为Kronecker product，$H<em>k$为第$k$层的预激活Hessian，L为目标函数。因此，当递归公式反向传播到浅层时，Hessian范数累积，这也解释了为什么表3中第一个块的$\lambda</em>{max}$比最后一个块大得多。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210606/13.png" alt=""></p><h4 id="结论2：Greater-weight-norms"><a href="#结论2：Greater-weight-norms" class="headerlink" title="结论2：Greater weight norms"></a>结论2：Greater weight norms</h4><p>应用SAM后，作者发现激活后的值$a<em>{k−1}$的范数和权重$W</em>{k+1}$的范数变得更大(见表3)，说明常用的权重衰减可能不能有效地正则化ViTs和MLP-Mixers。</p><h4 id="结论3：MLP-Mixers中较稀疏的active-neurons"><a href="#结论3：MLP-Mixers中较稀疏的active-neurons" class="headerlink" title="结论3：MLP-Mixers中较稀疏的active neurons"></a>结论3：MLP-Mixers中较稀疏的active neurons</h4><p>根据递归公式(3)到(4)，作者确定了另一个影响Hessian的MLP-Mixers的内在度量:激活神经元的数量。</p><p>事实上，$B_k$是由大于零的被激活神经元决定的，因为当输入为负时，GELU的一阶导数变得非常小。因此，活跃的GELU神经元的数量直接与Hessian规范相连。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210606/14.png" alt=""></p><p>图2(右)显示了每个块中被激活的神经元的比例，使用ImageNet训练集的10%进行计算。可以看到，SAM极大地减少了前几层被激活神经元的比例，使它们处于更稀疏的状态。这一结果也说明了图像patch的潜在冗余性。</p><h4 id="结论4：ViTs的active-neurons高度稀疏"><a href="#结论4：ViTs的active-neurons高度稀疏" class="headerlink" title="结论4：ViTs的active neurons高度稀疏"></a>结论4：ViTs的active neurons高度稀疏</h4><p>虽然公式(3)和(4)只涉及mlp，但仍然可以观察到vit的第1层激活神经元的减少(但不如MLP-Mixers显著)。更有趣的是，作者发现ViT中被激活神经元的比例比ResNets或MLP-Mixers中要小得多——在大多数ViT层中，只有不到5%的神经元的值大于零。换句话说，ViT为网络修剪提供了巨大的潜力。</p><p>这种稀疏性也可以解释<strong>为什么一个Transformer可以处理多模态信号(视觉、文本和音频)?</strong></p><h4 id="结论5：ViTs中有更多的感知注意力Maps"><a href="#结论5：ViTs中有更多的感知注意力Maps" class="headerlink" title="结论5：ViTs中有更多的感知注意力Maps"></a>结论5：ViTs中有更多的感知注意力Maps</h4><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210606/15.png" alt=""></p><p>在图3中可视化了classification token的attention map。有趣的是，经过SAM优化的ViT模型能够编码合理的分割信息，比传统SGD优化训练的模型具有更好的可解释性。</p><h4 id="结论6：Higher-training-errors"><a href="#结论6：Higher-training-errors" class="headerlink" title="结论6：Higher training errors"></a>结论6：Higher training errors</h4><p>如图2(左)所示，使用SAM的ViT-B/16比使用vanilla SGD的训练误差更高。当在训练中使用强数据增强时，这种正则化效应也会发生，它迫使网络显式地学习RandAugment中的旋转平移等方差和mixup中的线性插值等先验。然而，增益对不同的训练设置很敏感(第5.2节)，并导致高噪声损失曲线(图2(中间))。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210606/16.png" alt=""></p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>具有smoother loss geometry的ViTs和MLP-Mixers可以更好地迁移到下游任务。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210606/17.png" alt=""></p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210606/18.png" alt=""></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>[1].When Vision Transformers Outperform ResNets without Pretraining or Strong Data Augmentations.<br></p>]]></content>
      
      
      <categories>
          
          <category> Transformer </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ResNet </tag>
            
            <tag> Transformer </tag>
            
            <tag> Tricks </tag>
            
            <tag> 图像分类 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>多域自适应MSDA-YOLO解读，恶劣天气也看得见</title>
      <link href="/2021/09/05/6%20%E5%A4%9A%E5%9F%9F%E8%87%AA%E9%80%82%E5%BA%94MSDA-YOLO%E8%A7%A3%E8%AF%BB%EF%BC%8C%E6%81%B6%E5%8A%A3%E5%A4%A9%E6%B0%94%E4%B9%9F%E7%9C%8B%E5%BE%97%E8%A7%81/"/>
      <url>/2021/09/05/6%20%E5%A4%9A%E5%9F%9F%E8%87%AA%E9%80%82%E5%BA%94MSDA-YOLO%E8%A7%A3%E8%AF%BB%EF%BC%8C%E6%81%B6%E5%8A%A3%E5%A4%A9%E6%B0%94%E4%B9%9F%E7%9C%8B%E5%BE%97%E8%A7%81/</url>
      
        <content type="html"><![CDATA[<p><br></p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210604/1.png" alt=""></p><blockquote><p>本文介绍了一种新的多尺度域自适应YOLO(MS-DAYOLO)框架，该框架在YOLOv4检测器的不同尺度上使用多个域自适应路径和相应的域分类器来生成域不变特征。</p></blockquote><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>Domain Adaptation在解决许多应用中遇到的Domain Shift问题方面发挥了重要作用。这个问题的出现是由于用于训练的源数据的分布与实际测试场景中使用的目标数据之间存在差异。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210604/2.png" alt=""></p><p> 本文介绍了一种新的多尺度域自适应YOLO(MS-DAYOLO)框架，该框架在YOLOv4检测器的不同尺度上使用多个域自适应路径和相应的域分类器来生成域不变特征。实验表明，当使用本文提出的MS-DAYOLO训练YOLOv4时，以及在自动驾驶应用中具有挑战性的天气条件的目标数据上进行测试时，目标检测性能得到了显著改善。</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><h3 id="2-1-YOLO-V4简述"><a href="#2-1-YOLO-V4简述" class="headerlink" title="2.1 YOLO V4简述"></a>2.1 YOLO V4简述</h3><p>相对于YOLO V3，YOLOv4包含了许多新的改进和新技术，以提高整体检测精度。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210604/3.png" alt=""></p><p>如图所示YOLOv4有3个主要部分:backbone、neck和head。</p><p>backbone负责提取不同尺度下的多层特征。</p><p>neck使用上采样层将backbone的3种不同尺度的特征聚集在一起，并将它们输入head。</p><p>最后，head预测目标周围的边界框以及与每个边界框相关联的类别概率。</p><p>本文作者的目标是将域适应应用于这3个特征（图中的F1、F2、F3），使它们对不同尺度的域变化具有鲁棒性，从而使它们在基于域适应的训练中向域不变性收敛。</p><h3 id="2-2-Domain-Adaptive-Network-for-YOLO"><a href="#2-2-Domain-Adaptive-Network-for-YOLO" class="headerlink" title="2.2 Domain Adaptive Network for YOLO"></a>2.2 Domain Adaptive Network for YOLO</h3><p>提出的域自适应网络(DAN)仅在训练时附加到YOLOv4中以学习域不变特征。对于推理，在推理阶段，将使用原始的YOLOv4体系结构中使用领域自适应训练的权重(没有DAN网络)。因此，本文所提出的框架不会增加推理过程中底层检测器的复杂性。</p><p>DAN使用backbone的3个不同的尺度特征作为输入。它有几个卷积层来预测域类。然后，利用二元交叉熵计算域分类损失(Ldc):</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210604/4.png" width = "400" align=center /></p><p>这里$t_i$为第$i$个训练图像的ground truth域标签，其中$t_i = 1$为源域，$t_i = 0$为目标域。$P^{(x,y)}$是第$i$个训练图像在位置$(x,Y)$的特征图。</p><p>DAN通过最小化这种上述损失来区分源域和目标域。另一方面，为了最大限度地学习域不变特征，对主干也进行了优化。因此，对于这2个域，backbone的特征应该是难以区分的。因此，这将提高目标域的目标检测性能。</p><p>为了解决联合最小化和最大化问题，作者采用了对抗学习策略。通过在backbone网络和DAN网络之间使用梯度反转层(GRL)来实现这个矛盾的目标。</p><p>GRL是一个双向算子，用于实现2个不同的优化目标。在前向传播方向上，GRL作为恒等算子。这导致了在DAN内执行局部反向传播时最小化分类错误的标准目标。另一方面，向主干网络反向传播时，GRL变成一个负标量$(\lambda)$。因此，在这种情况下，它会导致最大的二分类错误，这种最大化促进了由backbone生成领域不变特征。</p><p>为了计算检测损失(ldt)，只使用源图像。因此，通过最小化ldt, YOLOv4的所有3个部分(即backbone, neck和head)都得到了优化。另一方面，利用源标记图像和目标未标记图像计算域分类损失(Ldc)，Ldc通过最小化来优化DAN, Ldc通过最大化来优化backbone。因此，Ldet和Ldc都被用来优化backbone。换句话说，通过最小化以下总损失，backbone被优化了：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210604/5.png" width = "300" align=center /></p><p>其中$(\lambda)$是GRL的一个负标量，用来平衡检测损失和域分类损失。事实上，$(\lambda)$是用来优化DAN对backbone的影响。</p><h3 id="2-3-DAN-Architecture"><a href="#2-3-DAN-Architecture" class="headerlink" title="2.3 DAN Architecture"></a>2.3 DAN Architecture</h3><p>与在Domain Adaptive Faster R-CNN架构中只对特征提取器的最终尺度应用域自适应不同，本文分别开发了3个尺度的域自适应来解决梯度消失问题。也就是说，只对最终的尺度(F3)进行域自适应，由于之前的尺度(F1和F2)之间有很多层，存在梯度消失的问题，因此对之前的尺度(F1和F2)没有显著影响。</p><p>因此，作者采用了一个多尺度策略，将主干的三个特征F1、F2和F3通过三个相应的grl连接到DAN，如图2所示。对于每个尺度，GRL之后有2个卷积层，第1个卷积层将特征通道减少一半，第2个卷积层预测域类概率。最后，利用域分类器层计算域分类损失。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="3-1-Clear-gt-Foggy"><a href="#3-1-Clear-gt-Foggy" class="headerlink" title="3.1 Clear=&gt;Foggy"></a>3.1 Clear=&gt;Foggy</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210604/6.png" width = "500" align=center /></p><p>从这些结果可以看出，将域自适应应用于所有3个特征尺度提高了目标域的检测性能，取得了最好的结果。此外，作者提出的MS-DAYOLO在性能上大大优于原来的YOLOv4方法，几乎达到了理想(oracle)场景的性能。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210604/7.png" alt=""></p><h3 id="3-2-Sunny-gt-Rainy"><a href="#3-2-Sunny-gt-Rainy" class="headerlink" title="3.2 Sunny=&gt;Rainy"></a>3.2 Sunny=&gt;Rainy</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210604/8.png" width = "500" align=center /></p><p>结果如表2所示。在2个数据集中，本文的方法都比原始的YOLO得到了明显的性能提升。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>[1].Multiscale Domain Adaptive YOLO for Cross-Domain Object Detection<br></p>]]></content>
      
      
      <categories>
          
          <category> YOLO </category>
          
      </categories>
      
      
        <tags>
            
            <tag> YOLO </tag>
            
            <tag> 目标检测 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>没有Attention的Transformer依然是顶流！！！</title>
      <link href="/2021/09/05/5%20%E6%B2%A1%E6%9C%89Attention%E7%9A%84Transformer%E4%BE%9D%E7%84%B6%E6%98%AF%E9%A1%B6%E6%B5%81%EF%BC%81%EF%BC%81%EF%BC%81/"/>
      <url>/2021/09/05/5%20%E6%B2%A1%E6%9C%89Attention%E7%9A%84Transformer%E4%BE%9D%E7%84%B6%E6%98%AF%E9%A1%B6%E6%B5%81%EF%BC%81%EF%BC%81%EF%BC%81/</url>
      
        <content type="html"><![CDATA[<p><br></p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210602/1.png" alt=""></p><blockquote><p>本文主要介绍了Attention Free Transformer(AFT)，同时作者还引入了AFT-local和AFT-Conv，这两个模型在保持全局连通性的同时，利用了局域性和空间权重共享的思想。通过实验验证了AFT在所有benchmarks上具有竞争性能的同时具有出色的效率。</p></blockquote><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>本文主要介绍了Attention Free Transformer(AFT)，在AFT层中，首先将key和value与一组学习到的位置偏差结合起来，然后以元素方式将其结果与query相乘。这个新的操作在context size和特征维度上都具有线性的内存复杂度，使得它能够兼容大的输入和模型大小。</p><p>作者还引入了AFT-local和AFT-Conv，这两个模型变种在保持全局连通性的同时还利用了局域性和空间权重共享的思想。作者对2个自回归建模任务(CIFAR10和Enwik8)以及一个图像识别任务(ImageNet-1K分类)进行了广泛的实验。验证了AFT在所有benchmarks上不仅具有不错的性能，同时还具有出色的效率。</p><h2 id="本文方法"><a href="#本文方法" class="headerlink" title="本文方法"></a>本文方法</h2><h3 id="2-1-Attention-Free-Transformer"><a href="#2-1-Attention-Free-Transformer" class="headerlink" title="2.1 Attention Free Transformer"></a>2.1 Attention Free Transformer</h3><p>首先，定义了Attention Free Transformer(AFT)，它是MHA的plugin replacement，不需要改变Transformer的其他架构。给定输入X, AFT先将它们线性变换为$Q=XW^Q$,$K=XW^K$,$V=XW^V$，然后执行以下操作:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210602/2.png" width = "500" align=center /></p><p>其中，$\bigodot$是元素的乘积;$\sigma_q$是应用于query的非线性映射，默认为sigmoid;$w\in R^{T\times T}$是学习到成对的位置偏差。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210602/3.png" width = "500" align=center /></p><p>换句话说，对于每个目标位置$t$, AFT把加权平均的结果与具有元素级乘法的query相结合。而加权操作则是由key和一组学习成对的位置偏差组成。这提供了一个直接的优势，即不需要计算和存储消耗大的注意力矩阵，同时能够像MHA那样维护query和value之间的全局交互。</p><p>为了进一步了解AFT与MHA的关系可以将方程改写为：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210602/4.png" width = "400" align=center /></p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210602/5.png" width = "200" align=center /></p><p>这里使用上标$i$来索引矩阵的特征维数。在这种重新排列的形式中，能够再次用注意力来表达AFT。具体来说，对于每个位置有一个关注向量$a_t^i\in R^T$，每个维度由$Q、K、w$组成。换句话说，AFT可以解释为与特征尺寸一样多的Head中进行implicit attention，其中注意力矩阵采用因数分解的形式进行求解。</p><h3 id="2-2-AFT-variants-locality-weight-sharing-and-parameterization"><a href="#2-2-AFT-variants-locality-weight-sharing-and-parameterization" class="headerlink" title="2.2 AFT variants: locality, weight sharing and parameterization"></a>2.2 AFT variants: locality, weight sharing and parameterization</h3><h4 id="1-AFT-full"><a href="#1-AFT-full" class="headerlink" title="1 AFT-full"></a>1 AFT-full</h4><p>将下面方程中定义的AFT的基本版本表示为AFT-full：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210602/6.png" width = "500" align=center /></p><h4 id="2-AFT-local"><a href="#2-AFT-local" class="headerlink" title="2 AFT-local"></a>2 AFT-local</h4><p>作者发现了训练的标准Transformers倾向于表现出广泛的局部注意力模式。具体地说，把ImagenetNet预训练Vision Transformer(ViT)，由12层组成，每层6个Head。为了实现可视化忽略分类标记，将每一层的注意力张量reshape为6×196×196(因为ViT特征图的空间大小为14×14)。然后从ImageNet验证集中采样256张图像。对于每一层和每一个Head，计算平均的average relative 2d attentions、averaged across position和images。这就产生了一组尺寸为12×6×27×27的注意力map（如下图）。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210602/7.png" alt=""></p><p>通过上图可以看到，相对注意力Map显示出强烈的局部模式，特别是在lower layers。这激发了AFT的一种变体，称为<strong>AFT-local</strong>，即只在局部应用一组学习到的相对位置偏差:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210602/8.png" width = "400" align=center /></p><p>这里s≤T是一个局部window size。AFT-local提供了进一步的计算量的节省，包括参数的数量和时间/空间复杂度。</p><h4 id="3-AFT-simple"><a href="#3-AFT-simple" class="headerlink" title="3 AFT-simple"></a>3 AFT-simple</h4><p>AFT-local的一个极端形式是当s=0时，即没有学习到位置偏差。这就产生了一个极其简单的AFT版本，<strong>AFT-simple</strong>，有:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210602/9.png" width = "300" align=center /></p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210602/10.png" width = "300" align=center /></p><p>在这个版本中，context reduction进一步简化为元素操作和全局池化。其实AFT-simple类似于线性化注意，公式为：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210602/11.png" width = "400" align=center /></p><p>然而，AFT-simple完全摆脱了点积操作，这促使复杂度从$O(Td^2)$降低为$O(Td)$。</p><h4 id="4-AFT-conv"><a href="#4-AFT-conv" class="headerlink" title="4 AFT-conv"></a>4 AFT-conv</h4><p>作者还可以进一步扩展局部化locality的思想，<strong>加入空间权值共享</strong>，即<strong>卷积</strong>。这种变体与视觉任务特别相关，因为它通常希望将一个预训练模型扩展到可变大小的输入。具体来说，让$w_{t,t’}$的值只依赖于$t$和$t’$, 而$w.r.t.$为在给定的空间网格(1d或2d)中的相对位置。与CNN类似也可以学习多组位置偏差(重用head的概念作为参考)。为了考虑到#parameters随着 #heads的增加而增长，作者还采用了一个设计，将K的维度与#heads联系起来。这使得AFT-conv可修改为依赖于深度可分离卷积、全局池化和元素操作来实现。</p><p>类似的尺寸的AFT-conv学习到的相对位置偏差。 </p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210602/12.png" alt=""></p><p>举一个例子，这里将模型构型表示为AFT-conv-h-s，其中h为head的个数，s×s为2d local window size。$w\in R^{h\times s\times s}, Q,V\in R^{T\times h\times d/h}, K\in R^{T\times h}$，于是对于每一个head $i=1,2,…,h$来说，有：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210602/13.png" width = "500" align=center /></p><p>注意，上式可以很容易地解释为一个特殊的卷积层，具有：<br>1) <strong>全局连通性</strong></p><p>2) <strong>非负卷积权值</strong></p><p>3) <strong>复杂的除法/乘法门机制</strong></p><p>实验表明，这3个方面对AFT-conv的性能都有显著的影响。</p><h4 id="5-Parameterization"><a href="#5-Parameterization" class="headerlink" title="5 Parameterization"></a>5 Parameterization</h4><p>根据经验，作者发现适当地参数化位置偏差是很重要的。</p><p>对于AFT-full和AFT-local，采用w的因数分解形式:</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210602/14.png" width = "400" align=center /></p><p>其中$d’$是一个小的嵌入维数(例如128)。这种简单的因式分解不仅大大减少了参数量，而且在训练和测试中都有效地提高了模型的性能。</p><p>对于AFT-conv，因式分解的技巧并不适用。相反，作者采用一个简单的重新参数化，对于每个head i，让：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210602/15.png" width = "300" align=center /></p><p>其中$\gamma\in R^h, \beta \in R^h$是可学习增益和偏置参数，均初始化为0。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210602/16.png" width = "500" align=center /></p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="3-1-Image-Autoregressive-Modeling"><a href="#3-1-Image-Autoregressive-Modeling" class="headerlink" title="3.1 Image Autoregressive Modeling"></a>3.1 Image Autoregressive Modeling</h3><h4 id="SOTA模型对比"><a href="#SOTA模型对比" class="headerlink" title="SOTA模型对比"></a>SOTA模型对比</h4><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210602/17.png" alt=""></p><h4 id="Factorization的影响"><a href="#Factorization的影响" class="headerlink" title="Factorization的影响"></a>Factorization的影响</h4><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210602/18.png" width = "500" align=center /></p><h3 id="3-2-Language-Modeling"><a href="#3-2-Language-Modeling" class="headerlink" title="3.2 Language Modeling"></a>3.2 Language Modeling</h3><h4 id="SOTA模型对比-1"><a href="#SOTA模型对比-1" class="headerlink" title="SOTA模型对比"></a>SOTA模型对比</h4><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210602/19.png" alt=""></p><h4 id="local-window-size的影响"><a href="#local-window-size的影响" class="headerlink" title="local window size的影响"></a>local window size的影响</h4><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210602/20.png" alt=""></p><h4 id="Longer-sequence-size"><a href="#Longer-sequence-size" class="headerlink" title="Longer sequence size"></a>Longer sequence size</h4><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210602/21.png" alt=""></p><h3 id="3-3-Image-Classification"><a href="#3-3-Image-Classification" class="headerlink" title="3.3 Image Classification"></a>3.3 Image Classification</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210602/22.png" alt=""></p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210602/23.png" alt=""></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>[1].An Attention Free Transformer<br></p>]]></content>
      
      
      <categories>
          
          <category> Transformer </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Tansformer </tag>
            
            <tag> Attention </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>从零开始边缘部署轻量化人脸检测模型————EAIDK310部署篇</title>
      <link href="/2021/09/05/4%20%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E8%BE%B9%E7%BC%98%E9%83%A8%E7%BD%B2%E8%BD%BB%E9%87%8F%E5%8C%96%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B%E6%A8%A1%E5%9E%8B%E2%80%94%E2%80%94%E2%80%94%E2%80%94EAIDK310%E9%83%A8%E7%BD%B2%E7%AF%87/"/>
      <url>/2021/09/05/4%20%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E8%BE%B9%E7%BC%98%E9%83%A8%E7%BD%B2%E8%BD%BB%E9%87%8F%E5%8C%96%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B%E6%A8%A1%E5%9E%8B%E2%80%94%E2%80%94%E2%80%94%E2%80%94EAIDK310%E9%83%A8%E7%BD%B2%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<p><br></p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210527/0.png" alt=""></p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210527/1.png" alt=""></p><blockquote><p>继续上一章的话题，前面我们主要聊到关于人脸检测模型UltraFace的训练任务，本文将和大家讨论在开发板上如何部署UltraFace模型，并进行实时视频人脸检测，或者图片流人脸检测。</p></blockquote><h2 id="Tengine简介"><a href="#Tengine简介" class="headerlink" title="Tengine简介"></a>Tengine简介</h2><p>Tengine 由 OPEN AI LAB 主导开发，该项目实现了深度学习神经网络模型在嵌入式设备上的快速、高效部署需求。为实现在众多 AIoT 应用中的跨平台部署，本项目基于原有 Tengine 项目使用 C 语言进行重构，针对嵌入式设备资源有限的特点进行了深度框架裁剪。同时采用了完全分离的前后端设计，有利于 CPU、GPU、NPU 等异构计算单元的快速移植和部署，同时降低评估和迁移成本。</p><h3 id="Tengine推理流程"><a href="#Tengine推理流程" class="headerlink" title="Tengine推理流程"></a>Tengine推理流程</h3><p>依照顺序调用Tengine核心API如下：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210530/1.png" alt=""></p><h2 id="模块实现"><a href="#模块实现" class="headerlink" title="模块实现"></a>模块实现</h2><h3 id="1-模型转换"><a href="#1-模型转换" class="headerlink" title="1 模型转换"></a>1 模型转换</h3><h4 id="第1步：转换到onnx模型"><a href="#第1步：转换到onnx模型" class="headerlink" title="第1步：转换到onnx模型"></a>第1步：转换到onnx模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">model_path = <span class="string">&quot;models/pretrained/version-RFB-320.pth&quot;</span></span><br><span class="line">net = create_Mb_Tiny_RFB_fd(<span class="built_in">len</span>(class_names), is_test=<span class="literal">True</span>)</span><br><span class="line">net.load(model_path)</span><br><span class="line">net.<span class="built_in">eval</span>()</span><br><span class="line">net.to(<span class="string">&quot;cuda&quot;</span>)</span><br><span class="line"></span><br><span class="line">model_name = model_path.split(<span class="string">&quot;/&quot;</span>)[-<span class="number">1</span>].split(<span class="string">&quot;.&quot;</span>)[<span class="number">0</span>]</span><br><span class="line">model_path = <span class="string">f&quot;models/onnx/<span class="subst">&#123;model_name&#125;</span>.onnx&quot;</span></span><br><span class="line"></span><br><span class="line">dummy_input = torch.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">240</span>, <span class="number">320</span>).to(<span class="string">&quot;cuda&quot;</span>)</span><br><span class="line">torch.onnx.export(net, dummy_input, model_path, verbose=<span class="literal">False</span>, input_names=[<span class="string">&#x27;input&#x27;</span>], output_names=[<span class="string">&#x27;scores&#x27;</span>, <span class="string">&#x27;boxes&#x27;</span>])</span><br></pre></td></tr></table></figure><h4 id="第2步：编译Tengine模型转换工具"><a href="#第2步：编译Tengine模型转换工具" class="headerlink" title="第2步：编译Tengine模型转换工具"></a>第2步：编译Tengine模型转换工具</h4><p>依赖库安装</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install libprotobuf-dev protobuf-compiler</span><br></pre></td></tr></table></figure><p>源码编译<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir build &amp;&amp; cd build</span><br><span class="line">cmake ..</span><br><span class="line">make -j`nproc` &amp;&amp; make install</span><br></pre></td></tr></table></figure><br>编译完成后，生成的可行性文件tm_convert_tool存放在 ./build/install/bin/ 目录下。</p><h4 id="第3步：转换onnx模型为tmfile模型"><a href="#第3步：转换onnx模型为tmfile模型" class="headerlink" title="第3步：转换onnx模型为tmfile模型"></a>第3步：转换onnx模型为tmfile模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./tm_convert_tool -m xxx.onnx -o xxx.tmfile</span><br></pre></td></tr></table></figure><ul><li>-m 为<em>.caffemodel, </em>.params, <em>.weight, </em>.pb, <em>.onnx, </em>.tflite等模型；</li><li>-o 为output fp32 tmfile</li></ul><h3 id="2-NMS计算"><a href="#2-NMS计算" class="headerlink" title="2 NMS计算"></a>2 NMS计算</h3><h3 id="伪代码："><a href="#伪代码：" class="headerlink" title="伪代码："></a>伪代码：</h3><ul><li><p>1 将各组box按照score降序排列;</p></li><li><p>2 从score最大值开始，置为当前box，保存idex，然后依次遍历后面的box，计算与当前box的IOU值，若大于阈值，则抑制，不会输出;</p></li><li><p>3 完成一轮遍历后，继续选择下一个非抑制的box作为当前box，重复步骤2;</p></li><li><p>4 返回没有被抑制的index即符合条件的box;</p></li></ul><h3 id="python版本"><a href="#python版本" class="headerlink" title="python版本"></a>python版本</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">NMS</span>(<span class="params">dects,threshhold</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    detcs:二维数组(n_samples,5)</span></span><br><span class="line"><span class="string">    5列：x1,y1,x2,y2,score</span></span><br><span class="line"><span class="string">    threshhold: IOU阈值</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    x1=dects[:,<span class="number">0</span>]</span><br><span class="line">    y1=dects[:,<span class="number">1</span>]</span><br><span class="line">    x2=dects[:,<span class="number">2</span>]</span><br><span class="line">    y2=dects[:,<span class="number">3</span>]</span><br><span class="line">    score=dects[:,<span class="number">4</span>]</span><br><span class="line">    ndects=dects.shape[<span class="number">0</span>]<span class="comment">#box的数量</span></span><br><span class="line">    area=(x2-x1+<span class="number">1</span>)*(y2-y1+<span class="number">1</span>)</span><br><span class="line">    order=score.argsort()[::-<span class="number">1</span>] <span class="comment">#score从大到小排列的indexs,一维数组</span></span><br><span class="line">    keep=[] <span class="comment">#保存符合条件的index</span></span><br><span class="line">    suppressed=np.array([<span class="number">0</span>]*ndects) <span class="comment">#初始化为0，若大于threshhold,变为1，表示被抑制</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> _i <span class="keyword">in</span> <span class="built_in">range</span>(ndects):</span><br><span class="line">        i=order[_i]  <span class="comment">#从得分最高的开始遍历</span></span><br><span class="line">        <span class="keyword">if</span> suppressed[i]==<span class="number">1</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        keep.append(i) </span><br><span class="line">        <span class="keyword">for</span> _j <span class="keyword">in</span> <span class="built_in">range</span>(i+<span class="number">1</span>,ndects):</span><br><span class="line">            j=order[_j]</span><br><span class="line">            <span class="keyword">if</span> suppressed[j]==<span class="number">1</span>: <span class="comment">#若已经被抑制，跳过</span></span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            xx1=np.<span class="built_in">max</span>(x1[i],x1[j])<span class="comment">#求两个box的交集面积interface</span></span><br><span class="line">            yy1=np.<span class="built_in">max</span>(y1[i],y1j])</span><br><span class="line">            xx2=np.<span class="built_in">min</span>(x2[i],x2[j])</span><br><span class="line">            yy2=np.<span class="built_in">min</span>(y2[i],y2[j])</span><br><span class="line">            w=np.<span class="built_in">max</span>(<span class="number">0</span>,xx2-xx1+<span class="number">1</span>)</span><br><span class="line">            h=np.<span class="built_in">max</span>(<span class="number">0</span>,yy2-yy1+<span class="number">1</span>)</span><br><span class="line">            interface=w*h</span><br><span class="line">            overlap=interface/(area[i]+area[j]-interface) <span class="comment">#计算IOU（交/并）</span></span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> overlap&gt;=threshhold:<span class="comment">#IOU若大于阈值，则抑制</span></span><br><span class="line">                suppressed[j]=<span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> keep</span><br></pre></td></tr></table></figure><h3 id="C-版本"><a href="#C-版本" class="headerlink" title="C++版本"></a>C++版本</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">UltraFace::nms</span><span class="params">(std::vector&lt;FaceInfo&gt; &amp;input, std::vector&lt;FaceInfo&gt; &amp;output, <span class="keyword">int</span> type)</span> </span>&#123;</span><br><span class="line">    <span class="comment">//根据score对候选框进行 sort 排序操作</span></span><br><span class="line">    std::<span class="built_in">sort</span>(input.<span class="built_in">begin</span>(), input.<span class="built_in">end</span>(), [](<span class="keyword">const</span> FaceInfo &amp;a, <span class="keyword">const</span> FaceInfo &amp;b) &#123; <span class="keyword">return</span> a.score &gt; b.score; &#125;);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> box_num = input.<span class="built_in">size</span>();</span><br><span class="line"></span><br><span class="line">    <span class="function">std::vector&lt;<span class="keyword">int</span>&gt; <span class="title">merged</span><span class="params">(box_num, <span class="number">0</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; box_num; i++) &#123;</span><br><span class="line">        <span class="keyword">if</span> (merged[i])</span><br><span class="line">            <span class="keyword">continue</span>;</span><br><span class="line">        std::vector&lt;FaceInfo&gt; buf;</span><br><span class="line"></span><br><span class="line">        buf.<span class="built_in">push_back</span>(input[i]);</span><br><span class="line">        merged[i] = <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">float</span> h0 = input[i].y2 - input[i].y1 + <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">float</span> w0 = input[i].x2 - input[i].x1 + <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">float</span> area0 = h0 * w0;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j = i + <span class="number">1</span>; j &lt; box_num; j++) &#123;</span><br><span class="line">            <span class="keyword">if</span> (merged[j])</span><br><span class="line">                <span class="keyword">continue</span>;</span><br><span class="line">            <span class="comment">//确立每个候选框的坐标以及宽高</span></span><br><span class="line">            <span class="keyword">float</span> inner_x0 = input[i].x1 &gt; input[j].x1 ? input[i].x1 : input[j].x1;</span><br><span class="line">            <span class="keyword">float</span> inner_y0 = input[i].y1 &gt; input[j].y1 ? input[i].y1 : input[j].y1;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">float</span> inner_x1 = input[i].x2 &lt; input[j].x2 ? input[i].x2 : input[j].x2;</span><br><span class="line">            <span class="keyword">float</span> inner_y1 = input[i].y2 &lt; input[j].y2 ? input[i].y2 : input[j].y2;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">float</span> inner_h = inner_y1 - inner_y0 + <span class="number">1</span>;</span><br><span class="line">            <span class="keyword">float</span> inner_w = inner_x1 - inner_x0 + <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (inner_h &lt;= <span class="number">0</span> || inner_w &lt;= <span class="number">0</span>)</span><br><span class="line">                <span class="keyword">continue</span>;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">float</span> inner_area = inner_h * inner_w;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">float</span> h1 = input[j].y2 - input[j].y1 + <span class="number">1</span>;</span><br><span class="line">            <span class="keyword">float</span> w1 = input[j].x2 - input[j].x1 + <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">float</span> area1 = h1 * w1;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">float</span> score;</span><br><span class="line">            <span class="comment">//计算IOU</span></span><br><span class="line">            score = inner_area / (area0 + area1 - inner_area);</span><br><span class="line">            <span class="comment">//根据阈值进行极大值抑制的筛选</span></span><br><span class="line">            <span class="keyword">if</span> (score &gt; iou_threshold) &#123;</span><br><span class="line">                merged[j] = <span class="number">1</span>;</span><br><span class="line">                buf.<span class="built_in">push_back</span>(input[j]);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-3-获取候选框"><a href="#2-3-获取候选框" class="headerlink" title="2.3 获取候选框"></a>2.3 获取候选框</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//获取候选框</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">UltraFace::generateBBox</span><span class="params">(std::vector&lt;FaceInfo&gt; &amp;bbox_collection, <span class="keyword">tensor_t</span> scores, <span class="keyword">tensor_t</span> boxes)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">float</span>* scores_blob = ( <span class="keyword">float</span>* )<span class="built_in">get_tensor_buffer</span>(scores);</span><br><span class="line">    <span class="keyword">float</span>* boxes_blob = ( <span class="keyword">float</span>* )<span class="built_in">get_tensor_buffer</span>(boxes);</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; num_anchors; i++) &#123;</span><br><span class="line">        <span class="keyword">if</span> (scores_blob[i * <span class="number">2</span> + <span class="number">1</span>] &gt; score_threshold) &#123;</span><br><span class="line">            FaceInfo rects;</span><br><span class="line">            <span class="comment">//确定坐标中心以及box的宽高</span></span><br><span class="line">            <span class="keyword">float</span> x_center = boxes_blob[i * <span class="number">4</span>] * center_variance * priors[i][<span class="number">2</span>] + priors[i][<span class="number">0</span>];</span><br><span class="line">            <span class="keyword">float</span> y_center = boxes_blob[i * <span class="number">4</span> + <span class="number">1</span>] * center_variance * priors[i][<span class="number">3</span>] + priors[i][<span class="number">1</span>];</span><br><span class="line">            <span class="keyword">float</span> w = <span class="built_in">exp</span>(boxes_blob[i * <span class="number">4</span> + <span class="number">2</span>] * size_variance) * priors[i][<span class="number">2</span>];</span><br><span class="line">            <span class="keyword">float</span> h = <span class="built_in">exp</span>(boxes_blob[i * <span class="number">4</span> + <span class="number">3</span>] * size_variance) * priors[i][<span class="number">3</span>];</span><br><span class="line">            <span class="comment">//截取坐标结果</span></span><br><span class="line">            rects.x1 = <span class="built_in">clip</span>(x_center - w / <span class="number">2.0</span>, <span class="number">1</span>) * image_w;</span><br><span class="line">            rects.y1 = <span class="built_in">clip</span>(y_center - h / <span class="number">2.0</span>, <span class="number">1</span>) * image_h;</span><br><span class="line">            rects.x2 = <span class="built_in">clip</span>(x_center + w / <span class="number">2.0</span>, <span class="number">1</span>) * image_w;</span><br><span class="line">            rects.y2 = <span class="built_in">clip</span>(y_center + h / <span class="number">2.0</span>, <span class="number">1</span>) * image_h;</span><br><span class="line">            rects.score = <span class="built_in">clip</span>(scores_blob[i * <span class="number">2</span> + <span class="number">1</span>], <span class="number">1</span>);</span><br><span class="line">            bbox_collection.<span class="built_in">push_back</span>(rects);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-4-模型检测函数"><a href="#2-4-模型检测函数" class="headerlink" title="2.4 模型检测函数"></a>2.4 模型检测函数</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//模型检测函数</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">UltraFace::detect</span><span class="params">(cv::Mat &amp;raw_image, std::vector&lt;FaceInfo&gt; &amp;face_list)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (raw_image.<span class="built_in">empty</span>()) &#123;</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;image is empty ,please check!&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    image_h = raw_image.rows;</span><br><span class="line">    image_w = raw_image.cols;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> img_size      = in_w * in_h * <span class="number">3</span>;</span><br><span class="line">    <span class="keyword">float</span>* input_data = ( <span class="keyword">float</span>* )<span class="built_in">malloc</span>(img_size * <span class="built_in"><span class="keyword">sizeof</span></span>(<span class="keyword">float</span>));</span><br><span class="line">    <span class="comment">// 获取来自opencv读取的图片或者视频数据，并返回一个适应模型输入的结果</span></span><br><span class="line">    <span class="built_in">get_input_data_cv</span>(raw_image, input_data, in_w, in_h, mean_vals, norm_vals, <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">set_tensor_buffer</span>(input_tensor, input_data, (in_w * in_h * <span class="number">3</span>) * <span class="number">4</span>) &lt; <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Set input tensor buffer failed\n&quot;</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//开始计时⏲</span></span><br><span class="line">    <span class="keyword">auto</span> start = chrono::steady_clock::<span class="built_in">now</span>();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 6、Run网络</span></span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">run_graph</span>(graph, <span class="number">1</span>) &lt; <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Run graph failed\n&quot;</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取输出结果</span></span><br><span class="line">    string scores = <span class="string">&quot;scores&quot;</span>;</span><br><span class="line">    string boxes = <span class="string">&quot;boxes&quot;</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//7.1、获取分类得分结果</span></span><br><span class="line">    <span class="keyword">tensor_t</span> tensor_scores = <span class="built_in">get_graph_tensor</span>(graph, scores.<span class="built_in">c_str</span>());</span><br><span class="line">    <span class="comment">//7.2、获取检测框坐标结果</span></span><br><span class="line">    <span class="keyword">tensor_t</span> tensor_boxes = <span class="built_in">get_graph_tensor</span>(graph, boxes.<span class="built_in">c_str</span>());</span><br><span class="line"></span><br><span class="line">    std::vector&lt;FaceInfo&gt; bbox_collection;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//结束计时，然后计算推理时间</span></span><br><span class="line">    <span class="keyword">auto</span> end = chrono::steady_clock::<span class="built_in">now</span>();</span><br><span class="line">    chrono::duration&lt;<span class="keyword">double</span>&gt; elapsed = end - start;</span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;inference time:&quot;</span> &lt;&lt; elapsed.<span class="built_in">count</span>() &lt;&lt; <span class="string">&quot; s&quot;</span> &lt;&lt; endl;</span><br><span class="line">    <span class="comment">//后处理操作，主要是获取BBox以及NMS操作</span></span><br><span class="line">    <span class="built_in">generateBBox</span>(bbox_collection, tensor_scores, tensor_boxes);</span><br><span class="line">    <span class="built_in">nms</span>(bbox_collection, face_list);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">free</span>(input_data);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-5-主函数"><a href="#2-5-主函数" class="headerlink" title="2.5 主函数"></a>2.5 主函数</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&quot;UltraFace.hpp&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;opencv2/opencv.hpp&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;opencv2/highgui.hpp&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;opencv2/imgproc.hpp&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    string tengine_path = <span class="string">&quot;/home/chaucer/Tengine_Tutorial/2_FaceDetector/models/version-RFB-320_simplified.tmfile&quot;</span>;</span><br><span class="line">    <span class="function">UltraFace <span class="title">ultraface</span><span class="params">(tengine_path, <span class="number">320</span>, <span class="number">240</span>, <span class="number">4</span>, <span class="number">0.65</span>)</span></span>; <span class="comment">// config model input</span></span><br><span class="line"></span><br><span class="line">    cv::Mat frame;</span><br><span class="line">    <span class="comment">//cv::VideoCapture capture(0);</span></span><br><span class="line">    <span class="function">cv::VideoCapture <span class="title">capture</span><span class="params">(<span class="string">&quot;/home/chaucer/face_detect/test_1.mp4&quot;</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//cv::Mat frame = cv::imread(image_file);</span></span><br><span class="line">    <span class="keyword">while</span>(<span class="number">1</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        capture &gt;&gt; frame;</span><br><span class="line">        <span class="keyword">auto</span> start = chrono::steady_clock::<span class="built_in">now</span>();</span><br><span class="line">        vector&lt;FaceInfo&gt; face_info;</span><br><span class="line">        ultraface.<span class="built_in">detect</span>(frame, face_info);</span><br><span class="line"></span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;face_info &quot;</span> &lt;&lt; face_info.<span class="built_in">size</span>() &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">auto</span> face : face_info) &#123;</span><br><span class="line">            <span class="function">cv::Point <span class="title">pt1</span><span class="params">(face.x1, face.y1)</span></span>;</span><br><span class="line">            <span class="function">cv::Point <span class="title">pt2</span><span class="params">(face.x2, face.y2)</span></span>;</span><br><span class="line">            cv::<span class="built_in">rectangle</span>(frame, pt1, pt2, cv::<span class="built_in">Scalar</span>(<span class="number">0</span>, <span class="number">255</span>, <span class="number">0</span>), <span class="number">2</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">auto</span> end = chrono::steady_clock::<span class="built_in">now</span>();</span><br><span class="line">        chrono::duration&lt;<span class="keyword">double</span>&gt; elapsed = end - start;</span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;all time: &quot;</span> &lt;&lt; elapsed.<span class="built_in">count</span>() &lt;&lt; <span class="string">&quot; s&quot;</span> &lt;&lt; endl;</span><br><span class="line">        cv::<span class="built_in">imshow</span>(<span class="string">&quot;UltraFace&quot;</span>, frame);</span><br><span class="line">        cv::<span class="built_in">waitKey</span>(<span class="number">1</span>);</span><br><span class="line">        string result_name = <span class="string">&quot;result&quot;</span> + <span class="built_in">to_string</span>(<span class="number">2</span>) + <span class="string">&quot;.jpg&quot;</span>;</span><br><span class="line">        cv::<span class="built_in">imwrite</span>(result_name, frame);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="输出结果"><a href="#输出结果" class="headerlink" title="输出结果"></a>输出结果</h2><h3 id="3-1-图片检测结果"><a href="#3-1-图片检测结果" class="headerlink" title="3.1 图片检测结果"></a>3.1 图片检测结果</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210530/2.png" alt=""></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>[1].<a href="https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB">https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB</a><br><br>[2].<a href="https://github.com/OAID/Tengine">https://github.com/OAID/Tengine</a><br><br>[3].<a href="https://github.com/jiangzhongbo/Tengine_Tutorial">https://github.com/jiangzhongbo/Tengine_Tutorial</a><br></p>]]></content>
      
      
      <categories>
          
          <category> 项目部署 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人脸识别 </tag>
            
            <tag> 人脸检测 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>从零开始边缘部署轻量化人脸检测模型————训练篇</title>
      <link href="/2021/09/05/3%20%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E8%BE%B9%E7%BC%98%E9%83%A8%E7%BD%B2%E8%BD%BB%E9%87%8F%E5%8C%96%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B%E6%A8%A1%E5%9E%8B%E2%80%94%E2%80%94%E8%AE%AD%E7%BB%83%E7%AF%87/"/>
      <url>/2021/09/05/3%20%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E8%BE%B9%E7%BC%98%E9%83%A8%E7%BD%B2%E8%BD%BB%E9%87%8F%E5%8C%96%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B%E6%A8%A1%E5%9E%8B%E2%80%94%E2%80%94%E8%AE%AD%E7%BB%83%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210527/0.png" alt=""></p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210527/1.png" alt=""></p><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>该模型是针对边缘计算设备设计的轻量人脸检测模型。</p><ul><li>在模型大小上，默认FP32精度下（.pth）文件大小为 1.04~1.1MB，推理框架int8量化后大小为 300KB 左右。</li><li>在模型计算量上，320x240的输入分辨率下 90~109 MFlops左右。</li><li>模型有两个版本，version-slim(主干精简速度略快)，version-RFB(加入了修改后的RFB模块，精度更高)。</li><li>提供320x240、640x480不同输入分辨率下使用widerface训练的预训练模型，更好的工作于不同的应用场景。</li></ul><h2 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h2><h3 id="2-1-输入尺寸的选择"><a href="#2-1-输入尺寸的选择" class="headerlink" title="2.1 输入尺寸的选择"></a>2.1 输入尺寸的选择</h3><p>由于涉及实际部署时的推理速度，因此模型输入尺寸的选择也是一个很重要的话题。</p><p>在作者的原github中，也提到了一点，如果在实际部署的场景中大多数情况为中近距离、人脸大同时人脸的数量也比较少的时候，则可以采用$320\times 240$的输入尺寸；</p><p>如果在实际部署的场景中大多数情况为中远距离、人脸小同时人脸的数量也比较多的时候，则可以采用$640\times 480$或者$480\times 360$的输入尺寸；</p><blockquote><p>这里由于使用的是EAIDK310进行部署测试，边缘性能不是很好，因此选择原作者推荐的最小尺寸$320\times 240$进行训练和部署测试。<br><br><strong>注意：过小的输入分辨率虽然会明显加快推理速度，但是会大幅降低小人脸的召回率。</strong></p></blockquote><h3 id="2-2-数据筛选"><a href="#2-2-数据筛选" class="headerlink" title="2.2 数据筛选"></a>2.2 数据筛选</h3><p>由于widerface官网数据集中有比较多的低于10像素的人脸照片，因此在这里选择剔除这些像素长宽低于10个pixel的照片；</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210527/2.png" alt=""></p><blockquote><p>这样做的原因是：<strong>不清楚的人脸，不太利于高效模型的收敛，所以需要进行过滤训练。</strong></p></blockquote><h2 id="SSD网络结构"><a href="#SSD网络结构" class="headerlink" title="SSD网络结构"></a>SSD网络结构</h2><p>SSD是一个端到端的模型，所有的检测过程和识别过程都是在同一个网络中进行的；同时SSD借鉴了Faster R-CNN的Anchor机制的想法，这样就像相当于在基于回归的的检测过程中结合了区域的思想，可以使得检测效果较定制化边界框的YOLO v1有比较好的提升。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210527/3.png" alt=""></p><p>SSD较传统的检测方法使用顶层特征图的方法选择了使用多尺度特征图，因为在比较浅的特征图中可以对于小目标有比较好的表达，随着特征图的深入，网络对于比较大特征也有了比较好表达能力，故SSD选择使用多尺度特征图可以很好的兼顾大目标和小目标。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210527/4.png" alt=""></p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210527/5.png" alt=""></p><p>SSD模型结构如下：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210527/6.png" alt=""></p><p>这里关于SSD不进行更多的阐述，想了解的小伙伴可以扫描下方的二维码查看（是小编在CSDN的记录，非常详细！！！）：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210527/7.png" alt=""></p><p>整个项目模型搭建如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 网络的主题结构为SSD模型</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SSD</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, num_classes: <span class="built_in">int</span>, base_net: nn.ModuleList, source_layer_indexes: <span class="type">List</span>[<span class="built_in">int</span>],</span></span></span><br><span class="line"><span class="params"><span class="function">                 extras: nn.ModuleList, classification_headers: nn.ModuleList,</span></span></span><br><span class="line"><span class="params"><span class="function">                 regression_headers: nn.ModuleList, is_test=<span class="literal">False</span>, config=<span class="literal">None</span>, device=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Compose a SSD model using the given components.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(SSD, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.num_classes = num_classes</span><br><span class="line">        self.base_net = base_net</span><br><span class="line">        self.source_layer_indexes = source_layer_indexes</span><br><span class="line">        self.extras = extras</span><br><span class="line">        self.classification_headers = classification_headers</span><br><span class="line">        self.regression_headers = regression_headers</span><br><span class="line">        self.is_test = is_test</span><br><span class="line">        self.config = config</span><br><span class="line"></span><br><span class="line">        <span class="comment"># register layers in source_layer_indexes by adding them to a module list</span></span><br><span class="line">        self.source_layer_add_ons = nn.ModuleList([t[<span class="number">1</span>] <span class="keyword">for</span> t <span class="keyword">in</span> source_layer_indexes</span><br><span class="line">                                                   <span class="keyword">if</span> <span class="built_in">isinstance</span>(t, <span class="built_in">tuple</span>) <span class="keyword">and</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(t, GraphPath)])</span><br><span class="line">        <span class="keyword">if</span> device:</span><br><span class="line">            self.device = device</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">        <span class="keyword">if</span> is_test:</span><br><span class="line">            self.config = config</span><br><span class="line">            self.priors = config.priors.to(self.device)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x: torch.Tensor</span>) -&gt; <span class="type">Tuple</span>[torch.Tensor, torch.Tensor]:</span></span><br><span class="line">        confidences = []</span><br><span class="line">        locations = []</span><br><span class="line">        start_layer_index = <span class="number">0</span></span><br><span class="line">        header_index = <span class="number">0</span></span><br><span class="line">        end_layer_index = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> end_layer_index <span class="keyword">in</span> self.source_layer_indexes:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(end_layer_index, GraphPath):</span><br><span class="line">                path = end_layer_index</span><br><span class="line">                end_layer_index = end_layer_index.s0</span><br><span class="line">                added_layer = <span class="literal">None</span></span><br><span class="line">            <span class="keyword">elif</span> <span class="built_in">isinstance</span>(end_layer_index, <span class="built_in">tuple</span>):</span><br><span class="line">                added_layer = end_layer_index[<span class="number">1</span>]</span><br><span class="line">                end_layer_index = end_layer_index[<span class="number">0</span>]</span><br><span class="line">                path = <span class="literal">None</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                added_layer = <span class="literal">None</span></span><br><span class="line">                path = <span class="literal">None</span></span><br><span class="line">            <span class="keyword">for</span> layer <span class="keyword">in</span> self.base_net[start_layer_index: end_layer_index]:</span><br><span class="line">                x = layer(x)</span><br><span class="line">            <span class="keyword">if</span> added_layer:</span><br><span class="line">                y = added_layer(x)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                y = x</span><br><span class="line">            <span class="keyword">if</span> path:</span><br><span class="line">                sub = <span class="built_in">getattr</span>(self.base_net[end_layer_index], path.name)</span><br><span class="line">                <span class="keyword">for</span> layer <span class="keyword">in</span> sub[:path.s1]:</span><br><span class="line">                    x = layer(x)</span><br><span class="line">                y = x</span><br><span class="line">                <span class="keyword">for</span> layer <span class="keyword">in</span> sub[path.s1:]:</span><br><span class="line">                    x = layer(x)</span><br><span class="line">                end_layer_index += <span class="number">1</span></span><br><span class="line">            start_layer_index = end_layer_index</span><br><span class="line">            confidence, location = self.compute_header(header_index, y)</span><br><span class="line">            header_index += <span class="number">1</span></span><br><span class="line">            confidences.append(confidence)</span><br><span class="line">            locations.append(location)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.base_net[end_layer_index:]:</span><br><span class="line">            x = layer(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.extras:</span><br><span class="line">            x = layer(x)</span><br><span class="line">            confidence, location = self.compute_header(header_index, x)</span><br><span class="line">            header_index += <span class="number">1</span></span><br><span class="line">            confidences.append(confidence)</span><br><span class="line">            locations.append(location)</span><br><span class="line"></span><br><span class="line">        confidences = torch.cat(confidences, <span class="number">1</span>)</span><br><span class="line">        locations = torch.cat(locations, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.is_test:</span><br><span class="line">            confidences = F.softmax(confidences, dim=<span class="number">2</span>)</span><br><span class="line">            boxes = box_utils.convert_locations_to_boxes(</span><br><span class="line">                locations, self.priors, self.config.center_variance, self.config.size_variance</span><br><span class="line">            )</span><br><span class="line">            boxes = box_utils.center_form_to_corner_form(boxes)</span><br><span class="line">            <span class="keyword">return</span> confidences, boxes</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> confidences, locations</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compute_header</span>(<span class="params">self, i, x</span>):</span></span><br><span class="line">        confidence = self.classification_headers[i](x)</span><br><span class="line">        confidence = confidence.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>).contiguous()</span><br><span class="line">        confidence = confidence.view(confidence.size(<span class="number">0</span>), -<span class="number">1</span>, self.num_classes)</span><br><span class="line"></span><br><span class="line">        location = self.regression_headers[i](x)</span><br><span class="line">        location = location.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>).contiguous()</span><br><span class="line">        location = location.view(location.size(<span class="number">0</span>), -<span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> confidence, location</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_from_base_net</span>(<span class="params">self, model</span>):</span></span><br><span class="line">        self.base_net.load_state_dict(torch.load(model, map_location=<span class="keyword">lambda</span> storage, loc: storage), strict=<span class="literal">True</span>)</span><br><span class="line">        self.source_layer_add_ons.apply(_xavier_init_)</span><br><span class="line">        self.extras.apply(_xavier_init_)</span><br><span class="line">        self.classification_headers.apply(_xavier_init_)</span><br><span class="line">        self.regression_headers.apply(_xavier_init_)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_from_pretrained_ssd</span>(<span class="params">self, model</span>):</span></span><br><span class="line">        state_dict = torch.load(model, map_location=<span class="keyword">lambda</span> storage, loc: storage)</span><br><span class="line">        state_dict = &#123;k: v <span class="keyword">for</span> k, v <span class="keyword">in</span> state_dict.items() <span class="keyword">if</span> <span class="keyword">not</span> (k.startswith(<span class="string">&quot;classification_headers&quot;</span>) <span class="keyword">or</span> k.startswith(<span class="string">&quot;regression_headers&quot;</span>))&#125;</span><br><span class="line">        model_dict = self.state_dict()</span><br><span class="line">        model_dict.update(state_dict)</span><br><span class="line">        self.load_state_dict(model_dict)</span><br><span class="line">        self.classification_headers.apply(_xavier_init_)</span><br><span class="line">        self.regression_headers.apply(_xavier_init_)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.base_net.apply(_xavier_init_)</span><br><span class="line">        self.source_layer_add_ons.apply(_xavier_init_)</span><br><span class="line">        self.extras.apply(_xavier_init_)</span><br><span class="line">        self.classification_headers.apply(_xavier_init_)</span><br><span class="line">        self.regression_headers.apply(_xavier_init_)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">load</span>(<span class="params">self, model</span>):</span></span><br><span class="line">        self.load_state_dict(torch.load(model, map_location=<span class="keyword">lambda</span> storage, loc: storage))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save</span>(<span class="params">self, model_path</span>):</span></span><br><span class="line">        torch.save(self.state_dict(), model_path)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>损失函数作者选择使用的依旧是SSD的Smooth L1 Loss以及Cross Entropy Loss，其中Smooth L1 Loss用于边界框的回归，而Cross Entropy Loss则用于分类。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210527/8.png" width = "500" align=center /></p><p>具体pytorch实现如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiboxLoss</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, priors, neg_pos_ratio,</span></span></span><br><span class="line"><span class="params"><span class="function">                 center_variance, size_variance, device</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Implement SSD Multibox Loss.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Basically, Multibox loss combines classification loss</span></span><br><span class="line"><span class="string">         and Smooth L1 regression loss.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(MultiboxLoss, self).__init__()</span><br><span class="line">        self.neg_pos_ratio = neg_pos_ratio</span><br><span class="line">        self.center_variance = center_variance</span><br><span class="line">        self.size_variance = size_variance</span><br><span class="line">        self.priors = priors</span><br><span class="line">        self.priors.to(device)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, confidence, predicted_locations, labels, gt_locations</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Compute classification loss and smooth l1 loss.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            confidence (batch_size, num_priors, num_classes): class predictions.</span></span><br><span class="line"><span class="string">            locations (batch_size, num_priors, 4): predicted locations.</span></span><br><span class="line"><span class="string">            labels (batch_size, num_priors): real labels of all the priors.</span></span><br><span class="line"><span class="string">            boxes (batch_size, num_priors, 4): real boxes corresponding all the priors.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        num_classes = confidence.size(<span class="number">2</span>)</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            <span class="comment"># derived from cross_entropy=sum(log(p))</span></span><br><span class="line">            loss = -F.log_softmax(confidence, dim=<span class="number">2</span>)[:, :, <span class="number">0</span>]</span><br><span class="line">            mask = box_utils.hard_negative_mining(loss, labels, self.neg_pos_ratio)</span><br><span class="line"></span><br><span class="line">        confidence = confidence[mask, :]</span><br><span class="line">        <span class="comment"># 分类损失函数</span></span><br><span class="line">        classification_loss = F.cross_entropy(confidence.reshape(-<span class="number">1</span>, num_classes), labels[mask], reduction=<span class="string">&#x27;sum&#x27;</span>)</span><br><span class="line">        pos_mask = labels &gt; <span class="number">0</span></span><br><span class="line">        predicted_locations = predicted_locations[pos_mask, :].reshape(-<span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line">        gt_locations = gt_locations[pos_mask, :].reshape(-<span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line">        <span class="comment"># 边界框回归损失函数</span></span><br><span class="line">        smooth_l1_loss = F.smooth_l1_loss(predicted_locations, gt_locations, reduction=<span class="string">&#x27;sum&#x27;</span>)  <span class="comment"># smooth_l1_loss</span></span><br><span class="line">        <span class="comment"># smooth_l1_loss = F.mse_loss(predicted_locations, gt_locations, reduction=&#x27;sum&#x27;)  #l2 loss</span></span><br><span class="line">        num_pos = gt_locations.size(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> smooth_l1_loss / num_pos, classification_loss / num_pos</span><br></pre></td></tr></table></figure></p><h2 id="结果预测"><a href="#结果预测" class="headerlink" title="结果预测"></a>结果预测</h2><p>输入为：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210527/9.png" alt=""></p><p>输出为：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210527/10.png" alt=""></p><p>输入为：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210527/11.png" alt=""></p><p>输出为：<br><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210527/12.png" alt=""></p><h2 id="模型转换"><a href="#模型转换" class="headerlink" title="模型转换"></a>模型转换</h2><p>由于部署使用的是Tengine边缘推理框架，由于pytorch输出的模型无法直接转换到tmfile模型下，因此还是选择使用onnx中间件的形式进行过度，具体实现代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">model_path = <span class="string">&quot;models/pretrained/version-RFB-320.pth&quot;</span></span><br><span class="line">net = create_Mb_Tiny_RFB_fd(<span class="built_in">len</span>(class_names), is_test=<span class="literal">True</span>)</span><br><span class="line">net.load(model_path)</span><br><span class="line">net.<span class="built_in">eval</span>()</span><br><span class="line">net.to(<span class="string">&quot;cuda&quot;</span>)</span><br><span class="line"></span><br><span class="line">model_name = model_path.split(<span class="string">&quot;/&quot;</span>)[-<span class="number">1</span>].split(<span class="string">&quot;.&quot;</span>)[<span class="number">0</span>]</span><br><span class="line">model_path = <span class="string">f&quot;models/onnx/<span class="subst">&#123;model_name&#125;</span>.onnx&quot;</span></span><br><span class="line"></span><br><span class="line">dummy_input = torch.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">240</span>, <span class="number">320</span>).to(<span class="string">&quot;cuda&quot;</span>)</span><br><span class="line"><span class="comment"># dummy_input = torch.randn(1, 3, 480, 640).to(&quot;cuda&quot;) #if input size is 640*480</span></span><br><span class="line">torch.onnx.export(net, dummy_input, model_path, verbose=<span class="literal">False</span>, input_names=[<span class="string">&#x27;input&#x27;</span>], output_names=[<span class="string">&#x27;scores&#x27;</span>, <span class="string">&#x27;boxes&#x27;</span>])</span><br></pre></td></tr></table></figure></p><p>得到onnx模型后便可以进行Tengine模型的转换和部署，该部分将在下一篇文章继续讨论。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>[1].<a href="https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB">https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB</a><br></p><p>[2].<a href="https://github.com/onnx/onnx">https://github.com/onnx/onnx</a><br></p>]]></content>
      
      
      <categories>
          
          <category> 项目实践 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人脸识别 </tag>
            
            <tag> 人脸检测 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>详细解读Transformer那些有趣的特性</title>
      <link href="/2021/09/05/2%20%E8%AF%A6%E7%BB%86%E8%A7%A3%E8%AF%BBTransformer%E9%82%A3%E4%BA%9B%E6%9C%89%E8%B6%A3%E7%9A%84%E7%89%B9%E6%80%A7/"/>
      <url>/2021/09/05/2%20%E8%AF%A6%E7%BB%86%E8%A7%A3%E8%AF%BBTransformer%E9%82%A3%E4%BA%9B%E6%9C%89%E8%B6%A3%E7%9A%84%E7%89%B9%E6%80%A7/</url>
      
        <content type="html"><![CDATA[<p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210525/1.png" alt=""></p><blockquote><p>本文发现了Transformer的一些重要特性，如<strong>Transformer对严重的遮挡，扰动和域偏移具有很高的鲁棒性</strong>、<strong>与CNN相比，ViT更符合人类视觉系统，泛化性更强</strong>，等等…  代码即将开源！<br><br><strong>作者单位</strong>：澳大利亚国立大学, 蒙纳士大学, 谷歌等7家高校/企业</p><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2></blockquote><p>近期Vision Transformer（ViT）在各个垂直任务上均表现出非常不错的性能。这些模型基于multi-head自注意力机制，该机制可以灵活地处理一系列图像patches以对上下文cues进行编码。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210525/2.png" alt=""></p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210525/3.png" alt=""></p><p>一个重要的问题是，在以给定patch为条件的图像范围内，如何灵活地处理图像中的干扰，例如严重的遮挡问题、域偏移问题、空间排列问题、对抗性和自然扰动等等问题。作者通过涵盖3个ViT系列的大量实验，以及与高性能卷积神经网络（CNN）的比较，系统地研究了这些问题。并通过分析得出了ViT的以下的特性：</p><p>1) Transformer对严重的遮挡，扰动和域偏移具有很高的鲁棒性，例如，即使随机遮挡80％的图像内容，其在ImageNet上仍可保持高达60％的top-1精度; </p><p>2) Transformer对于遮挡的良好表现并不是由于依赖局部纹理信息，与CNN相比，ViT对纹理的依赖要小得多。当经过适当训练以对基于shape的特征进行编码时，ViT可以展现出与人类视觉系统相当的shape识别能力;</p><p>3) 使用ViT对shape进行编码会产生有趣的现象，在即使没有像素级监督的情况下也可以进行精确的语义分割;</p><p>4) 可以将单个ViT模型提取的特征进行组合以创建特征集合，从而在传统学习模型和少量学习模型中的一系列分类数据集上实现较高的准确率。实验表明，ViT的有效特征是由于通过自注意力机制可以产生的灵活和动态的感受野所带来的。</p><h2 id="本文讨论主题"><a href="#本文讨论主题" class="headerlink" title="本文讨论主题"></a>本文讨论主题</h2><h3 id="2-1-ViT对遮挡鲁棒否？"><a href="#2-1-ViT对遮挡鲁棒否？" class="headerlink" title="2.1 ViT对遮挡鲁棒否？"></a>2.1 ViT对遮挡鲁棒否？</h3><p>这里假设有一个网络模型$f$，它通过处理一个输入图像$x$来预测一个标签$y$，其中$x$可以表示为一个patch $x={x<em>i}</em>{i=1}^N$的序列，$N$是图像patch的总数。</p><p>虽然可以有很多种方法来建模遮挡，但本文还是选择了采用一个简单的掩蔽策略，选择整个图像patch的一个子集，$M &lt; N$，并将这些patch的像素值设为0，这样便创建一个遮挡图像$x’$。</p><p>作者将上述方法称为<strong>PatchDrop</strong>。目的是观察鲁棒性$f(x’)_{argmax}=y$。</p><p>作者总共实验了3种遮挡方法:<br>1) Random PatchDrop<br>2) Salient(foreground) PatchDrop<br>3) Non-salient (background) PatchDrop</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210525/4.png" alt=""></p><h4 id="1、Random-PatchDrop"><a href="#1、Random-PatchDrop" class="headerlink" title="1、Random PatchDrop"></a><strong><em>1、Random PatchDrop</em></strong></h4><p>ViT通常将图像划分为196个patch，每个patch为14x14网格，这样一幅224x224x3大小的图像分割成196个patches，每个patch的大小为16x16x3。例如，随机从输入中删除100个这样的补丁就相当于丢失了51%的图像内容。而这个随机删除的过程即为<strong>Random PatchDrop</strong>。</p><h4 id="2、Salient-foreground-PatchDrop"><a href="#2、Salient-foreground-PatchDrop" class="headerlink" title="2、Salient(foreground) PatchDrop"></a><strong><em>2、Salient(foreground) PatchDrop</em></strong></h4><p>对于分类器来说，并不是所有的像素都具有相同的值。为了估计显著区域，作者利用了一个自监督的ViT模型DINO，该模型使用注意力分割图像中的显著目标。按照这种方法可以从196个包含前n个百分比的前景信息的patches中选择一个子集并删除它们。而这种通过自监督模型删除显著区域的过程即为<strong>Salient (foreground) PatchDrop</strong>。</p><h4 id="3、Non-salient-background-PatchDrop"><a href="#3、Non-salient-background-PatchDrop" class="headerlink" title="3、Non-salient(background) PatchDrop"></a><strong><em>3、Non-salient(background) PatchDrop</em></strong></h4><p>采用与SP（Salient(foreground) PatchDrop）相同的方法选择图像中最不显著的区域。包含前景信息中最低n%的patch被选中并删除。同样，而这种通过自监督模型删除非显著区域的过程即为<strong>Non-salient(background) PatchDrop</strong>。</p><h4 id="鲁棒性分析"><a href="#鲁棒性分析" class="headerlink" title="鲁棒性分析"></a>鲁棒性分析</h4><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210525/5.png" alt=""></p><p>以Random PatchDrop为例，作者给出5次测试的平均值和标准偏差。对于显著性和非显著性Patchdrop，由于获得的遮挡掩模是确定性的，作者只给出了1次运行的精度值。</p><p>Random PatchDrop 50%的图像信息几乎完全破坏了CNN的识别能力。例如，当去掉50%的图像内容时ResNet50的准确率为0.1%，而DeiT-S的准确率为70%。一个极端的例子可以观察到，当90%的图像信息丢失，但Deit-B仍然显示出37%的识别精度。这个结果在不同的ViT体系结构中是一致的。同样，ViT对前景(显著)和背景(非显著)内容的去除也有很不错的表现。</p><h4 id="Class-Token-Preserves-Information"><a href="#Class-Token-Preserves-Information" class="headerlink" title="Class Token Preserves Information"></a>Class Token Preserves Information</h4><p>为了更好地理解模型在这种遮挡下的性能鲁棒的原有，作者将不同层的注意力可视化(图4)。 通过下图可以看出浅层更关注遮挡区域，而较深的层更关注图像中的遮挡以外的信息。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210525/6.png" alt=""></p><p>然后作者还研究这种从浅层到更深层次的变化是否是导致针对遮挡的Token不变性的原因，而这对分类是非常重要的。作者测量了原始图像和遮挡图像的特征/标记之间的相关系数。在ResNet50的情况下测试在logit层之前的特性，对于ViT模型，Class Token从最后一个Transformer block中提取。与ResNet50特性相比，来自Transformer的Class Token明显更鲁棒，也不会遭受太多信息损失(表1)。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210525/7.png" alt=""></p><p>此外，作者还可视化了ImageNet中12个选择的超类的相关系数，并注意到这种趋势在不同的类类型中都存在，即使是相对较小的对象类型，如昆虫，食物和鸟类。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210525/8.png" alt=""></p><h3 id="2-2-ViT能否同时学习Shape和Texture这2种特性？"><a href="#2-2-ViT能否同时学习Shape和Texture这2种特性？" class="headerlink" title="2.2 ViT能否同时学习Shape和Texture这2种特性？"></a>2.2 ViT能否同时学习Shape和Texture这2种特性？</h3><p>Geirhos等人引入了Shape vs Texture的假设，并提出了一种训练框架来增强卷积神经网络(CNNs)中的shape偏差。</p><p>首先，作者对ViT模型进行了类似的分析，得出了比CNN更强的shape偏差，与人类视觉系统识别形状的能力相当。然而，这种方法会导致自然图像的精度显著下降。</p><p>为了解决这种问题，在第2种方法中，作者将shape token引入到Transformer体系结构中，专门学习shape信息，使用一组不同的Token在同一体系结构中建模Shape和Texture相关的特征。为此，作者从预训练的高shape偏差CNN模型中提取shape信息。而作者的这种蒸馏方法提供了一种平衡，既保持合理的分类精度，又提供比原始ViT模型更好的shape偏差。</p><h4 id="Training-without-Local-Texture"><a href="#Training-without-Local-Texture" class="headerlink" title="Training without Local Texture"></a>Training without Local Texture</h4><p>在训练中首先通过创建一个SIN风格化的ImageNet数据（从训练数据中删除局部纹理信息）。在这个数据集上训练非常小的DeiT模型。通常情况下，vit在训练期间需要大量的数据增强。然而，由于较少的纹理细节，使用SIN进行学习是一项困难的任务，并且在风格化样本上进行进一步的扩展会破坏shape信息，使训练不稳定。因此，在SIN上训练模型不使用任何augmentation、label smoothing或Mixup。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210525/9.png" alt=""></p><p>作者观察到，在ImageNet上训练的ViT模型比类似参数量的CNN模型表现出更高的shape偏差，例如，具有2200万个参数的DeiT-S比ResNet50表现更好(右图)。当比较SIN训练模型时，ViT模型始终优于cnn模型。有趣的是，DeiT-S在SIN数据集上训练时达到了人类水平(左图)。</p><h4 id="Shape-Distillation"><a href="#Shape-Distillation" class="headerlink" title="Shape Distillation"></a>Shape Distillation</h4><p>通过学习Teacher models 提供的soft labels，知识蒸馏可以将大teacher models压缩成较小的Student Model。本文作者引入了一种新的shape token，并采用 Adapt Attentive Distillation从SIN dataset(ResNet50-SIN)训练的CNN中提取Shape特征。作者注意到，ViT特性本质上是动态的，可以通过Auxiliary Token来控制其学习所需的特征。这意味着单个ViT模型可以同时使用单独的标记显示high shape和texture bias(下表)。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210525/10.png" alt=""></p><p>当引入shape token时在分类和形状偏差度量方面获得了更平衡的性能(图6)。为了证明这些不同的token(用于分类和shape)可以确实模型独特的特征，作者计算了所蒸馏的模型DeiT-T-SIN和DeiT-S-SIN的class和shape token之间的余弦相似度，结果分别是0.35和0.68。这明显低于class和distill token之间的相似性；DeiT-T和Deit-S分别为0.96和0.94。这证实了关于在Transformer中使用单独的Token可以用来建模不同特征的假设，这是一种独特的能力，但是不能直接用在CNN模型中。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210525/11.png" alt=""></p><h4 id="Shape-biased-ViT-Offers-Automated-Object-Segmentation"><a href="#Shape-biased-ViT-Offers-Automated-Object-Segmentation" class="headerlink" title="Shape-biased ViT Offers Automated Object Segmentation"></a>Shape-biased ViT Offers Automated Object Segmentation</h4><p>有趣的是，没有局部纹理或形状蒸馏的训练可以让ViT专注于场景中的前景物体而忽略背景(图4)。这为图像提供了自动语义分割的特征，尽管该模型从未显示像素级对象标签。这也表明，在ViT中促进shape偏差作为一个自监督信号，模型可以学习不同shape相关的特征，这有助于定位正确的前景对象。值得注意的是，没有使用shape token的训练中ViT表现得比较差(Table 3)。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210525/12.png" alt=""></p><h3 id="2-3-位置编码是否真的可以表征Global-Context？"><a href="#2-3-位置编码是否真的可以表征Global-Context？" class="headerlink" title="2.3 位置编码是否真的可以表征Global Context？"></a>2.3 位置编码是否真的可以表征Global Context？</h3><p>Transformer使用self-attention(而不是RNN中的顺序设计)并行处理长序列，其序列排序是不变的。但是它的明显缺点是忽略了输入序列元素的顺序，这可能很重要。</p><p>在视觉领域patch的排列顺序代表了图像的整体结构和整体构成。由于ViT对图像块进行序列处理，改变序列的顺序，例如对图像块进行shuffle操作但是该操作会破坏图像结构。</p><p>当前的ViT使用位置编码来保存Context。在这里问题是，如果序列顺序建模的位置编码允许ViT在遮挡处理是否依然有效?</p><p>然而，分析表明，Transformer显示排列不变的patch位置。位置编码对向ViT模型注入图像结构信息的作用是有限的。这一观察结果也与语言领域的发现相一致。</p><h4 id="Sensitivity-to-Spatial-Structure"><a href="#Sensitivity-to-Spatial-Structure" class="headerlink" title="Sensitivity to Spatial Structure"></a>Sensitivity to Spatial Structure</h4><p>通过对输入图像patch使用shuffle操作来消除下图所示的图像(空间关系)中的结构信息。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210525/13.png" alt=""></p><p>作者观察到，当输入图像的空间结构受到干扰时，DeiT模型比CNN模型保持了更高程度的准确性。这也一方面证明了位置编码对于做出正确的分类决策并不是至关重要的，并且该模型并没有使用位置编码中保存的patch序列信息来恢复全局图像context。即使在没有这种编码的情况下，与使用位置编码的ViT相比，ViT也能够保持其性能，并表现出更好的排列不变性(下图)。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210525/14.png" alt=""></p><p>最后，在ViT训练过程中，当patch大小发生变化时，对自然图像进行非混叠处理时，其排列不变性也会随着精度的降低而降低(下图)。作者将ViT的排列不变性归因于它们的动态感受野，该感受野依赖于输入小patch，可以与其他序列元素调整注意，从而在中等变换速率下，改变小patch的顺序不会显著降低表现。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210525/15.png" alt=""></p><blockquote><p>从上面的分析可以看出，就像texture bias假设是错误的一样，依赖位置编码来在遮挡下表现良好也是不准确的。作者得出这样的结论，这种鲁棒性可能只是由于ViT灵活和动态的感受野所带来的，这同时也取决于输入图像的内容。</p></blockquote><h3 id="2-4-ViT对对抗信息和自然扰动的鲁棒性又如何？"><a href="#2-4-ViT对对抗信息和自然扰动的鲁棒性又如何？" class="headerlink" title="2.4 ViT对对抗信息和自然扰动的鲁棒性又如何？"></a>2.4 ViT对对抗信息和自然扰动的鲁棒性又如何？</h3><p>作者通过计算针对雨、雾、雪和噪声等多种综合常见干扰的平均损坏误差(mCE)来研究这一问题。具有类似CNN参数的ViT(例如，DeiT-S)比经过增强训练的ResNet50(Augmix)对图像干扰更加鲁棒。有趣的是，在ImageNet或SIN上未经增强训练的卷积和Transformer模型更容易受到图像干扰的影响(表6)。这些发现与此一致，表明数据增强对于提高常见干扰的鲁棒性是很必要的。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210525/16.png" alt=""></p><p>作者还观察到adversarial patch攻击的类似问题。ViT的鲁棒性高于CNN，通用adversarial patch在白盒设置(完全了解模型参数)。在SIN上训练的ViT和CNN比在ImageNet上训练的模型更容易受到adversarial patch攻击(图10)，这是由于shape偏差与鲁棒性权衡导致的。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210525/17.png" alt=""></p><h3 id="2-5-当前ViT的最佳Token是什么？"><a href="#2-5-当前ViT的最佳Token是什么？" class="headerlink" title="2.5 当前ViT的最佳Token是什么？"></a>2.5 当前ViT的最佳Token是什么？</h3><p>ViT模型的一个独特特征是模型中的每个patch产生一个class token，class head可以单独处理该class token(下图所示)。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210525/18.png" alt=""></p><p>使得可以测量一个ImageNet预先训练的ViT的每个单独patch的区分能力，如图12所示，由更深的区块产生的class token更具鉴别性，作者利用这一结果来识别其token具有best downstream transferability最优patch token集合。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210525/19.png" alt=""></p><h4 id="Transfer-Methodology"><a href="#Transfer-Methodology" class="headerlink" title="Transfer Methodology"></a>Transfer Methodology</h4><p>如图12所示，作者分析了DeiT模型的block的分类精度，发现在最后几个block的class token中捕获了最优的判别信息。为了验证是否可以将这些信息组合起来以获得更好的性能，作者使用DeiT-S对细粒度分类数据集上现成的迁移学习进行了消融研究(CUB)，如下表所示。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210525/20.png" alt=""></p><p>在这里，作者从不同的块连接class token(可选地结合平均补丁标记)，并训练一个线性分类器来将特征转移到下游任务。</p><p>请注意，一个patch token是通过沿着patch维度进行平均生成的。最后4个块的class token连接得到了最好的迁移学习性能。</p><p>作者将这种迁移方法称为<strong>DeiT-S(ensemble)</strong>。来自所有块的class token和averaged patch tokens的拼接表现出与来自最后4个块的token相似的性能，但是需要显著的大参数来训练。作者进一步在更大范围的任务中使用DeiT-S(集成)进行进一步实验，以验证假设。在接下来的实验中，同时还将CNN Baseline与在预训练的ResNet50的logit层之前提取的特征进行比较。</p><h4 id="General-Classification"><a href="#General-Classification" class="headerlink" title="General Classification"></a>General Classification</h4><p>作者还研究了几个数据集的现成特征的可迁移性，包括Aircraft, CUB, DTD, GTSRB, Fungi, Places365和iNaturalist数据集。这些数据集分别用于细粒度识别、纹理分类、交通标志识别、真菌种类分类和场景识别，分别有100、200、47、43、1394、365和1010类。在每个数据集上训练一个线性分类器，并在测试分割上评估其性能。与CNN Baseline相比，ViT特征有了明显的改善(图13)。事实上，参数比ResNet50少5倍左右的DeiT-T性能更好。此外，本文提出的集成策略在所有数据集上都获得了最好的结果。</p><h4 id="Few-Shot-Learning"><a href="#Few-Shot-Learning" class="headerlink" title="Few-Shot Learning"></a>Few-Shot Learning</h4><p>在few-shot learning的情况下，元数据集是一个大规模的benchmark，包含一个不同的数据集集覆盖多个领域。作者使用提取的特征为每个query学习support set上的线性分类器，并使用标准FSL协议评估。ViT特征在这些不同的领域之间转移得更好(图13)。作者还强调了QuickDraw的一个改进，包含手绘草图的数据集，这与研究结果一致。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210525/21.png" alt=""></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>[1].Intriguing Properties of Vision Transformers.<br></p>]]></content>
      
      
      <categories>
          
          <category> Transformer </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>在ResNet与Transformer均适用的Skip Connection解读</title>
      <link href="/2021/09/05/1%20%E5%9C%A8ResNet%E4%B8%8ETransformer%E5%9D%87%E9%80%82%E7%94%A8%E7%9A%84Skip%20Connection%E8%A7%A3%E8%AF%BB/"/>
      <url>/2021/09/05/1%20%E5%9C%A8ResNet%E4%B8%8ETransformer%E5%9D%87%E9%80%82%E7%94%A8%E7%9A%84Skip%20Connection%E8%A7%A3%E8%AF%BB/</url>
      
        <content type="html"><![CDATA[<p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210521/1.png" alt=""></p><blockquote><p>该文主要是分析和讨论了跳跃连接的一些局限，同时分析了BN的一些限制，提出了通过递归的Skip connection和layer normalization来自适应地调整输入scale的策略，可以很好的提升跳Skip connection的性能，该方法在CV和NLP领域均适用。</p></blockquote><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>Skip connection是一种广泛应用于提高深度神经网络性能和收敛性的技术，它通过神经网络层传播的线性分量，缓解了非线性带来的优化困难。但是，从另一个角度来看，它也可以看作是输入和输出之间的调制机制，输入按预定义值1进行缩放。</p><p>在本文中，作者通过研究Skip connection的有效性和scale factors显示，一个微不足道的调整将导致spurious gradient爆炸或消失，这可以通过normalization来解决，特别是layer normalization。受此启发作者进一步提出通过递归的Skip connection和layer normalization来自适应地调整输入scale，这大大提高了性能，并且在包括机器翻译和图像分类数据集在内的各种任务中具有很好的泛化效果。</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210521/2.png" alt="图1 常用skip connections"></p><h3 id="这项工作的特点："><a href="#这项工作的特点：" class="headerlink" title="这项工作的特点："></a>这项工作的特点：</h3><p>1) 主要关注LN和skip connection的结合；<br>2) 重新思考了层归一化的作用，选择不进行缩放；<br>3) 在具有代表性的计算机视觉和自然语言处理任务上进行实验；<br>4) 摆脱了泛化了所有以前工作的残差块的一般形式，并提出了一种新的递归残差块结构，它具有层归一化，优于本工作中检查的所有一般形式的变体；</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><h3 id="connection-problem"><a href="#connection-problem" class="headerlink" title="connection problem"></a>connection problem</h3><p>在进行尺度scaling时，会出现梯度爆炸或消失的问题，阻碍了深度神经网络的高效优化。</p><h3 id="optimization-problem"><a href="#optimization-problem" class="headerlink" title="optimization problem"></a>optimization problem</h3><p>由于早期的工作已经确定，将Skip connection直接结合到神经网络的前向传播中就足够了，不需要任何尺度，后续的优化问题研究大多遵循Skip connection结构。</p><h3 id="架构说明"><a href="#架构说明" class="headerlink" title="架构说明"></a>架构说明</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210521/3.png" alt="图2 常见LN与skip connections组合"></p><h4 id="Expanded-Skip-Connection-xSkip-："><a href="#Expanded-Skip-Connection-xSkip-：" class="headerlink" title="Expanded Skip Connection (xSkip)："></a><strong>Expanded Skip Connection (xSkip)</strong>：</h4><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210521/4.png" alt=""></p><p>其中，$x$和$y$分别为残差块的输入和输出。$F$为weighted neural network layer，$\lambda$为modulating scalar。</p><p>考虑到神经网络层可能具有不同的表示能力和优化难度，这种结构自然调整了跳跃的重要性。然而，需要注意的是，在这项工作中$\lambda$是固定的，目的是隔离缩放的影响。虽然学习过的$\lambda$可能更好地捕捉到这2个部分之间的平衡，但是学习$\lambda$变成了另一个变量。</p><h4 id="Expanded-Skip-Connection-with-Layer-Normalization-xSkip-LN-："><a href="#Expanded-Skip-Connection-with-Layer-Normalization-xSkip-LN-：" class="headerlink" title="Expanded Skip Connection with Layer Normalization (xSkip+LN)："></a><strong>Expanded Skip Connection with Layer Normalization (xSkip+LN)</strong>：</h4><p>在Transformer将跳跃连接与层规范化相结合的激励下，作者进一步研究了层规范化对扩展跳跃连接的影响：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210521/5.png" alt=""></p><p>实验表明层归一化有助于缓解调制因子在优化过程中引起的梯度畸变。不同于作用于“样本空间”的BN，LN则是作用于“特征空间”。同时在神经网络难以优化的情况下，LN仍然可以帮助学习shortcut，而BN可能会失败。</p><h4 id="Recursive-Skip-Connection-with-Layer-Normalization-rSkip-LN-："><a href="#Recursive-Skip-Connection-with-Layer-Normalization-rSkip-LN-：" class="headerlink" title="Recursive Skip Connection with Layer Normalization (rSkip+LN)："></a><strong>Recursive Skip Connection with Layer Normalization (rSkip+LN)</strong>：</h4><p>另一种稳定梯度的方法是每次保持$\lambda$=1，但重复添加带有LN的shortcut，这样更多的输入信息也被建模。它被递归定义为：</p><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210521/6.png" alt=""></p><p>$\lambda$应该是一个不小于1的整数。例如，当$\lambda$=1时，上式便回归到Transformer中使用的block，并符合跳过不需要缩放的结果。</p><p>通过recursive skip connection with layer normalization，该模型鼓励多次使用层归一化来改进优化，通过跳跃连接可以包含更多的x信息。此外，与一次性简单地合并比例跳跃相比，该模型可能获得更强的表达能力，因为每一个递归步骤本质上构建了一个不同的特征分布，递归结构可以学习自适应的x与F(x,W)。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="实验1：PreAct-ResNet-110-on-cifar10"><a href="#实验1：PreAct-ResNet-110-on-cifar10" class="headerlink" title="实验1：PreAct-ResNet-110 on cifar10"></a>实验1：PreAct-ResNet-110 on cifar10</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210521/7.png" alt=""></p><h3 id="实验2：EN-VI-machine-translation"><a href="#实验2：EN-VI-machine-translation" class="headerlink" title="实验2：EN-VI machine translation"></a>实验2：EN-VI machine translation</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210521/8.png" alt=""></p><h3 id="实验3：BN代替LN"><a href="#实验3：BN代替LN" class="headerlink" title="实验3：BN代替LN"></a>实验3：BN代替LN</h3><p><img src="https://gitee.com/chaucerg/pic_-web/raw/master/images_20210521/9.png" alt=""></p><p>可以看出，与LN结合跳跃连接相比，BN的效果较差。而本文所提出的递归策略可以帮助BN提升效果。</p><h3 id="实验结论"><a href="#实验结论" class="headerlink" title="实验结论"></a>实验结论</h3><p>作者通过对不同任务的实验（Transformer和ResNet），得出如下结论:</p><ul><li><p>没有经过任何归一化的expanded skip connection确实会造成梯度畸形，导致神经网络的学习效果不理想。层归一化在一定程度上有助于解决 expanded skip connection带来的优化问题。</p></li><li><p>本文提出的带有LN的recursive skip connection，通过将expanded skip connection划分为多个阶段，以更好地融合转换输入的效果，进一步简化了优化过程。</p></li><li><p>利用Transformer在WMT-2014 EN-DE机器翻译数据集上的实验结果进一步证明了递归架构的有效性和效率，模型性能甚至优于3倍大的模型。</p></li></ul><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>[1].Rethinking Skip Connection with Layer Normalization in Transformers and ResNets<br></p>]]></content>
      
      
      <categories>
          
          <category> 卷积CNN </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 残差连接 </tag>
            
            <tag> CNN </tag>
            
            <tag> Tansformer </tag>
            
            <tag> ResNet </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
